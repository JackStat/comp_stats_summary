<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="variable-selection-ridge-regression-and-lasso.html">
<link rel="next" href="round-up.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
<li class="chapter" data-level="2.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>2.9</b> Model Selection</a><ul>
<li class="chapter" data-level="2.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mallows-cp-statistic"><i class="fa fa-check"></i><b>2.9.1</b> Mallow’s cp statistic</a></li>
<li class="chapter" data-level="2.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#search-strategies"><i class="fa fa-check"></i><b>2.9.2</b> Search strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression and Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
<li class="chapter" data-level="11.3" data-path="round-up.html"><a href="round-up.html#cheatsheet"><i class="fa fa-check"></i><b>11.3</b> Cheatsheet</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bagging-and-boosting" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Bagging and Boosting</h1>
<p>Bagging stands for bootstrapping aggregating and</p>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">10.1</span> Bagging</h2>
<p>Bagging works as follows: Consider a base procedure <span class="math display">\[ \hat{g}(\cdot): \mathbb{R}^p \rightarrow \mathbb{R}\]</span></p>
<ol style="list-style-type: decimal">
<li>Draw a bootstrap sample <span class="math inline">\((Y_1^*, X_1^*), ..., (Y_1^*, X_n^*)\)</span> and compute the bootstrap estimator <span class="math inline">\(\hat{g}(\cdot)^*\)</span>.</li>
<li>Repeat the above step <span class="math inline">\(B\)</span> times yielding <span class="math inline">\(\hat{g}(\cdot)^{*1}, ..., \hat{g}(\cdot)^{*B}\)</span> bootstrap estimators</li>
<li>Average the estimates to construct the bagged estimator: <span class="math display">\[ \hat{g}_{Bag}(x) = n^{-1} \sum\limits_{i = 1}^B\hat{g}(\cdot)^{*i}\]</span> Then, <span class="math inline">\(\hat{g}(\cdot)\)</span> is nothing else than an approximation of the bootstrap expectation. <span class="math display">\[\hat{g}(\cdot) \approx \mathbb{E}^*[g(x)^{*}]\]</span> Note that the novel point is that we now use this approximation as an estimate of <span class="math inline">\(g(\cdot)\)</span>. However,
<span class="math display">\[\begin{equation}
   \begin{split}
   \hat{g}(\cdot)_{Bag} &amp; = \hat{g}(\cdot) + \mathbb{E}^*[g(x)^{*}] - \hat{g}(\cdot) \\
   &amp; = \hat{g}(\cdot) + \text{bootstrap bias estimate}
   \end{split}
   \end{equation}\]</span></li>
</ol>
<p>Instead of subtracting the bootstrap bias estimate, we are adding it! However, for trees for example, bagging reduces the variance of the estimate so much that the bias increase will not be strong enough to push the mean square error up. Let us consider a one-dimensional example.</p>
<p>The variance of an indicator function (which is a Bernoulli experiment) is given by <span class="math display">\[ Var(1_{X&gt;d}) = \mathbb{P}(X &gt; d)(1 - (\mathbb{P}(X &gt; d))\]</span> If we assume <span class="math inline">\(X \sim N(0, 1)\)</span> and <span class="math inline">\(d = 0\)</span>, <span class="math inline">\(\mathbb{P}(X &gt; d) = 1 - \mathbb{P}(X&lt;d) = 0.5\)</span>, so the above quantity is <span class="math inline">\(1/4\)</span>. For bagged trees, it turns out that the estimator is a product of probit functions <span class="math inline">\(\phi(d-X)\)</span>. Since it holds that <span class="math display">\[ \text{if}\;\; X \sim F, \;\;F(X) \sim U\]</span> the variance of random forest is <span class="math display">\[Var(\phi(d-X)) = Var(U) = 1/12\]</span> So the variance was reduced by a factor of 3.</p>
</div>
<div id="subagging" class="section level2">
<h2><span class="header-section-number">10.2</span> Subagging</h2>
<p>Subagging stands for sub-sampling and aggregation. It is different from bagging in that instead of drawing bootstrap samples of size <span class="math inline">\(n\)</span>, we only draw samples of size <span class="math inline">\(m &lt; n\)</span>, that is <span class="math inline">\((X_1^*, Y_1^*), ..., (X_m^*, Y_m^*)\)</span> but <strong>without replacement</strong>. It can be shown that for some situations, subagging with <span class="math inline">\(m = n/2\)</span> is equivalent to bagging, hence, subagging is a cheap version of bagging.</p>
<p>Bagging (and subagging) have one drawback which is the lack of interpretability. In the script, there is a comparison between MARS and trees. Both subagging and bagging helps for trees, but does not help for MARS. How come? Remember that fitted values of trees are piece-wise constant. Bagging makes them smoother (and hence more like MARS). MARS yield a piece-wise linear function by nature. Trees have a high variance and hence the bias-variance trade-off can be optimized with bagging. MARS don’t have such a high variance, hence, optimization is not possible to the same extend.</p>
</div>
<div id="l_2-boosting" class="section level2">
<h2><span class="header-section-number">10.3</span> <span class="math inline">\(L_2\)</span>-Boosting</h2>
<p>We saw bagging was a variance reduction technique. Boosting is a bias reduction technique. As with bagging there is a base estimator <span class="math inline">\(\hat{g}(\cdot)\)</span>. Then, you basically refit it to the residuals to reduce them many times. The concrete implementation is:</p>
<ul>
<li><p>Fit an estimator to the data and compute the residuals <span class="math display">\[U_i = Y_i - \nu\hat{g}_1(x_i)\]</span> where $&lt; 1 $ is a tuning parameter. The smaller <span class="math inline">\(\nu\)</span>, the more <em>explainable</em> residuals you leave for suceeding estimators. Denote <span class="math inline">\(\hat{f}_1(x) = \nu g_1(x)\)</span></p></li>
<li><p>For m = 2, 3, …, M: Fit the residuals to the data, i.e <span class="math display">\[ (X_i, U_i) \rightarrow \hat{g}(x)\]</span> Set <span class="math display">\[ \hat{f}_m(x) = \hat{f}_{m-1}(x) + \nu \hat{g}_m(x)\]</span> Compute the current residuals: <span class="math display">\[U_i = Y_i - \hat{f}_m(x)\]</span></p></li>
</ul>
<p>A small <span class="math inline">\(\nu\)</span> can be interpreted as follows: You go into the right direction, but you do that slowly and you allow yourself to be corrected later.</p>
<p>Boosting can be useful in the context of thinning-out with trees, where you don’t have enough observations in the terminal nodes to continue. Boosting then comes up with a more complex solution by taking <em>linear combinations</em> of trees. For classificcation trees, you can also extend the tree by taking</p>
<p>Boosting can also be used for varible selection. For example, let <span class="math inline">\(\hat{g}\)</span> be a GAM with one predictor. Then, for each variable fit such a gam and set <span class="math inline">\(f_1(x) = arg\min\limits_{\hat{g}_j(x) \; j =\{1, ..., p\}}\|Y - \hat{g}_j(x)\|\)</span>, i.e. take the the additive model that had the smallest residual sum of squares. Then, use the <em>adjusted</em> residuals from it and fit another <span class="math inline">\(p\)</span> GAMs and select again the gam with the smallest RSS.</p>
</div>
<div id="some-unfinished-stuff" class="section level2">
<h2><span class="header-section-number">10.4</span> Some unfinished stuff</h2>
<p>Let’s consider a simple one-dimensional example. If you have a cut-off at <span class="math inline">\(d\)</span> (so for example for classification, <span class="math inline">\(X &gt; d\)</span> is classified as <span class="math inline">\(1\)</span>, <span class="math inline">\(X &lt; d\)</span> as <span class="math inline">\(0\)</span>) and you take the mean over many bootstrap samples (which you do with random forests, where each tree has a potentially different <span class="math inline">\(d\)</span>), with <span class="math inline">\(n \rightarrow \infty\)</span> you get a smooth function (probably some form of a transformed binomial that is asymptotically normal)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">one_mean &lt;-<span class="st"> </span>function(<span class="dt">d =</span> <span class="dv">0</span>, ...) {
  x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(...)
  x_boolean &lt;-<span class="st"> </span>x &gt;<span class="st"> </span>d
  <span class="kw">mean</span>(x_boolean)
}

mean_sim &lt;-<span class="st"> </span><span class="kw">rerun</span>(<span class="dv">1000</span>, <span class="kw">one_mean</span>(<span class="dt">n =</span> <span class="dv">200</span>)) %&gt;%
<span class="st">  </span><span class="kw">flatten_dbl</span>()

<span class="kw">ggplot</span>(<span class="kw">data_frame</span>(<span class="dt">mean =</span> mean_sim), <span class="kw">aes</span>(<span class="dt">x =</span> mean)) +
<span class="st">  </span><span class="kw">stat_ecdf</span>(<span class="dt">geom =</span> <span class="st">&quot;step&quot;</span>)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Here you can see that if <span class="math inline">\(x \sim F\)</span>, <span class="math inline">\(F(X) \sim Unif\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)
<span class="kw">plot.ecdf</span>(x)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">pnorm</span>(x)
<span class="kw">plot.ecdf</span>(y)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-46-2.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="variable-selection-ridge-regression-and-lasso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="round-up.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
