[
["index.html", "Computational Statistics - Summary Chapter 1 Introduction", " Computational Statistics - Summary Lorenz Walthert Chapter 1 Introduction This is a summary (or rather rephrased version) of the class computational statistics at ETH Zurich, enriched with notes from the lecture, examples and R code. The main source is the script of the course. One last thing: If you find one, you can make pull request or let me know, so we can fix it. "],
["multiple-linear-regression.html", "Chapter 2 Multiple Linear Regression 2.1 Assumptions of the linear model 2.2 Geometric interpretation 2.3 Hat matrix 2.4 Multiple regression vs. simple regression 2.5 Properties 2.6 Tests 2.7 Diagnostics 2.8 Generalized least squares 2.9 Model Selection", " Chapter 2 Multiple Linear Regression This chapter has not yet been summarized completely. Since liner models are well known (at least to me), the chapter is kept very brief. The goal of linear modelling is A good fit: Explain a lot of variance and keep the errors small. Good parameter estimates. Prediction: Predict new values. Inference: Say something about whether a variable has an influence or not on the response via significance levels and confidence intervals. Note that a good (in-sample) fit conflicts with good prediction out-of-sample. Linear models are stochastic, since \\(\\epsilon\\) is stochastic and hence the response variable \\(Y_i\\) also. We can assume that the predictors are non-random. Linear models are called linear because they are linear in the coefficients, not linear in the predictors. The predictors can be transformed arbitrary. Note the difference between the residuals \\(r_i\\) and the errors \\(\\epsilon_i\\). The residuals are the estimates of the errors. The variance of the error can be estimated with \\[ \\hat{\\sigma}^2 = \\frac{1}{n - p} \\sum\\limits_{i = 1}^n {r_i}^2\\] The solution linear models is computed via ordinary least squares and can be derived analytically. \\[\\hat{\\beta} = (X&#39;X)^{-1}X&#39;Y\\] For numerical stability reasons, do not compute the inverse of a matrix if possible, but do a QR decomposition instead. Here, we can do that by using the normal equations \\[ \\begin{split} \\hat{\\beta} &amp; = (X&#39;X)^{-1}X&#39;Y\\;\\;\\;\\;\\, | \\times (X&#39;X) \\;\\; \\text{from left} \\\\ (X&#39;X)\\hat{\\beta} &amp;= X&#39;Y \\;\\;\\; \\;\\;\\; \\;\\;\\; \\;\\;\\; \\;\\;\\; | X = QR \\;\\;\\; \\text{whereas} \\;\\;Q \\;\\; \\text{is orthogonal} \\\\ R&#39;R\\hat{\\beta} &amp;= X&#39;Y \\;\\;\\; \\;\\;\\; \\;\\;\\; \\;\\;\\; \\;\\;\\; | \\;\\; \\text{set} \\;\\; c= R\\beta \\\\ R&#39;c &amp; = X&#39;Y \\\\ \\end{split} \\] Since \\(R&#39;\\) is a lower triangular matrix, you can now solve for \\(c\\) recursively. \\[\\begin{pmatrix} \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ &amp; &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ &amp; &amp; &amp; \\cdots &amp; \\cdots \\\\ \\large 0 &amp; &amp; &amp; &amp; r_{nn} \\end{pmatrix} \\times \\begin{pmatrix} c_1 \\\\ \\vdots \\\\ \\vdots \\\\\\vdots \\\\ c_n \\\\ \\end{pmatrix} = (X&#39;Y)_{n \\times 1} \\] First, solve \\(c_n\\) using \\(r_{nn} c_n = (X&#39;Y)_{n}\\), then for \\(c_{n-1}\\) etc. when \\(c\\) is solved, solve \\[ R\\beta = c\\] Which has the same structure. 2.1 Assumptions of the linear model The functional form of the model is correct \\(\\mathbb{E}[\\epsilon_i] = 0\\). If violated, you cannot use linear models. The \\(x_i\\)s are exact. The variance of the errors is constant (homoskedasticity). If violated, use weighted least squares. The errors are uncorrelated. If violated, use generalized least squares (pre-whitening) The errors are jointly normally distributed. If violated, use robust methods or variable transformation. 2.2 Geometric interpretation The respone variable is a vector in \\(\\mathbb{R}^n\\). The fitted values \\(\\hat{Y} = X \\hat{\\beta}\\) span are a \\(p\\) dimensional subspace in \\(\\mathbb{R}^n\\) when varying \\(\\hat{\\beta}\\). The least square solution is the solution for \\(\\hat{\\beta}\\) such that the eucledian distance between \\(Y\\) and \\(\\hat{Y}\\) is minimal. Note that the true \\(\\beta\\) (i.e. the one that from the data-generatling process) corresponds to the solution \\(\\mathbb{E}[Y]\\), which also lays within the subspace, but due to the error introduced, in the process, the two do not coincide. Hence, \\(X\\beta\\) is the orthogonal projection of \\(Y\\) onto \\(\\mathcal{X}\\). Note that the vector [1, …, 1] is contained in the \\(p\\)-dimensional subspace if our model has an interecept. You can see that if you imagine we only had a model with an intercept. The design matrix would then just be such a [1, …, 1] vector. Setting \\(\\beta\\) equal to any number will yield a scaled unit vector as a prediction, i.e. just \\(\\beta \\times [1, ..., 1]\\). This subspace contains the vector [1, …, 1], since you can connect any two points in the space with a scaled version of this vector. Therefore, the residuals are orthogonal to the vector [1, …, 1] , as one can see from the picture. This means \\(r&#39;\\textbf{1} = 0\\) and hence \\[ \\frac{1}{n} \\sum r&#39;\\textbf{1} = \\bar{r} = 0\\] I.e. the residuals are zero on average. 2.3 Hat matrix We can also look at another projection or mapping. \\[ P: \\; Y \\rightarrow \\hat{Y}\\] Using \\[X \\hat{\\beta} = X(X&#39;X)^{-1}X&#39;y = Py \\;\\; \\text{with} \\;\\; P = X(X&#39;X)^{-1}X&#39;\\] We call \\(P\\) the hat matrix. The aforementioned projection is an orthogonal projection since: \\(P\\) is symmetric: \\(P = P&#39;\\) \\(P\\) is idempotent: \\(P^2 = P\\) The trace of \\(P_{p\\times p}\\) is equal to \\(p\\) The last one can be shown as follows: \\[ tr(X(X&#39;X)^{-1}X&#39;) = tr((X&#39;X)^{-1}X&#39;X) = tr(I_{p \\times p}) = p\\] 2.4 Multiple regression vs. simple regression Doing many simple regressions instead of one multiple linear regression does not give the same result in general. Let’s consider two predictors. If they are positively correlated and we omit one, we will overestimate the effect of the remaining predictor. If they are negatively correlated, we will underestimate it. Simple regressions only yield the same result as multiple linear regression if the variables are not correlated. This can be seen well algebraically. Orthogonal predictors means \\[ (X&#39;X) = diag(\\sum\\limits_{i = 1}^n {x_{1i}}^2, ..., \\sum\\limits_{i = 1}^n{x_{pi}}^2)\\] Which yields \\[ \\beta_j = \\sum\\limits_{i = 1}^n x_{ij} Y_i / \\sum\\limits_{i = 1}^n {x_{ij}}^2\\] So we can easily see that the coefficient \\(\\beta_j\\) only depends on the j-th predictor. 2.5 Properties 2.6 Tests We know that, if \\(\\epsilon \\sim \\mathcal{N}_p(0, \\sigma^2)\\) the distribution of our coefficients is as follows: \\(\\hat{\\beta} \\sim \\mathcal{N}_p(\\beta, \\sigma^2(X&#39;X)^{-1})\\). Hence, we know that the standardized versions of the coefficients, i.e. \\[\\frac{\\hat{\\beta}_j - \\beta_j}{\\sigma^2(X&#39;X)_{jj}} \\sim \\mathcal{N}_p(0, 1)\\] If our Null Hypothesis is \\(\\beta_j = 0\\) the above becomes \\[\\frac{\\hat{\\beta}_j}{\\sigma^2(X&#39;X)_{jj}} \\sim \\mathcal{N}_p(0, 1)\\] We can plug in our estimate for \\(\\sigma^2\\), \\[\\frac{\\hat{\\beta}_j}{\\hat{\\sigma}^2(X&#39;X)_{jj}} \\sim \\mathcal{t}_{n-p}(0, 1)\\] Individual tests measure the significance of a variable given all the other variables in the model. If you want to look at whether a group of variables is significant, i.e. your Null Hypothesis is \\(H_0: \\beta_1 = ... = \\beta_k = 0\\) If you recall the picture from above, we can see that we can decompose the total sum of squares into \\[ SST = SSR + SSE\\] The F-ratio is nothing else than the share of variance we can explain with our mode, scaled by some degree of freedom related stuff. \\[F = \\frac{\\|\\hat{Y} - \\bar{Y}\\|^2 / (p-1)}{\\|Y - \\bar{Y}\\|^2/(n-p)}\\] For the goodness of fit measure \\(R^2\\) drop those scaling factors. \\[R^2 = \\frac{\\|\\hat{Y} - \\bar{Y}\\|^2}{\\|Y - \\bar{Y}\\|^2}\\] Similarly to t-tests derived above, we can construct confidence intervals. \\[\\hat{\\beta}_j ± \\sqrt{\\hat{\\sigma}^2 {(X&#39;X)_{jj}}^{-1}} * t_{n-p, 1-\\alpha/2}\\] 2.7 Diagnostics The Tuskey-Anscombe plot is a good tool to detect model violations. It takes advantage of the fact that the residuals are never correlated with the predictors and we can display the response versus the residuals also for \\(p&gt;1\\), which is not possible if we plot against the predictors (at least not in one plot). Ideally, the points fluctuate randomly around the horizontal zero-line. The Normal-plot helps to identify violations of the normality assumption. You can also plot the residuals against time or calculate auto-correlation to detect serial dependency in the residuals. 2.8 Generalized least squares Applies to the situation where \\[ Y = X\\beta + \\epsilon \\;\\;\\; |\\; \\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\] Where \\(\\Sigma\\) takes a more general form than previously: The errors can be correlated (so \\(\\Sigma\\) does not have to be diagonal matrix) The errors do not need to have identical variance (homoscedasticity) We can transform the data with a matrix \\(A\\) such that we end up with \\[ \\tilde{Y} = \\tilde{X}\\beta + \\tilde{\\epsilon} \\;\\;\\; |\\; \\tilde{\\epsilon} \\sim \\mathcal{N}(0, \\bf{1_{p \\times p}})\\] The following must hold \\[ \\begin{split} Cov(A \\epsilon) &amp;:= \\bf{1_{n \\times n}} \\\\ \\mathbb{E}[A\\epsilon \\epsilon&#39; A&#39;] &amp; = A \\Sigma A&#39; \\;\\; \\;\\;\\;\\;| \\;\\text{decomposing} \\; \\Sigma \\; \\text{e.g. with Cholesky} \\;\\; \\\\ &amp;= ACC&#39;A&#39; \\;\\; | \\text{now choosing}\\; A = C^{-1} \\\\ &amp; = \\bf{1_{n \\times n}} \\\\ \\end{split} \\] Hence, we transform the data with the inverse of the (for example) lower triangular matrix resulting from the Cholesky decomposition and we will get uncorrelated errors. A special case is when we have uncorrelated errors, but heteroskedasticity. Choose the weight \\(\\tilde{w}\\) such that \\(Var(\\tilde{w}_i x_i) = 1\\). If \\(Var(x_i) = {\\sigma_i}^2\\) set \\(\\tilde{w}_i = 1 / {\\sigma_i}\\) to fulfill the above equation. The optimization problem becomes \\[ \\begin{split} \\hat{\\beta} &amp;= \\arg\\min\\limits_{\\beta} \\sum {\\tilde{r}_i}^2 = \\arg\\min\\limits_{\\beta} \\sum (\\tilde{w}_i r_i)^2 \\\\ &amp;=\\arg\\min\\limits_{\\beta} \\sum w {r_i}^2 = \\arg\\min\\limits_{\\beta} \\sum w {(y_i - {x_i}&#39;\\beta)}^2 \\end{split} \\] So \\(w_i = \\tilde{w_i}^2 = 1/ \\sigma_i^2\\) is just a weighted regression. 2.9 Model Selection If we have a set of \\(\\mathcal{M}\\) predictors, which ones should we use in our model? Every variable we add to the model adds a bit of variability to the estimated hyper plane. Hence, having irrelevant predictors in the model is not desirable. In fact, we face a bias-variance trade-off, which we can derive more formally. The mean square error of a model is \\[ \\begin{split} MSE(x) &amp;= n^{-1} \\sum\\limits_{i = 1}^n \\mathbb{E}[(m(x_i) - \\hat{m}(x_i))^2] \\\\ &amp;= n^{-1} \\sum\\limits_{i = 1}^n \\mathbb{E}[\\hat{m}(x_i) - (m(x_i) )^2] \\\\ \\end{split} \\] Due to the fundamental law \\(Var(x) = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\), we can rewrite this as \\[ \\begin{split} MSE(x) &amp;= n^{-1} \\sum\\limits_{i = 1}^n Var[\\hat{m}(x_i) - m(x_i)] + n^{-1} \\sum\\limits_{i = 1}^n \\mathbb{E}[\\hat{m}(x_i) - m(x_i)]^2 \\\\ \\end{split} \\] Since \\(m(x)\\) is constant but unknown, it drops out of the variance expression. The bias is the expected deviation of our estimate from the true value, so \\(\\mathbb{E}[(\\hat{m}(x_i) - m(x_i))]^2\\) is the squared bias. We can plug in our the regression model for our estimator \\(\\hat{m}(x)\\) end we end up with \\[ \\begin{split} MSE(x) &amp;= n^{-1} \\sum\\limits_{i = 1}^n Var[\\sum\\limits_{j = 1}^q \\hat{\\beta}_j x_{ij}] + n^{-1} \\sum\\limits_{i = 1}^n \\mathbb{E}[\\sum\\limits_{j = 1}^q \\hat{\\beta}_j x_{ij} - m(x_i)]^2 \\\\ &amp;= Var(\\cdot) + bias(\\cdot)^2 \\end{split} \\] The bias is decreasing exponentially with growing \\(q\\), the variance decreases linearly. This means there will be a unique minimum. 2.9.1 Mallow’s cp statistic The problem with the above equation is that since \\(m(x)\\) is unknown, there is no way we can estimate (and hence optimize) the mean squared error. However, you can estimate the mean squared error indirectly by the following quantity \\[ n^{-1} SSE(\\mathcal{M}) - \\hat{\\sigma}^2 + 2 \\hat{\\sigma}^2|\\mathcal{M}| / n\\] Since \\(\\hat{\\sigma}^2\\) and \\(n\\) are constants for each sub-model, we can also optimize the well-known cp statistic \\[\\mathcal{C}_p = \\frac{SSE(\\mathcal{M})}{\\hat{\\sigma}^2} - n + 2 |\\mathcal{M}| \\] 2.9.2 Search strategies Note that there are \\(2^p-1\\) possible models (every variable is in the model or not, and the empty model is not considered). You can use forward or backward search. Forward Search Start with the model that contains only an intercept. Add the predictor variable to the model that decreases the residual sum of squares the most. Proceed until a large number of predictors is chosen. Select the model that has the lowest cp statistic. Backward selection Start with the full model that contains all variables. Remove the variable from the model such that the residual sum of squares increases the least. Continue until you reached the empty model Among those models, select the one that has the smallest cp score. Note that the algorithm is greedy so it is for example not possible to add a variable to the model once it has been removed in the backward selection. Also, backward selection is typically a bit better, but also way more expensive. Note that for \\(p&gt;n\\), backward selection is not possible. Also, p-values are not honest anymore. "],
["non-parametric-density-estimation.html", "Chapter 3 Non-parametric Density Estimation 3.1 The Histogramm 3.2 Kernels 3.3 The Bandwidth 3.4 Bringing it all together 3.5 Other Density Estimators 3.6 Higher Dimensions", " Chapter 3 Non-parametric Density Estimation 3.1 The Histogramm The simplest density estimator is the histogram. The drawback is that we need to specify two parameters. The origin \\(x_0\\) and the bandwidth \\(h\\). At least the former is highly arbitrary and both affect the resulting histogram remarkably. data_frame(x = rnorm(1000)) %&gt;% ggplot(aes(x = x)) + geom_histogram(bins = 30, center = 0) # center is what we called x_0 3.2 Kernels 3.2.1 The naive Estimator We can compute the relative frequency of observations falling into some region and use these frequencies as estimators for the probabilities. Let’s consider some example data distributed uniformly at random between \\(-0.5\\) and \\(+0.5\\). Let us try to estimate the densities via the relative frequencies. We consider three points in x: \\(-0.5, 0\\) and \\(+0.5\\). We set \\(h\\) to 1/2 for now. For x equal zero, we know that all points are comprised in the range, so the relative frequency is one, for the two other x values, it is a half each. We can interpolate and then the a triangle. x &lt;- runif(10000, min = -1/2, max = 1/2) dens &lt;- density(x, kernel = &quot;rectangular&quot;, bw = 1/2) eval_one &lt;- function(e_eval_one, x, h) { 1/(2*h)* mean(e_eval_one + h &gt; x &amp; e_eval_one - h &lt; x) } naive_density &lt;- function(x, h, x_eval) { data_frame( x_eval = x_eval, y_est = map_dbl(x_eval, eval_one, x = x, h = h), h = h ) } one_est &lt;- naive_density(x, 1/2, seq(-1, 1, length.out = 3)) ggplot(one_est, aes(x = x_eval, y = y_est)) + geom_path() We don’t need to normalize anything since the area has size one already, which we need for it to be a density. However, if we use a larger \\(h\\), for example \\(h = 1\\), we need to divide by \\(2\\) to get an area of size one since at all three points, the relative frequency is one and it becomes zero only when the absolute value of x is larger than 3/2. Hence, we found the formula to obtain densities. \\[ f(x) = \\frac{1}{2h}\\mathbb{P}[x-h&lt; X &lt; x+h] \\;\\;\\; \\text{for}\\;\\; h \\rightarrow 0\\] Which translates into frequencies for our estimator \\[ \\hat{f}(x) = \\frac{1}{2hn}\\#(X_i \\in [x-h, x+h))\\] This is quite wrong, as we know the true distribution is uniform between minus one and one. However, as you can see by playing around with the shiny app further down the page, it seems that with \\(h \\rightarrow \\infty\\), we are approaching the true distribution. The nice thing about this estimator is that there is no need to set the tuning parameter \\(x_0\\) anymore. You can represent the naive kernel differently, namely as a kernel. We have \\[\\begin{equation} \\begin{split} f(x) = &amp; \\frac{1}{2nh}\\#\\{X_i \\in [x- d, x+ d)\\} \\\\ = &amp; \\frac{1}{2nh} \\sum\\limits_{i = 1}^n 1_{[x_i \\in [x- d, x+ d)]} \\\\ = &amp; \\frac{1}{2nh} \\sum\\limits_{i = 1}^n 1_{[(x_i - x)/h \\in [-1, 1)]} \\\\ = &amp; \\frac{1}{2nh} \\sum\\limits_{i = 1}^n 1_{[(x - x_i)/h \\in [-1, 1)]} | \\text{You can flip signs} \\\\ = &amp; \\frac{1}{2nh} \\sum\\limits_{i = 1}^n w&#39;\\big(\\frac{x -x_i}{h}\\big) \\\\ \\end{split} \\end{equation}\\] \\[\\begin{equation} w&#39;\\big(\\frac{x - x_i}{h}\\big) = \\begin{cases} 1 \\;\\;\\ \\text{if} \\;\\;\\ | \\frac{x - x_i}{h} | &gt; 1 \\\\ 0 \\;\\;\\ \\text{else}. \\end{cases} \\end{equation}\\] Where we can collapse \\(2\\) and our weight function \\(w&#39;(x)\\), so we end up with the following. \\[\\begin{equation} \\begin{split} f(x) = &amp; \\frac{1}{nh} \\sum\\limits_{i = 1}^n w\\big(\\frac{x - x_i}{h}\\big) \\\\ \\end{split} \\end{equation}\\] \\[\\begin{equation} w\\big(\\frac{x - x_i}{h}\\big) = \\begin{cases} \\frac{1}{2} \\;\\;\\ \\text{if} \\;\\;\\ | \\frac{x - x_i}{h} | &gt; 1 \\\\ 0 \\;\\;\\ \\text{else}. \\end{cases} \\end{equation}\\] 3.2.2 Other Kernels The function \\(w\\) from above is an example of a kernel function since it satisfies the condition of a kernel, which are integrates to one, \\(\\int\\limits_{-\\infty}^\\infty K(X)dx = 1\\) is symmetric, \\(K(x) = K(-X)\\) is strictly positive, \\(K(X)\\geqslant 0\\) \\(w(\\cdot)\\) is the rectangular kernel, which becomes obvious when plotting it. rectangular &lt;- function(x) { if_else(abs(x) &lt; 1/2, 1, 0) } normal &lt;- function(x) { (2*pi)^(-1/2)*exp(-x^2/2) } cosine &lt;- function(x) { if_else(abs(x) &lt; 1, pi / 4 * cos(pi / 2 * x), 0) } ggplot(data_frame(x = 0), aes(x = x)) + stat_function(fun = rectangular, aes(color = &quot;rectangular&quot;)) + stat_function(fun = normal, aes(color = &quot;normal&quot;)) + stat_function(fun = cosine, aes(color = &quot;cosine&quot;)) + scale_x_continuous(limits = c(-2, 2)) It is piece-wise constant, since every point is either in or out of the interval. The density estimate is not smooth since the kernel is not smooth. \\(\\hat{f}(x)\\) inherits also other properties from the kernel, e.g. it is strictly positive if the kernel is strictly positive and it integrates to one if the kernel does so. Therefore, to obtain a smooth density estimate, we need to choose a smooth kernel. The Gaussian kernel seems to be a natural choice. Instead of giving an observation a weight of either one or zero, values in between are also possible, depending on the distance of the point relative to the point for which we want to obtain a density estimate and depending on \\(h\\). Some kernels such as the normal kernel have the drawback that they their weights are never getting to zero, which implies computational cost at limited gain. We can instead choose a kernel like the cosine kernel, which also smooth but reaches zero relatively quickly, which speeds up calculations. 3.3 The Bandwidth More important than the kernel is the bandwidth. A small bandwidth yields a very wiggly function (since only the points in the very close neighborhood are given a significant weight), a large bandwidth will produce a function that is varying slowly as a function of x. The bandwidth is closely related to the bias-variance trade-off. A large bandwidth means a large bias and a low variance, a small bandwidth the opposite. For the extreme cases of h, we have: For a very large bandwidth, the curve estimate becomes the kernel. For a very small bandwidth, the deity takes the shape of many small kernels. Around each point, a kernel with density \\(1/n\\) forms. Hence, they integrate to one. Points may be close to each other, so the mini kernels overlap. We can also use locally varying bandwidths. In regions where data is sparse we want to use larger bandwidths. A simple approach is to use the k nearest neighbour of each x as the local bandwidth for each x. Note that with such bandwidths the density will not necessarily integrate to one anymore. We can decompose the mean squared error of an estimator evaluated at a point x similar to the one of a random variable (since the estimator is derived from realizations of random variables). \\[\\begin{equation} \\begin{split} Var(\\hat{f(x)}) &amp; = Var(\\hat{f(x)} - f(x)) \\\\ &amp; = \\mathbb{E}[(\\hat{f(x)} - f(x))^2] - \\mathbb{E}[(\\hat{f(x)} - f(x))]^2 \\\\ &amp; = \\text{MSE(f(x))} - \\text{bias}^2 \\\\ \\text{MSE(f(x))} &amp; = \\text{bias}^2 + Var(\\hat{f(x)}) \\\\ \\end{split} \\end{equation}\\] Instead of minimizing the mean squared error at one point, we minimize the integrated mean square error over the support of x. \\[ \\text{IMSE}(x) = \\int \\text{MSE}(x)dx\\] 3.4 Bringing it all together Here is a shiny app that lets you try different parameters. 3.5 Other Density Estimators In regions where data is sparse, your estimate will typically be close to zero, but it cannot become negative. This constraint is a bit nasty. If you take logs, you no longer have such constraints. The constraint is just that the exp of your estimate has to integrate to one. So you estimate the log of \\(f\\) and exponentiate to get \\(f\\). 3.6 Higher Dimensions Very similar to one-dimensional density estimation, you can estimate densities of multi-dimensional data. The main difference is that the kernel is now multivariate. \\[ \\hat{f}(\\mathbf{x}) = \\frac{1}{nh^d}\\sum\\limits_{j = 1}^n K\\Big(\\frac{\\mathbf{x}- \\mathbf{X_i}}{h}\\Big)\\] A simple multivariate kernel is the product of uni-variate kernels. \\[K(\\mathbf{u}) = \\prod\\limits_{j = 1}^dK_{univ}(u_j)\\] If in addition, the kernel should be radially symmetric (which means a function that is constant in circles around the origin, which also means that the the value the kernel takes only depends on the distance to the origin), it can be shown that the only kernel that meets this two conditions, i.e. that is is a product kernel is radially symmetric is the Gaussian kernel \\(K(\\mathbf{u}) = ce^{-\\frac{1}{2}u_1^2} * ce^{-\\frac{1}{2}u_2^2} ... = ce^{-\\frac{1}{2}\\mathbf{u}&#39;\\mathbf{u}} = ce^{-\\frac{1}{2}\\|u\\|^2}\\) 3.6.1 The Curse of Dimensionality The curse of dimensionality is the phenomena that data becomes sparse if the number of dimensions increase. That means that for every other dimension we add, we must add over-proportionally many data points to keep the distance between the data points at a reasonable (low) level. "],
["non-parametric-regression.html", "Chapter 4 Non-parametric Regression 4.1 Alternative Interpretation 4.2 The Bandwidth 4.3 Hat Matrix 4.4 Degrees of Freedom 4.5 Inference 4.6 Local Polynomial Estimator", " Chapter 4 Non-parametric Regression We assume a model of the form \\[ E[Y|X = x] = m(x)\\] which is more general than assuming \\(Y_i = m(x_i) + \\epsilon_i\\) since in the former, the noise does not have to be additive. We can use Bayes’ Theorem to deduce such an estimator \\(\\hat{m}\\) using density estimates of our predictor and response variable. In that sense, non-parametric regression really builds on top of non-parametric density estimation. \\[\\begin{equation} \\begin{split} P[Y|X] &amp;= \\frac{\\mathbb{P}[X|Y]}{\\mathbb{P}[Y]}\\mathbb{P}[Y] \\\\ f_{Y|X} &amp; = \\frac{f_{X|Y}}{f_X}f_Y \\\\ f_{Y|X} &amp; = \\frac{f_{X, Y}}{f_X} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{|} \\times y\\\\ f_{Y|X}y &amp; = \\frac{f_{X, Y}}{f_X}y \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{| taking the integral}\\\\ E[X|Y] &amp; = \\int \\frac{f_{X, Y}}{f_X}ydy \\end{split} \\end{equation}\\] Whereas for \\(\\hat{f}_{X, Y}\\), we can use a product kernel. This formula simplifies quite a bit and yields the Nadaraja-Watson Kernel, which is essentially just a weighted mean of response values. \\[\\hat{m}(x) = \\frac{\\sum\\limits_{i = 1}^n K\\Big(\\frac{x-X_i}{h}\\Big)Y_i}{\\sum\\limits_{i = 1}^nK\\Big(\\frac{x-X_i}{h}\\Big)} = \\frac{\\sum\\mathcal{w}(x)_i Y_i}{\\sum\\mathcal{w}(x)_i} = \\sum\\limits_{i = 1}^n \\tilde{\\mathcal{w}}(x)_i Y_i = \\tilde{\\mathbf{w}}(x)&#39;\\mathbf{Y}\\] The weights \\(\\tilde{\\mathcal{w}}_i\\) are normalized weights, i.e. \\(\\tilde{\\mathcal{w}}_i = \\mathcal{w}_i / \\sum\\limits_{k = 1}^n \\mathcal{w}_k\\) 4.1 Alternative Interpretation It can be easily shown that the solution corresponds to the following minimization problem: \\[ m(x) = \\arg\\min\\limits_{m_x} \\sum\\limits_{i = 1}^nK\\Big(\\frac{x-X_i}{h}\\Big)\\big(Y_i-m_x\\big)^2 \\] We can interpret this as a weighted (local) regression. For a given \\(x\\), search for the best local constant \\(m_x\\) that minimizes the weighted residual sum of squares the most. Residuals of data points close to \\(x\\) get a high weight in this sum (via the kernel). 4.2 The Bandwidth The bandwidth parameter \\(h\\) has a similar role as in non-parametric density estimation. Large \\(h\\) implies very smooth functions, low variance, high bias. Small \\(h\\) on the other hand imply (more) erratic functions, high variance, low bias. An interesting case is \\(h \\rightarrow \\infty\\), for which all weights become equal. This corresponds to an estimator \\(m(x) = \\bar{y}\\), with one degree of freedom (see below). 4.3 Hat Matrix As in chapter 1, we can also obtain a smoother matrix \\(S\\) that maps the observed response values to the fitted values. From above, we have: \\[\\hat{y}_i = \\tilde{\\mathbf{w}}(x_i)&#39;\\mathbf{Y}\\] \\[\\mathbf{\\hat{Y}} = \\mathcal{S} \\mathbf{Y} = \\begin{pmatrix}\\hat{y}_1 \\\\\\vdots\\\\\\hat{y}_n\\end{pmatrix} = \\begin{pmatrix}\\tilde{\\mathbf{w}}(x_1)&#39;\\\\\\vdots\\\\\\tilde{\\mathbf{w}}(x_n)&#39;\\end{pmatrix} \\times \\begin{pmatrix}y_1 \\\\\\vdots\\\\y_n\\end{pmatrix}\\] Where \\(\\tilde{\\mathbf{w}}(x_1)&#39;\\) is a row vector of length \\(n\\) with the normalized kernel weights. ** here: How to compute S and why** 4.4 Degrees of Freedom Note that we need a different measurement of degrees of freedom than the one we used in the parametric case in chapter 1, where the degrees of freedom simply corresponded to the number of parameters used. As this is non-parametric regression, we don’t have parameters and hence cannot sum up the number of parameters to calculate the degree of freedom. Recall from chapter 1 that the trace of the smoothing matrix was equal to the number of parameters used: \\[tr(P) = \\textbf{tr}(X(X&#39;X)^{-1}X&#39;) = \\textbf{tr}((X&#39;X)^{-1}X&#39;X) = \\textbf{tr}(I_{p\\times p}) = p\\] Hence, we can generalize the concept of degrees of freedom from number of parameters to the trace of the smoother matrix. For regression, the two coincide, for non-parametric methods, we get an estimate of the degrees of freedom only via the trace. There is a one-to-one relationship between the degrees of freedom and the bandwidth parameter \\(h\\), which we can show in a diagram: This can be derived from the two extreme cases: \\(h \\rightarrow \\infty\\) means all weights are equal, which means for each data point \\(x_i\\), the fitted value \\(\\hat{m}(x_i)\\) is just the grand mean of \\(y\\). This corresponds to one degree of freedom. \\[\\hat{m}(x) = \\bar{y} \\] \\(h \\rightarrow 0\\). If we assume \\(n\\) distinct, equi-spaced \\(x_i\\) values, then the fitted values in the neighborhood of \\(\\hat{m}(x_i)\\) are just the observed response value \\(y_i\\), since all weights for points other than \\(y_i\\) are zero. Or to be more precise: If the distance between neighboring points is \\(\\eta\\), then, for \\(h \\leqslant \\eta\\) : \\[ \\hat{m}(x) = y_{i^*} \\;\\;\\ \\text{with} \\;\\; i^* = \\arg\\min\\limits_{0 \\leqslant i \\leqslant n} |x - x_i| \\] This corresponds to \\(n\\) degrees of freedom. We can interpolate between these two extreme cases in a monotone way. 4.4.1 Applications There are two main applications for the degree of freedom: As a tuning parameter. For every \\(h\\), we can find the corresponding degrees of freedom or more interestingly - for every(desired) degree of freedom, we can find an \\(h\\). Instead of varying \\(h\\) directly, we can vary the degrees of freedom, which are directly comparable accross different techniques. We can compare OLS with different kernel smoothers, splines etc. (see below), which is not possible with \\(h\\) alone. For inference. For unbiased estimators (such as the variance of the residuals), we need the degrees of freedom (see below). 4.5 Inference As with parametric regression, we want to do inference. Not on any parameters (since there are none), but on the fitted values. First and foremost, we want to obtain confidence intervals for the regression line, that is, obtaining point-wise lower and upper bounds of the confidence internal \\[ I_i = \\hat{m}(x_i) ± 1.96 *\\hat{sd}(\\hat{m}(x)) \\] Which holds because the fitted value is asymtotically normally distributed. We know already how to obtain \\(\\hat{m}(x)\\), now we need to find the standard deviation of the fitted values. From the fundamental equation \\(Cov(\\mathbf{A}x) = \\mathbf{A} Cov(x) \\mathbf{A}&#39;\\), we get \\[ \\text{Cov}(\\hat{m}(x)) = \\text{Cov}(\\mathcal{S}\\mathbf{y}) = \\mathcal{S} \\text{Cov}(y) \\mathcal{S}&#39; = \\sigma_\\epsilon^2\\mathcal{SS}&#39; \\;\\;\\;\\;\\;|\\;\\; \\text{since Cov}(y) = \\text{Cov}(\\epsilon)\\] Which is a \\(p\\times p\\) matrix. For a specific data point \\(x_i\\), we have \\[ \\text{Var}(\\hat{m}(x_i))= \\text{Cov}(\\hat{m}(x_i), \\hat{m}(x_i)) = \\sigma_\\epsilon^2 (\\mathcal{S} \\mathcal{S}&#39;)_{ii} \\] Now, we only need to estimate \\(\\sigma^2_\\epsilon\\). Using the generalized method to compute degrees of freedom, we can use the following estimator: \\[ \\hat{\\sigma}_\\epsilon^2 = \\frac{1}{n-\\textbf{tr}(\\mathcal{S})} \\sum\\limits_{i = 1}^n (Y_i - \\hat{m}(x_i))^2\\] 4.6 Local Polynomial Estimator Expanding on the formulation of the Nadaraya-Watson Kernel as a weighted least square problem, we can use centered polynomials to approximate \\(y_i\\) instead of a local constant \\(\\hat{m}_x\\). That is, computing our estimator as \\[\\hat{m}(x) = \\arg\\min\\limits_{m_x} \\sum\\limits_{i = 1}^nK\\Big(\\frac{x-X_i}{h}\\Big) \\big(Y_i-\\beta_0 + \\beta_1 (x_i -x) + ... \\beta_{p-1} (x_i-x)^{p-1}\\big)^2 \\] This has the advantage of yielding a lolcal linear regression curve at the borders, whereas the Nadaraya-Watson Kernel yields local constant regression curves in these regions (since at the border, no new points are are taken into account). Another advantage is that we can find derivaties of the regression estimate at each point easily. "],
["cross-validation.html", "Chapter 5 Cross Validation 5.1 Motivation and Core Idea 5.2 Loss Function 5.3 Implementations 5.4 K-fold Cross-Validation 5.5 Properties of the different schemes 5.6 Shortcuts for (some) linear fitting operators 5.7 Isolation of each cross validation sample 5.8 Examples with R", " Chapter 5 Cross Validation 5.1 Motivation and Core Idea Cross-validation is a tool for estimating the performance of an algorithm on new data points, the so-called the generalization error. An estimate of the generalization allows us to do two important things: Tuning the parameters of a statistical technique. Comparing statistical techniques with regard to their accuracy. If we use the training data to evaluate the performance of an algorithm, this estimate will be over-optimistic because an estimator is usually obtained by minimizing some sort of error in the training data. Therefore, we use a separate data pool, called the test data to evaluate the performance out of sample. Consider the regression function estimate \\(\\hat{m}\\) based on a sample \\((X_1, ..., X_n.)\\). By increasing the number of parameters in the model and by allowing for interactions between them, we can make the regression model fitting arbitrarily well to the data. However, such an extremely complex model will not perform as well with new data, that is, will not generalize well to other data sets, since we essentially modeled also a lot of noise. To estimate the performance of an algorithm on a new sample, we introduce the following notation: \\[ l^{-1}\\sum\\limits_{i = 1}^l\\rho(Y_{new, i}, \\hat{g}(X_{new, i}))\\] Where \\(\\rho\\) is a loss function to be evaluated on the new data points \\((Y_{new, 1}, ..., Y_{new, l})\\) and the prediction made for \\((X_{new, 1}, ..., X_{new, l})\\) with the function \\(\\hat{g}\\), which was estimated from the training data \\((X_1, ..., X_n)\\). When \\(l\\) gets large, this approximates the test error \\[\\mathbb{E}_{(X_{new}, Y_{new})}[\\rho(Y_{new}, \\hat{m}(X_{new})]\\] which is still a function of the training data (since it is conditional on the training data). Note that the test error is not the same as the generalization error. The latter is an expectation over both the training and the test data. The typical relationship between the test error and the training error is depicted in the figure below. The optimal model complexity is at around \\(20\\) degrees of freedom. With more degrees of freedom, the test set error increases again. We start to model noise. This is also called overfitting. 5.2 Loss Function Depending on the application, one can imagine different loss-functions. For example the squared deviance from the true value is often used, i.e. \\[n^{-1}\\sum\\limits_{i = 1}^n\\rho(Y_i, \\hat{m}(X_i)) = n^{-1}\\sum\\limits_{i = 1}^n(Y_i - \\hat{m}(X_i))^2\\] Hence, larger deviance is penalized over-proportionally. For classification, one often uses the zero-one error, i.e. \\[n^{-1}\\sum\\limits_{i = 1}^n1_{\\hat{m}(X_i) \\neq Y_i}\\] However, it might also be appropriate to use asymmetric loss functions if false negatives are worse than false positives (i.e. for cancer tests). 5.3 Implementations There are different ways to do cross validation while adhering to the principles introduced above. 5.3.1 Leave-one-out Use all but one data point to construct a model and predict on the remaining data point. Do that \\(n\\) times until all \\(n\\) points were used for prediction once. Compute the test error as an average over all n errors measured, i.e \\[n^{-1}\\sum\\limits_{i = 1}^n \\rho{(Y_{i}, \\hat{m}_{n-1}^{-i}(X_i)})\\] And use that as an approximation of the generalization error. 5.4 K-fold Cross-Validation This method is best explained with a picture. Here, one splits the data set into k equally (or as equal as possible) sized folds. Then, the idea is to use all \\(k-1\\) folds to build a model and the remaining fold to evaluate the model. Then, we average the \\(k\\) estimates of the generalization error. Or in mathematical notation: \\[K^{-1} \\sum\\limits_{k = 1}^K |B_k|^{-1} \\sum\\limits_{i \\in B_k}\\rho({Y_{i}, \\hat{m}^{-B_k}_{n-|B_k|}(X_i))}\\] Note that leave-one out cv is the same as k-fold cross validation with \\(=n\\). 5.4.1 Random Division into test and training data set The problem of K-fold cross-validation is that it depends on one realization of the split into k folds. Instead, we can generalize leave-one-out to leave-d-out. That means, we remove \\(d\\) observations from our initial data, apply our estimation procedure and evaluate on the \\(d\\) observations. \\[\\hat{\\theta}^{-C_k}_{n-k} \\;\\;\\; \\text{for all possible subsets}\\;\\; C_k, \\;\\; k=1, ..., {\\binom{n}{d}}\\] The generalization error can be estimated with \\[{\\binom{n}{d}}^{-1}\\sum\\limits_{k = 1}^{\\binom{n}{d}} d^{-1}\\sum\\limits_{i \\in C_k} \\rho(Y_i, \\hat{m}^{-C_k}_{n-d}(X_i))\\] For \\(d &gt; 3\\), the computational burden becomes immense. For that reason, instead of considering all \\({\\binom{n}{d}}\\) sets, we can uniformly draw \\(B\\) sets (\\(C_1^*, ... C_B^*\\)) from \\(C_1, ..., C_{\\binom{n}{d}}\\) without replacement. For \\(B=\\binom{n}{d}\\), we obviously get the full leave-d-out solution. The computational cost for computing such an approximation to the leave-d-out is linear in \\(B\\) (since evaluating is almost for free). For leave-one-out, the cost is linear in \\(n\\) in the same way. Hence, the stochastic approximation for leave-d-out can be even smaller than for leave-one-out if \\(B &lt; n\\). 5.5 Properties of the different schemes We can try to say something about both bias and variance of the cv schemes introduced above. leave-one-out is an asymptotically unbiased estimator for the generalization error and the true prediction. However, we use a sample size \\(n-1\\) instead of \\(n\\), which causes a slight bias (meaning we have less data as we do in a real world scenario, which most likely makes the CV score a tiny little bit worse than it should be). Because the training sets are very similar to each other the leave-one-out scheme has a large variance. leave-d-out has a higher bias than leave-one-out because the sample size is even smaller than \\(n-1\\) (for \\(d&gt;1\\)). However, since we aggregate over more (\\(\\binom{n}{d}\\) instead of \\(n\\)) cv scores, which can be shown to decrease the variance of the final cv estimator. k-fold cv has a higher bias than both leave one out. 5.6 Shortcuts for (some) linear fitting operators Leave-one-out cv score for some linear fitting procedures such as least squares or smoothing spline can be computed via a shortcut when our loss function is \\(\\rho(y, \\hat{y}) = |y-\\hat{y}|^2\\). In particular, we can compute the estimator for such a linear fitting procedure once, compute the linear fitting operator \\(S\\), which satisfies \\(\\mathbf{\\hat{Y}} = \\mathbf{SY}\\) and plug it in this formula: \\[n^{-1}\\sum\\limits_{i = 1}^n \\Bigg(\\frac{Y_i - \\hat{m}(X_i)}{1-S_{ii}}\\Bigg)^2\\] Computing \\(\\mathbf{S}\\) requires \\(O(n)\\) operations (see exercises). Historically, it has been computationally easier to compute the trace of \\(\\mathbf{S}\\) so there is also a quantity called generalized cross validation (which is a misleading terminology), which coincides with the formula above in certain cases. 5.7 Isolation of each cross validation sample Note that it is very important to re-estimate all parameters relevant for the cross validation within every sample. Otherwise, our estimates will be too optimistic. This problem was mentioned in the exercise series 10. Specifically, the taks was to train a tree algorithm. For every sample, we had to determine the optimal cp individually (via cross validation in the cross validation so to speak). Why not using the cp estimated from the whole data set? Because then, we would use information from samples other than the current cv batch and then evaluating the performance of the model based on the cv batch and the global parameter to evaluate our model performance. This contradicts the idea of cross validation and is similar to using the training sample to estimate the performance of a model. 5.8 Examples with R In the remainder of this chapter, we will discuss the two important applications of cross validation: Estimation of the GE and parameter tuning. 5.8.1 Application 1: Estimating the generalization error Key concepts to do CV are Do not split the data, split the indices of the data and work with them if ever possible and subset the data. sample() is your friend. use purrr::map(), purrr::map_df() and friends to “loop” over data. This is preferred over base R lapply / mapply / Map since it has a more coherent argument structure (consistent argument positioning, no strange MoreArgs arguments, pipable etc.) you can simplify structures with purr::flatten_dbl() and friends. Always work with lists, never work with data frames of indices. The reason is that data frames have structural constrains (all columns must have same number of elements) that are not natural in some situations. For example, out-of-bootstrap cv does have the same number of observations in the training set, but not in the test set. In conjunction with sample(), you can use purrr::rerun or replicate to create lists of indices. To get the other part of the sample, you can use negative indices or setdiff(). use helper function to solve “the small problems in the big problem”. Let’s first declare our functions. library(&quot;purrr&quot;) data(ozone, package = &quot;gss&quot;) #&#39; Estimate the generalization error of a ridge regression #&#39; @param test Test indices. #&#39; @param train Train indices. #&#39; @param .data The data. #&#39; @param lamda The lamda parameter for the ridge regression. ge_ridge &lt;- function(test, train, .data, lambda) { fit &lt;- MASS::lm.ridge(upo3~., lambda = lambda, data = .data[train,]) pred &lt;- as.matrix(cbind(1, .data[test, -1])) %*% coef(fit) mean((pred - .data[test,]$upo3)^2) } ## ............................................................................ ## functions to return list with indices #### get_boostrap_mat &lt;- function(B, n) { rerun(B, sample(n, n, replace = TRUE)) } get_all_mat &lt;- function(B, n) { rerun(B, 1:n) } get_complement &lt;- function(mat, n){ map(mat, ~setdiff(1:n, .x)) } get_k_fold_test &lt;- function(k, n) { step &lt;- trunc(n/k) current &lt;- list() for (i in (0:(k-1) * step + 1)) { current &lt;- append( current, list(i:(i+step-1)) ) } current } Now, let us apply the functions for three cv schemes to estimate the generalization error. ## ............................................................................ ## boostrap #### # use boostrap sample to train, use all to test n &lt;- nrow(ozone) train &lt;- get_boostrap_mat(10, n) test &lt;- get_all_mat(10, n) bs &lt;- map2_dbl(test, train, ge_ridge, .data = ozone, lambda = 5) ## ............................................................................ ## 10-fold #### test &lt;- get_k_fold_test(10, n) train &lt;- map(test, ~setdiff(1:n, .x)) kfold &lt;- map2_dbl(test, train, ge_ridge, .data = ozone, lambda = 5) ## ............................................................................ ## out-of-boostrap #### train &lt;- get_boostrap_mat(10, n) test &lt;- map(test, ~setdiff(1:n, .x)) oob &lt;- map2_dbl(test, train, ge_ridge, .data = ozone, lambda = 5) The results are as follows: out &lt;- cbind(bs, kfold, oob) %&gt;% as_data_frame() %&gt;% gather(key, value) ggplot(out, aes(y = value, x = key)) + geom_boxplot() 5.8.2 Application 2: Parameter Tuning We want to use the scheme k-fold cross validation for parameter tuning with a lasso. We first calculate the test set error for one value of lamda (as we did above). Then, change the value of lamda and recompute the model and then test set error, so that the test set error becomes a function of lamda, as depicted below. Then pick an optimal lamda, e.g. the one with the lowest test error (a bit arbitrary) or one according to some other rule (e.g. pick the least complex model that is within one standard error of the best model). #&#39; Given lambda, compute the test set error with k folds find_lambda_kfold_one &lt;- function(lambda, k, n, .data, ...) { x_test &lt;- get_k_fold_test(k, n) x_train &lt;- get_complement(x_test, n) map2_dbl(x_test, x_train, ge_ridge, lambda = lambda, .data = .data, ...) %&gt;% mean() } #&#39; Given a sequence of lambdas, return the corresponding test set errors find_lambda_kfold &lt;- function(seq, k, .data) { cv &lt;- map_dbl(seq, find_lambda_kfold_one, k = k, n = nrow(.data), .data = .data) results &lt;- data_frame(lambda = seq, cv_score = cv) results } We are almost done. Let us now compute the test set error that we use as an approximation of the generalization error and plot it against different values of lamda. find_lambda_kfold(seq = seq(5, 30, by = 3), 100, ozone) %&gt;% ggplot(aes(x = lambda, y = cv_score)) + geom_line() That looks reasonable. We could improve on that by also showing the distribution of the test set error at various lambadas. This could by done by altering find_lambda_kfold_one() to not return the mean, but also the upper and lower 95% confidence interval. "],
["bootstrap.html", "Chapter 6 Bootstrap 6.1 Motivation 6.2 The Bootstrap Distribution 6.3 Bootstrap Consistency 6.4 Boostrap Confidence Intervals 6.5 Boostrap Estimator of the Generalization Error 6.6 Out-of-Boostrap sample for estimating the GE 6.7 Double Boostrap Confidence Intervals 6.8 Three Versions of Boostrap 6.9 Conclusion", " Chapter 6 Bootstrap Bootstrap can be summarized as “simulating from an estimated model” It is used for inference (confidence intervals / hypothesis testing) It can also be used for estimating the predictive power of a model (similarly to cross validation) via out-of-bootstrap generalization error 6.1 Motivation Consider i.i.d. data. \\[ Z_1, .. Z_n \\sim\\ P \\;\\; with \\; \\;Z_i = (X_i, Y_i)\\] And assume a statistical procedure \\[ \\hat{\\theta} = g(Z_1, ..., Z_n) \\] \\(g(\\cdot)\\) can be a point estimator for a regression coefficient, a non-parametric curve estimator or a generalization error estimator based on one new observation, e.g. \\[ \\hat{\\theta}_{n+1} = g(Z_1, ..., Z_{n}, Z_{new}) = (Y_{new} - m_{Z_1, ..., Z_{n}}(X_{new}))^2 \\] To make inference, we want to know the distribution of \\(\\hat{\\theta}\\). For some cases, we can derive the distribution analytically if we know the distribution \\(P\\). The central limit theorem states that the sum of random variables approximates a normal distribution with \\(n \\rightarrow \\infty\\). Therefore, we know that the an estimator for the mean of the random variables follows the normal distribution. \\[ \\hat{\\theta}_{n} = n^{-1}\\sum x_i \\sim N(\\mu_x, \\sigma_x^2 / n) \\; \\; \\; n \\rightarrow \\infty \\] for any \\(P\\). However, if \\(\\hat{\\theta}\\) does not involve the sum of random variables, and the CLT does not apply, it’s not as straightforward to obtain the distribution of \\(\\hat{\\theta}\\). Also, if \\(P\\) is not the normal distribution, but some other distribution, we can’t find the distribution of \\(\\hat{\\theta}\\) easily. The script mentions the median estimator as an example for which the variance already depends on the density of \\(P\\). Hence, deriving properties of estimators analytically, even the asymptotic ones only, is a pain. Therefore, if we knew \\(P\\), we could simply simulate many times and get the distribution of \\(\\hat{\\theta}\\) this way. That is, draw many \\(({X_i}^*, {Y_i}^*)\\) from that distribution and compute \\(\\hat{\\theta}\\) for each draw. The problem is that we don’t know \\(P\\). But we have a data sample that was generated from \\(P\\). Hence, we can instead take the empirical distribution \\(\\hat{P}\\) that places probability mass of \\(1/n\\) on each observation, draw a sample from this distribution (which is simply drawing uniformly at random from our sample with replacement) and compute our estimate of interest from this sample. \\[ \\hat{\\theta}^{*} = g({Z_1}^{*}, ..., {Z_{new}}^{*})\\] We can do that many times to get an approximate distribution for \\(\\hat{\\theta}\\). A crucial assumption is that \\(\\hat{P}\\) reassembles \\(P\\). If our data is not i.i.d, this may not be the case and hence bootstrapping might be misleading. Below, we can see that i.i.d. sampling (red line) reassembles the true distribution (black line) quite well, whereas biased sampling (blue line) obviously does not. We produce a sample that places higher probability mass on the large (absolute) values. library(&quot;tidyverse&quot;) pop &lt;- data_frame(pop = rnorm(10000) * 1:10000) iid &lt;- sample(pop$pop, 1000) # sample iid # sample non-iid: sample is biased towards high absolute values ind &lt;- rbinom(10000, size = 1, prob = seq(0, 1, length.out = 10000)) not_iid &lt;- pop$pop[as.logical(ind)] # get sample not_iid &lt;- sample(not_iid, 1000) # reduce sample size to 1000 out &lt;- data_frame(iid = iid, not_iid = not_iid) %&gt;% gather(type, value, iid, not_iid) ggplot(out, aes(x = value, color = type)) + geom_density() + geom_density(aes(x = pop, color = NULL), data = pop) We can summarize the bootstrap procedure as follows. draw a bootstrap sample \\({Z_1}^{*}, ..., {Z_{n}}^{*}\\) compute your estimator \\({\\hat{\\theta}}^*\\) based on that sample. repeat the first two steps \\(B\\) times to get bootstrap estimators \\({\\hat{\\theta}_1}^*, ..., {\\hat{\\theta}_B}^*\\) and therefore an estimate of the distribution of \\(\\hat{\\theta}\\). Use the \\(B\\) estimated bootstrap estimators as approximations for the bootstrap expectation, quantiles and so on. \\(\\mathbb{E}[\\hat{\\theta}^*_n] \\approx B^{-1}\\sum\\limits_{j = 1}^n \\hat{\\theta}^{* j}_n\\) 6.2 The Bootstrap Distribution With \\(P^*\\), we denote the bootstrap distribution, which is the conditional probability distribution introduced by sampling i.i.d. from the empirical distribution \\(\\hat{P}\\). Hence, \\(P^*\\) of \\({\\hat{\\theta}}^*\\) is the distribution that arises from sampling i.i.d. from \\(\\hat{P}\\) and applying the transformation \\(g(\\cdot)\\) to the data. Conditioning on the data allows us to treat \\(\\hat{P}\\) as fixed. 6.3 Bootstrap Consistency The bootstrap is is called consistent if \\[ \\mathbb{P}[a_n(\\hat{\\theta} - \\theta) \\leq x ] - \\mathbb{P}[a_n(\\hat{\\theta}^* - \\hat{\\theta}) \\leq x ] \\rightarrow 0 \\;\\; (n \\rightarrow \\infty)\\] Consistency of the bootstrap typically holds if the limiting distribution is normal and the samples \\(Z_1, .., Z_n\\) are i.i.d. Consistency of the bootstrap implies consistent variance and bias estimation: \\[ \\frac{Var^* (\\hat{\\theta}^*)}{Var(\\hat{\\theta})} \\rightarrow 1\\] \\[ \\frac{\\mathbb{E}^* (\\hat{\\theta}^*) - \\hat{\\theta}}{\\mathbb{E}(\\hat{\\theta}) - \\theta} \\rightarrow 1\\] You can think of \\(\\theta\\) as the real parameter and \\(\\hat{\\theta}\\) as the estimate based on a sample. Similarly, in the bootstrap world, \\(\\hat{\\theta}\\) is the real parameter, and \\(\\hat{\\theta}^*_i\\) as an estimator of the real parameter \\(\\hat{\\theta}\\). The bootstrap world is an analogue of the real world. So in our bootstrap simulation, we know the true parameter \\(\\hat{\\theta}\\). From our simulation, we get many \\(\\hat{\\theta}^*_i\\) and can find the bootstrap expectation \\(\\mathbb{E}[\\hat{\\theta}^*_n] \\approx B^{-1}\\sum\\limits_{j = 1}^n \\hat{\\theta}^{* j}_n\\). The idea is now to generalize from the boostrap world to the real world, i.e. by saying that the relationship between \\(\\hat{\\theta}^*\\) and \\(\\hat{\\theta}\\) is similar to the one between \\(\\hat{\\theta}\\) and \\(\\theta\\). A simple trick to remember all of this is: if there is no hat, add one if there is a hat, add a star. 6.4 Boostrap Confidence Intervals Note that there confidence intervals are not simply taking the quantiles of the bootstrap distribution. The trick is really to make use of the analogy between the real world and the boostrap world. So when we see our bootstrap expectation \\(\\mathbb{E}[\\hat{\\theta}^*_n]\\) is way higher than \\(\\hat{\\theta}\\), then we also should believe that our \\(\\hat{\\theta}\\) is higher than \\(\\theta\\). The above procedure accounts for that. 6.5 Boostrap Estimator of the Generalization Error We can also use the bootstrap to estimate the generalization error. \\[ \\mathbb{E}[\\rho(Y_{new}, m^*(X_{new}))] \\] We draw a sample \\(({Z_1}^*, ..., {Z_n}^*, Z_{new})\\) from \\(\\hat{P}\\) We compute the bootstrapped estimator \\({m(\\cdot)}^*\\) based on the sample We estimate \\(\\mathbb{E}[\\rho(Y_{new}, {m^*(X_{new})}^*)]\\), which is with respect to both training and test data. We can rewrite the generalization error as follows: \\[ \\mathbb{E}[\\rho(Y_{new}, m^*({X_{new}}^*))] = \\mathbb{E}_{train}[E_{test}[\\rho({Y_{new}}^*, m^*({X_{new}}^*))| train]]\\] Conditioning on the training data in the inner expectation, \\(m(\\cdot)\\) is non-random / fixed. The only random component is \\({Y_{new}}^*\\). Since we draw from the empirical distribution and place a probability mass of \\(1/n\\) on every data point. we can calculate the inner (discrete) expectation easily via \\(\\mathbb{E}(X) = \\sum\\limits_{j = 1}^n p_j * x_j = n^{-1} \\sum\\limits_{j = 1}^n x_j\\). The expectation becomes \\[ \\mathbb{E}_{train}[n^{-1}\\sum\\rho(Y_{i}, m^*(X_{i}))] = n^{-1}\\sum\\mathbb{E}[\\rho(Y_{i}, m^*(X_{i}))]\\] We can see that there is no need to draw \\(Z_{new}\\) from the data. The final algorithm looks as follows: Draw \\(({Z_1}^*, ..., {Z_n}^*)\\) compute bootstrap estimator \\({\\hat{\\theta}}^*\\) Evaluate this estimator on all data points and average over them, i.e \\(err^* = n^{-1} \\sum \\rho(Y_i, m^*(X_i))\\) Repeat steps above B times and average all error estimates to get the bootstrap GE estimate, i.e. \\(GE^* = B^{-1} \\sum {err_i}^*\\) 6.6 Out-of-Boostrap sample for estimating the GE One can criticize the GE estimate above because some samples are used in the test as well as in the training set. This leads to over-optimistic estimations and can be avoided by using the out-of-bootstrap approach. With this technique, we first generate a bootstrap sample to compute our estimator and then use the remaining observations not used in the bootstrap sample to evaluate the estimator. We do that \\(B\\) times and the size of the test set may vary. You can see this as some kind of cross-validation with about 30% of the data used as the test set. The difference is that some observations were used multiple times in the training data, yielding a training set always of size n (instead of - for example \\(n*0.9\\) for 10-fold-CV). 6.7 Double Boostrap Confidence Intervals Confidence intervals are almost never exact, meaning that \\[\\mathbb{P}[\\theta \\in I^{**}(1-\\alpha)] = 1-\\alpha + \\Delta\\] Where \\(I^{**}(1-\\alpha)\\) is a \\(\\alpha\\)-confidence interval. However, by changing the nominal coverage of the confidence interval, it is possible to make the actual coverage equal to an arbitrary value, i.e \\[ \\mathbb{P}[\\theta \\in I^{**}(1-\\alpha&#39;)] = 1-\\alpha\\] The problem is that \\(\\alpha&#39;\\) is unknown. But another level of bootstrap can be used to estimate \\(\\alpha\\), denoted by \\(\\hat{\\alpha}\\), which typically achieves \\[\\mathbb{P}[\\theta \\in I^{**}(1-\\hat{\\alpha}&#39;)] = 1- \\alpha + \\Delta&#39;\\] with \\(\\Delta&#39; &lt; \\Delta\\) To implement a double bootstrap confidence interval, proceed as follows: Draw a bootstrap sample \\(({Z_1}^*, ..., {Z_n}^*)\\). From this sample, draw B second-level bootstrap samples and compute the estimator of interest and one confidence interval \\(I^{**}(1-\\alpha)\\) based on B second-level bootstrap samples. evaluate whether \\(\\hat{\\theta}\\) lays within the bootstrap confidence interval from a. \\(cover^*(1-\\alpha) = 1_{[\\hat{\\theta} \\in I^{**}(1-\\alpha)]}\\) Repeat the above M times to get \\(cover^{* 1}, ..., cover^{* M}\\) and hence approximate \\(\\mathbb{P}[\\theta \\in I^{**}(1-\\alpha)]\\) with \\[ p^*(\\alpha) = M^{-1} \\sum\\limits_{m = 1}^M cover^{* m}\\] Vary \\(\\alpha\\) in all of the steps above to find \\(\\alpha&#39;\\) so that \\(p^*(\\alpha&#39;) = 1- \\alpha\\) Question here (see Google docs) 6.8 Three Versions of Boostrap So far, we discussed the fully non-parametric bootstrap, which is simulating from the empirical distribution. On the other extreme of the scale, there is the parametric bootstrap. The middle way is the model-based bootstrap 6.8.1 Non-parametric Regression We draw a bootstrap sample \\(({Z_1}^*, ..., {Z_n}^*) \\sim \\hat{P}\\), i.e. we sample from the empirical distribution data with replacement. 6.8.2 Parametric Boostrap Here, we assume the data are realizations from a known distribution \\(P\\), which is is determined up to some unknown parameter (vector) \\(\\theta\\). That means we sample \\((Z_1, ..., Z_n) \\sim P_{\\hat{\\theta}}\\). For example, take the following regression model \\(y = X\\beta + \\epsilon\\) where we know the errors are Gaussian. We can estimate our regression model, obtain residuals and compute the mean (which is zero) and the standard deviation of them. To generate a bootstrap sample, we simulate residuals \\(\\epsilon^*\\) from \\(N(0, \\hat{\\mu})\\) and Add them to our observed data, i.e. we obtain \\(({Y_1}^*, ..., {Y_n}^*)\\) from \\(X\\beta + \\epsilon^*\\). Hence, the final bootstrap sample we use is \\((x_1, {Y_1}^*), ..., (x_1, {Y_n}^*))\\) where the \\(x_i\\) are just the observed data. We can compute bootstrap estimates \\(\\hat{\\beta}^*\\), the bootstrap expectation \\(\\mathbb{E}^*[\\beta*]\\) as well as confidence intervals for the regression coefficients or generalization errors just as shown in detail above. The difference is only how the bootstrap sample is obtained. Similarly, for time series data, we may assume an AR(p) model. Initializing \\({X_0}^*, ..., {X_{-p+1}}^*\\) with \\(0\\). Generate random noise \\({\\epsilon_1}^*, ..., {\\epsilon_{n+m}}^*\\) according to \\(P_{\\hat{\\theta}}\\). Construct our time series \\({X_t}^* = \\sum\\limits_{j = 1}^p \\hat{\\theta}{X_{t-j}}^* + {\\epsilon_t}^*\\) \\((X_1, ..., X_{n+m})\\) Throw away the first \\(M\\) observations that were used as fade-in. Proceed with the B bootstrap samples \\(({X_1}^*, ..., {X_n}^*)\\) as outlined above for the non-parametric bootstrap to obtain coefficient estimates for \\(\\theta\\), confidence intervals or estimating the generalization error of the model. 6.8.3 Model-Based Bootstrap The middle way is the model based bootstrap. As with the parametric bootstrap, we assume to know the model, e.g. \\(y = m(x) + \\epsilon\\) (where \\(m(\\cdot)\\) might be a non-parametric curve estimator), but we do not make an assumption about the error distribution. Instead of generating \\(\\epsilon^*\\) from the known distribution with unknown parameters (\\(\\epsilon^* \\sim P_{\\hat{\\theta}}\\), as in the parametric bootstrap)), we draw them with replacement from the empirical distribution. To sum it up, these are the steps necessary: Estimate \\(\\hat{m(\\cdot)}\\) from all data. Simulate \\({\\epsilon_1}^*, ..., {\\epsilon_n}^*\\) by drawing from \\(\\hat{P}\\) with replacement. Obtain \\(({Y_1}^*, ..., {Y_n}^*)\\) from \\(\\hat{m}(x) + \\epsilon^*\\). As for the parametric bootstrap, the final bootstrap sample we use is \\((x_1, {Y_1}^*), ..., (x_n, {Y_n}^*))\\) where the \\(x_i\\) are just the observed data. Again, you can use the bootstrap samples as for the other two methods. 6.9 Conclusion Which version of the bootstrap should I use? The answer is classical. If the parametric model fits the data very well, there is no need to estimate the distribution explicitly. Also, if there is very little data, it might be very difficult to estimate \\(P\\). On the other hand, the non-parametric bootstrap is less sensitive to model-misspecification and can deal with arbitrary distributions (? is that true?). "],
["classification.html", "Chapter 7 Classification 7.1 Indirect Classification - The Bayes Classifier 7.2 Direct Classification - The Discriminant View 7.3 Indirect Classification - The View of Logistic Regression 7.4 Discriminant Analysis or Logistic Regression? 7.5 Multiclass case (J &gt; 2)", " Chapter 7 Classification 7.1 Indirect Classification - The Bayes Classifier In classification, the goal is to assign observations to a group. Similar to regression, where we have \\(m(x) = E[Y | X = x]\\), we want to assign class probabilities to the observations \\[\\pi_j (x) = P[Y = j | X = x] \\;\\;\\; (j = 0,1, ..., J-1) \\] Def: A classifier maps A multidimensional input vector to a class label. Or mathematically: \\(C: \\mathbb{R}^p \\rightarrow \\{0, ..., J-1\\}\\) The quality of a classifier is measured via the zero-one test-error. \\[\\mathbb{P}[C(X_{new}) \\neq Y_{new}] \\] The optimal classifier with respect to the zero-one Error is the Bayes Classifier. It classifies an observation to the group for which the predicted probability was the highest. \\[ C_{bayes}(x) = \\arg\\max_{0&lt;j&lt;J-1}\\pi_j(x)\\] Hence, the Bayes Classifier is a point-wise classifier. For the Bayes Classifier, the zero-one test error is known as the Bayes Risk. \\[ \\mathbb{P}[C_{Bayes}(X_{new}) \\neq Y_{new}]\\] In practice, \\(\\pi_j(\\cdot)\\) is unknown (just as the MSE in regression is unknown) and hence, the the Bayes Classifier and Risk is unknown too. However, we can estimate \\(\\pi_j(\\cdot)\\) from the data and plug it in the Bayes Classifier. \\[\\hat{C}(X) = \\arg\\max_{0&lt;j&lt;J-1}\\hat{\\pi}_j(x)\\] This is an indirect estimator, since we first estimate the class probabilities \\(\\pi_j(\\cdot)\\) for each observation \\(x\\) and then assign the class to it for which the probability was the highest. Question how is that more indirect than Discriminant analysis? Don’t we use the Bayes classifier in the end? 7.2 Direct Classification - The Discriminant View 7.2.1 LDA One example for a direct classification is discriminant analysis. Using Bayes Theorem \\[ \\mathbb{P}[Y = j | X] = \\frac{\\mathbb{P}[X = x | y = j]}{\\mathbb{P}[X = x]}*\\mathbb{P}[Y = j] \\] And assuming \\[ (X| Y) \\sim N_p(\\mu_j, \\Sigma); \\;\\; \\sum\\limits_{k = 0}^{J-1}p_k = 1\\] We can write \\[ \\mathbb{P}[Y = j | X = x] = \\frac{f_{ x | Y = j } * p_j}{\\sum\\limits_{k = 0}^{J-1} f_{x | Y = k} * p_k} \\] Note that there is no distributional assumption on \\(Y\\) so far. You can estimate \\[\\mu_j = \\sum\\limits_{i = 1}^n{x_i*1_{Y_i = j}} / 1_{Y_i = j}\\] and \\[\\Sigma = \\frac{1}{n-j}\\sum\\limits_{j = 0}^{J-1}\\sum\\limits_{i = 1}^n(x_i - \\mu_j)(x_i - \\mu_j)&#39;\\;1_{Y_i = j} \\] Note that the means of the response of the groups are different, but the covariance structure is the same for all of them. We now also need to estimate \\(p_j\\). A straight-forward way is \\[ \\hat{p}_j = n^{-1}\\sum\\limits_{i = 1}^n{1_{[Y_i = j]}} = \\frac{n_j}{n} \\] From here, you can easily compute the classifier (as done in the exercise) by maximizing the log-likelihood. Then, you can derive the decision boundary by using \\(\\delta_j - \\delta_k = 0\\). In a two dimensional predictor space with two classes, the decision boundary is a line. Every combination of the two predictors on one side of the line will result in a prediction of class one, everything on the other side of the line of class two. Note that both the decision function (and hence the decision boundary) are linear in x. 7.2.2 QDA Quadratic discriminant analysis loosens the assumption of shared covariance matrices, namely each group has their own covariance matrix. This leads to quadratic decisions functions \\(\\delta\\) and hence to non-linear decision boundaries. QDA is more flexible but for high \\(p\\), the problem of over-fitting can occur, since the number of variables to estimate is \\(J*p(p+1)\\) variable for the covariance matrix only (instead of \\(p*(p+1)\\) for LDA. 7.3 Indirect Classification - The View of Logistic Regression There are also ways to come up with indirect assignment of the class label, namely via the Bayes classifier \\(\\hat{C}(X) = \\arg\\max_{0&lt;j&lt;J-1}\\hat{\\pi}_j(x)\\). The logistic model for \\(\\pi_j(x) = \\mathbb{P}(Y = j | X = x)\\) is \\(\\log(\\frac{\\pi_j(x)}{1-\\pi_j(x)}) = g(\\cdot)\\) or \\(\\pi_j(x) = \\frac{e^{g(\\cdot)}}{1+ e^{g(\\cdot)}}\\) equivalently. That model maps the real value \\(g(\\cdot)\\) can take to the interval \\((0, 1)\\), which gives a natural interpretation of the response as a probability. Note that we want the transformation to be monotone so the mapping is invertible. The response variable \\(Y_1, ..., Y_n\\) are distributed according to a Bernoulli distribution, i.e.. \\(Y_1, ..., Y_n \\sim \\textrm{Bernulli}(\\pi(x_i))\\). The logistic model belongs to the class of generalized linear models. These models have three characteristics: A link function (in our case the logit function \\(\\log(\\frac{\\pi_j(x)}{1-\\pi_j(x)}) = g(x)\\)). A response distribution (i.e. Bernoulli) The concrete form of \\(g(\\cdot)\\) (in logistic regression most often just a linear model \\(g(x) = \\sum\\limits_{j = 1}^p \\beta_j x_j\\)). This allows us to write down the likelihood of the logistic model \\[\\begin{equation} \\begin{split} L(\\beta, (x_1, Y_1), ..., (x_n, Y_n))&amp; = \\prod\\limits_{i = 1}^n \\mathbb{P}(Y_i = y_i)\\\\ &amp; =\\prod\\limits_{i = 1}^n \\pi(x_i)^{Y_i-1}(1-\\pi(x_i))^{Y_i-1}\\\\ \\log(L(\\cdot) &amp; = \\sum\\limits_{i = 1}^n Y_i \\pi(x_i) + (1 - Y_i) (1 - \\pi(x_i)) \\end{split} \\tag{7.1} \\end{equation}\\] There is no closed-form solution for the above problem, hence we need to rely on gradient descent to find the maximum likelihood solution. You can fit a logistic regression model in R as follows: # fit the model fit &lt;- glm(response~predictors, data = my_data, family = &quot;binomial&quot;) # predict prediction &lt;- predict(fit, newdata = my_data, type = &quot;response&quot;) &gt; 0.5 # evaluate in-sample mean(prediction == my_data$response) 7.4 Discriminant Analysis or Logistic Regression? The logistic regression assumes the log-odds to be linear in the predictors, i.e. \\(\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\sum\\limits_{i = 1}^p \\beta_i x_i\\). The discriminant analysis assumes \\(X|Y \\sim N(\\mu_j, \\Sigma\\), which leads to linear model in the decision variables. Hence, the methods are quite similar. It is quite natural to use factors with logistic regression, while for discriminant analysis, it is not very natural (?even not reasonable?). Empirically, the two methods yield similar results even under a violation of the normality assumption. TODO multinomial likelihood (see footnote.) 7.5 Multiclass case (J &gt; 2) With logistic regression, you can not model multiclass cases directly, but indirectly using one of the following methods: Encode a J-case problem as J binary problems (one class against the rest), that is \\[\\begin{equation} Y_i^{(j)} = \\begin{cases} 1 \\;\\;\\ \\text{if} \\;\\;\\ Y_i = j \\\\ 0 \\;\\;\\ \\text{else}. \\end{cases} \\end{equation}\\] For each case you want to label, you will obtain \\(J-1\\) probability estimates, i.e. \\(\\pi_j(x) = \\frac{\\exp\\big(\\sum\\beta^{(j)}_jx_j\\big)}{1 + \\exp\\big(\\sum\\beta^{(j)}_jx_j\\big)}\\). They don’t sum up to one necessarily, but you can normalize to obtain normed probabilities. Then, use the Bayes classifier to choose the class label (\\(\\arg\\max_{0 &lt; j &lt; J-1} \\pi_j\\)) Similarly, you can also model all against the reference, \\(\\log(\\frac{\\pi_1}{\\pi_0})\\). This might be helpful when we want to compare different group memberships with a reference group. For example in clinical trials, we want to compare how many times more likely someone belongs to group ill with disease A than healthy (the reference). You can also look at pair-wise comparisons. Choose two groups you want to compare, drop all observations that don’t belong to one of the two groups and estimate the model. There are \\(\\binom{J}{2}\\) possible models with all models potentially having different number of observations. In the special case of ordered groups, the correct model is often a proportional odds model that models \\[\\text{logit}(\\mathbb{P}(Y &lt; k |X) = \\alpha_k + g(\\cdot))\\] with \\(\\alpha_1 &lt; \\alpha_2 &lt; \\text{...} &lt; \\alpha_{J-1}\\). The log odds are proportional, which becomes obvious if we take \\(e\\) to the power of the above equation. Check this webpage for more information. Note that proportionality in the log odds does not mean proportionality in the probabilities, since they are only linked through a non-linear mapping (the logistic transformation). "],
["flexible-regression-and-classification-methods.html", "Chapter 8 Flexible regression and classification methods 8.1 Additive Models 8.2 MARS 8.3 Neural Networks 8.4 Projection Pursuit Regression 8.5 Classification and Regression Trees", " Chapter 8 Flexible regression and classification methods The curse of dimensionality makes it very hard to estimate fully non-parametric regression function \\(\\hat{m} = \\mathbb{E}[Y|X = x]\\) or classification function \\(\\hat{\\pi}_j = \\mathbb{P}[Y = j | X = x]\\). Hence, by making some (reasonable) structural assumptions, we can improve our models significantly. Generally, we consider the mapping \\(\\mathbb{R}^p \\rightarrow \\mathbb{P}\\) via the function \\(g(\\cdot)\\) for both the regression and the classification problem. 8.1 Additive Models 8.1.1 Structure One assumption we can make is to assume a particular functional form of \\(g(\\cdot)\\), namely an additive. That is \\[g_{add}(x) = \\mu + \\sum\\limits_{j = 1}^pg(x_j) \\] \\(E[g(x_j)] = 0\\) is required to make the model identifiable only. Note that we have not placed any assumption on \\(g(x_j)\\) yet, that is, \\(g(x_j)\\) can be fully non-parametric, but each dimension is mapped separately. In other words every \\(g(x_j) \\;\\; j = 1, ..., p\\) models one input dimension and mapping of input to output is obtained by summing the transformed inputs up. This eliminates the possibility of interaction effects. 8.1.2 Fitting Procedure Additive models can be estimated with a technique called back-fitting. However, the model can be estimated with any non-parametric method for one-dimensional smoothing. Here is the receipt: since we assume an additive model, we need to initialize all \\(p\\) components of it with zero, that is setting \\(g_j(\\cdot) = 0 \\;\\; j = 1,..., p\\) plus setting \\(\\mu = n^{-1}\\sum Y_i\\). * Then we fit one-dimensional smoother repeatedly, that is solving the one-dimensional smoothing problem \\(Y - \\mu - \\sum\\limits_{j \\neq k}\\hat{g}_j = \\hat{g}_j(x_j)\\), or put differently \\(\\hat{g}_j = S_j(Y - \\mu1 - \\sum\\limits_{j \\neq k}g_j)\\). This has to be done repeatedly for \\(j = 1, ..., p, 1, ..., p\\) etc. Stop iterating when functions don’t change much anymore, that is, when the following quantity is less than a certain tolerance. \\[\\frac{|\\hat{g}_{i, new} - \\hat{g}_{i, old}|}{|\\hat{g}_{i, old}|}\\] Normalize the functions by subtracting the mean from them: \\[\\tilde{g}_j = \\hat{g}_j - n^{-1} \\sum\\limits_{i = 1}^n \\hat{g}_j(x_{ij})\\] Back-fitting is a coordinate-wise optimization method that optimizes one coordinate at the time (one \\(g(\\cdot)\\), but can be more than one parameter), which may be slower in convergence than a general gradient descent that optimizes all directions simultaneously but also more robust. 8.1.3 Additive Models in R You can use the package mgcv data(&quot;ozone&quot;, package = &quot;gss&quot;) fit &lt;- mgcv::gam(upo3 ~ s(vdht) + s(wdsp) + s(hmdt) + s(sbtp), data = ozone) plot(fit, pages = 1) You can see that vdht enters the model almost linearly. That is, with an increase of one unit of vdht, the predicted 03 value increases linearly. sbtp is different. Depending on the value of sbtp, the increase in the predicted value is different. Low sbtp values hardly have an impact on the response, higher values do. 8.2 MARS MARS stands for multivariate adaptive regression splines and allows pice-wise linear curve fitting. In contrast to GAMs, they allow for interactions between the variables. MARS is very similar to regression trees, but it has a continuous response. It uses reflected pairs of basis functions \\[\\begin{equation} (x_j -d)_{+} = \\begin{cases} x_j - d \\;\\;\\ \\text{if} \\;\\;\\ x_j - d &gt; 0 \\\\ 0 \\;\\;\\ \\text{else}. \\end{cases} \\end{equation}\\] and it counterpart \\((d - x_j)_{+}\\). The index \\(j\\) refers to the j-th predictor variable, d is a knot at one of the \\(x_js\\). The pool of basis functions to consider is called \\(\\mathcal{B}\\) and contains all variables with all potential knots, that is \\[ \\mathcal{B} = \\{(x_j - d)_+ \\;\\;\\; (d - x_j)_+ \\;\\;\\;j = \\{1, ..., p\\} \\;\\; d = \\{x_{1j}, ..., x_{nj}\\}\\}\\] The model then is \\[ g(x) = \\mu + \\sum\\limits_{m = 1}^M\\beta_m h_m(x) = \\sum\\limits_{m = 0}^M\\beta_m h_m(x)\\] The model uses forward selection and backward deletion and optimizes some (generalized) cross-validation criterion. Here is the receipt: initialize \\(\\mathcal{M} = \\{h_0(\\cdot) = 1\\}\\) and estimate \\(\\beta_0\\) as the data average of the response \\(n^{-1}\\sum\\limits_{i = 1}^n Y_i\\). for \\(r = 1, 2, ...\\) search for a new pair of function \\((h_{2 r-1} \\;\\; h_{2 r})\\) which are of the form \\[h_{2 r-1} = (x_j -d)_+ \\times h_l\\] \\[h_{2 r} = (d - x_j)_+ \\times h_l\\] that reduce the residual sum of squares the most with some \\(h_l\\) from \\(\\mathcal{M}\\) that some basis functions from \\(\\mathcal{B}\\) does not contain \\(x_j\\). The model becomes \\[\\hat{g}(x) = \\sum\\limits_{m = 0}^{2r}\\hat{\\beta}_m h_m(x)\\] which can be estimated by least squares. Update the set \\(\\mathcal{M}\\) to be \\(\\mathcal{M} = \\mathcal{M}_{old} \\cup \\{h_{2r-1}, h_{2r}\\}\\) Iterate the above step until the set \\(\\mathcal{M}\\) becomes large enough. Do backward deletion (pruning) by removing the one function from a reflected pair that increases the residual sum of squares the least. Stop the pruning when some GCV score reaches its minimum. From the receipt above, we can see that \\(d\\)-way interactions are only possible if a \\(d-1\\)-way interaction with a subset of the \\(d\\)-way interaction is already in the model. For interpretability and other reasons, restricting the number of interactions to three or two is beneficial. Restricting the degree of interaction to one (which is actually no interaction) will yield an additive model. 8.2.1 Details for Dummies Note that by using reflective pairs \\(\\big\\{(x_j - d)_+ \\;\\;\\; (d - x_j)_{+}\\; \\}\\), we construct a piece-wise linear function with one knot at \\(d\\), since we sum the functions (which both a have a zero part that does not overlap) up. This hinge function (or better the two parts of individually) will be multiplied with a respective \\(\\beta\\), so the slope is adaptive. Also, since each of the functions have its own \\(d\\), the kink of the function must not occur at \\(y = 0\\) (this is wrong). 8.2.2 Example Let us consider the simple case of a one dimensional predictor space. We add noise to data that is piece-wise linear up to \\(x= 100\\) and then follows a sine-wave. We then fit three mars models with different number of maximal knots. library(&quot;earth&quot;) sim &lt;- data_frame( x = -25:75, y = pmax(0, x - 40) + rnorm(100, 0, 3)) %&gt;% bind_rows(data_frame( x = 75:100, y = -1*sqrt(x)* sin(x/3) + 30)) fitted &lt;- data_frame( nk3 = earth(y~x, data = sim, nk = 3), nk5 = earth(y~x, data = sim, nk = 5), nk100 = earth(y~x, data = sim, nk = 100) ) sim2 &lt;- fitted %&gt;% map_df(~predict(.x)) %&gt;% bind_cols(sim) %&gt;% gather(key, value, -x, -y) ggplot(sim2) + geom_point(aes(x = x, y = y)) + geom_line(aes(x = x, y = value, color = key)) summary(fitted$nk3) ## Call: earth(formula=y~x, data=sim, nk=3) ## ## coefficients ## (Intercept) 0.06140576 ## h(x-30) 0.53378814 ## ## Selected 2 of 3 terms, and 1 of 1 predictors ## Termination condition: Reached nk 3 ## Importance: x ## Number of terms at each degree of interaction: 1 1 (additive model) ## GCV 36.0235 RSS 4361.396 GRSq 0.8112337 RSq 0.8171787 The example shows what we expected. The green line with a maximum of three knots uses just one knot around 30, and only the right part of the reflected pair is used. It cannot distinguish between the linear segment between 40 and 75 and the sine-wave afterwards. By allowing more knots, we can see that the red line fits the data quite well. Note that the default of degree is just \\(1\\), so we don’t have interaction terms in the model. This does not matter for a one-dimensional example anyways. 8.3 Neural Networks Neural networks are high-dimensional non-linear regression models. The way it works is best illustrated with a picture. This is a neural network with one hidden layer, \\(p\\) input layers and \\(q\\) output layers. Mathematically speaking, the model is: \\[ g_k(x) = f_0\\Big(\\alpha_k + \\sum\\limits_{h = 1}^q w_{hk} \\phi(\\tilde{\\alpha}_h + \\sum\\limits_{j = 1}^p w_{jh}x_j)\\Big)\\] Where \\(\\phi(x)\\) is the sigmoid function \\(\\frac{exp(x)}{1 + exp(x)}\\), \\(f_0\\) is the sigmoid function for classification and the identity for regression. In the case of regression \\(k = 1\\) is used, for classification we use \\(g_0, ..., g_{J-1}\\) and then use the Bayes classifier \\(\\mathcal{\\hat{C}(x)} = \\arg\\max\\limits_{0&lt;j&lt;J-1} g_j(x)\\) (is that correct?), which is called the softmax in the neural network literature. 8.3.1 Fitting Neural Networks (in R) The nnet function from the package with the same name basically uses gradient descent to maximize the likelihood. It is important to first scale the data so the gradient descent does not get stuck in flat regions of the sigmoid function. set.seed(22) data(&quot;ozone&quot;, package = &quot;gss&quot;) unloadNamespace(&quot;MASS&quot;) scaled &lt;- ozone %&gt;% select(-upo3) %&gt;% scale() %&gt;% as_data_frame() %&gt;% mutate(log_upo3 = log(ozone$upo3)) fit &lt;- nnet::nnet( log_upo3 ~., data = scaled, size = 3, # how many nodes in the *one* hidden layer. decay = 4e-4, # regularization. Multiply weights by 1 - decay after # gradient step. linout = TRUE, # linear output units (refers to f0?). skip = TRUE, # add skip-layer connections between output and input. maxit = 500 ) The weights between the nodes are: summary(fit) ## a 9-3-1 network with 43 weights ## options were - skip-layer connections linear output units decay=4e-04 ## b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 ## -2.28 1.27 -0.34 -2.57 1.46 0.03 0.10 -1.02 -0.39 -0.33 ## b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 ## -12.43 5.09 2.04 8.19 -7.66 -7.01 2.40 -0.31 3.59 -1.19 ## b-&gt;h3 i1-&gt;h3 i2-&gt;h3 i3-&gt;h3 i4-&gt;h3 i5-&gt;h3 i6-&gt;h3 i7-&gt;h3 i8-&gt;h3 i9-&gt;h3 ## -19.77 -6.64 1.49 -4.53 -3.95 2.28 6.05 5.19 10.05 -0.20 ## b-&gt;o h1-&gt;o h2-&gt;o h3-&gt;o i1-&gt;o i2-&gt;o i3-&gt;o i4-&gt;o i5-&gt;o i6-&gt;o ## 2.50 -1.81 0.68 0.71 0.11 -0.09 -0.49 0.72 0.01 -0.03 ## i7-&gt;o i8-&gt;o i9-&gt;o ## 0.04 -0.29 -0.15 The in-sample MSE for the regression case is mean(residuals(fit)^2) ## [1] 0.1036706 8.4 Projection Pursuit Regression The model takes the form \\[g_{PPR} = \\mu + \\sum\\limits_{k = 1}^q f_k(\\sum \\limits_{j = 1}^p \\alpha_{jk}x_j) \\] With \\(\\sum \\limits_{j = 1}^p \\alpha_j = 1\\) and \\(E[f_k(\\sum \\limits_{j = 1}^p \\alpha_{jk}x_j)] = 0 \\;\\; \\text{for all k}\\). \\(\\mathbf{\\alpha}_k x_j\\) is the projection of the j-th column in the design matrix onto \\(\\alpha_k\\). The functions \\(f_k\\) only vary along one direction and are hence called ridge functions. Projection pursuit regression is similar to both neural nets and additive models. It is similar to GAMs because it can be seen as an additive model whereas the predictors were first projected into the optimal direction. And it is similar neural nets because if you assume the identity for \\(f_0\\) in the neral net (which you typically do for regression) the models become very similar up to the term \\(w_{hk} \\phi\\), which is just \\(f_k\\) in projection pursuit regression. Hence, instead of assuming a parapetric form (the sigmoid function) and multiplying that transformation with a weight \\(w_{hk}\\)1 we don’t make any assumption on the form of the function, that is, let \\(f_k\\) be fully non-parametric. Probably for that very reason, the model requires much smaller \\(q\\) than a neural net requires hidden units, at the expense of estimating the ridge functions (which is not necessary for neural nets). 8.4.1 Proejction Pursuit Example In the following, we illustrate how optimal projections of the initial predictor space can allow us to use an additive functional form to deal with interaction terms. Let us consider the following data-generating model \\[ Y = X_1 \\times X_2 + \\epsilon \\; \\text{with} \\;\\epsilon \\sim N(0, 1) \\; \\text{and}\\; X_1, X_2 \\sim \\text{Unif}(-1,1)\\] Where \\(X \\in \\mathbb{R}^2\\), i.e. a two-dimensional predictor space with the predictors \\(X_1\\) and \\(X_2\\). Using elementary calculus, this can be rewritten as \\[ Y = \\frac{1}{4} (X_1 + X_2)^2 - \\frac{1}{4}(X_1 - X_2)^2\\] Hence, we rewrote a multiplicative model as an additive model. As we are using arbitrary smooth functions \\(f_k\\), we can easily fit the quadratic terms in the equation above, so the problem to solve becomes \\[Y = \\mu + f_1(X_1 + X_2) - f_2(X_1 - X_2)\\] Therefore, the remaining question is how can we choose the two vectors \\(\\mathbf{\\alpha}_1\\) and \\(\\mathbf{\\alpha}_2\\) such that the result of the projection is \\(X_1 + X_2 \\;\\text{and}\\;X_1 - X_2\\). With the restriction \\(|\\alpha| = 1\\), it turns out we can proceed as follows: We first project predictor onto \\((\\alpha_{11}, \\alpha_{12}) = (0.7, 0.7)\\) and then onto \\((\\alpha_{11}, \\alpha_{12}) = (0.7, -0.7)\\). This yields \\(0.7(X_1 + X_2)\\) and \\(0.7(X_1 - X_2)\\). Let’s implement that in R data &lt;- data_frame( x1 = runif(500, -1, 1), x2 = runif(500, -1, 1), y = x1*x2 + rnorm(500, 0, 0.005) ) all &lt;- ggplot(data, aes(x = x1, y = x2)) + geom_point(aes(color = y), size = 3) + scale_color_gradient2() x1y &lt;- ggplot(data, aes(x = x1, y = y)) + geom_point(aes(color = y), size = 3) + geom_smooth() + scale_color_gradient2() grid.arrange(all, x1y) We can see the obvious pattern, but we can also see that an additive model would not do well on that. How about using the aforementioned projection? data &lt;- data %&gt;% mutate( projected_x1 = 0.7*(x1 + x2), projected_x2 = -0.7*(x1 - x2) ) projected_all &lt;- ggplot(data, aes(x = projected_x1, y = projected_x2)) + geom_point(aes(color = y), size = 3) + scale_color_gradient2() projected_x1 &lt;- ggplot(data, aes(x = projected_x1, y = y)) + geom_point(aes(color = y), size = 3) + geom_smooth() + scale_color_gradient2() projected_x2 &lt;- ggplot(data, aes(x = projected_x2, y = y)) + geom_point(aes(color = y), size = 3) + geom_smooth() + scale_color_gradient2() fitted_x1 &lt;- mgcv::gam(y~s(projected_x1), data = data) fitted_x2 &lt;- mgcv::gam(y~s(projected_x2), data = data) data &lt;- data %&gt;% mutate(fitted = predict(fitted_x1) + predict(fitted_x2)) fitted &lt;- ggplot(data, aes(x = x1, y = x2)) + geom_point(aes(color = fitted), size = 3) + scale_color_gradient2() grid.arrange(projected_all, projected_x1, projected_x2, fitted, nrow = 2) The bottom right picture shows the predictions with the projection pursuit approach, which resembles the original data pretty well. Again, the idea is to use an additive model to account for the interactions properly by first projecting the predictors optimally. Suppose we did not know the optimal projection. We could use the build-in ppr() command to fit a projection pursuit regression and then show us the projections used. First, we fit the model and check out the projections fit &lt;- ppr(y~ x1+x2, nterms = 2, data = data) sfsmisc::mult.fig(2) plot(fit) Now, let us look at the \\(\\alpha\\)s. fit$alpha ## term 1 term 2 ## x1 0.7795395 -0.7788790 ## x2 -0.6263530 -0.6271742 It’s pretty much the model we came up with before. There is also an interpretation of the projection vectors. Since the above example is not particularly interesting, we will look at the output of exercise series 9, problem 1g. fit_from_series9$alpha ## term 1 term 2 term 3 term 4 ## vdht 0.48 -0.09 -0.06 -0.02 ## wind -0.25 -0.11 0.09 0.12 ## humidity -0.03 -0.54 0.16 0.22 ## temp 0.48 0.43 -0.04 -0.37 ## ibht -0.06 -0.17 -0.05 0.01 ## dgpg 0.47 -0.45 -0.11 -0.66 ## ibtp 0.07 -0.10 -0.10 0.32 ## vsty -0.44 -0.10 0.05 0.26 ## day -0.23 -0.49 -0.97 0.44 If you take the mean over the absolute valeus in the rows, you can see that the variable ibht has relatively low average weights over the four terms. That means the variable does not get a high weight in general. You can also look at colums individually to find out which varialbes were important for a certain term, almost like in principle component analysis. You can see that term 3 is dominated by day. 8.5 Classification and Regression Trees The model function for trees is \\[g_{tree}(x) = \\sum\\limits_{r = 1}^M \\beta_r1_{[x \\in \\mathcal{P}_r]}\\] Where \\(\\mathcal{P} = \\cup_{j = 1}^M \\mathcal{P}_j\\), that is, the space \\(\\mathcal{P} \\in \\mathbb{R}^p\\) is devided into \\(M\\) disjoint partitions. Hence, note that in the sum above, \\(x\\) can only be in one of the \\(M\\) martitions and hence, all but one indicator functions are zero in the sum. The model yields a pice-wise constant response, that is, the prediction is the same for all \\(x \\in \\mathcal{P}_r\\). That can be visualized nicely in a two-dimensional predictor space. Figure 8.1: Partition with rpart(). Color indicate result of the majority voting Source: course script p. 73. Trees are similar to multivariate adaptive regression splines MARS, as mentioned in section 8.2 in the sense that they allow for interaction effects. This can be seen well in figure 8.1. Going form age 50 to age 100 has different effects depending on the start. Trees are different from MARS as they are piece-wise constant, whereas MARS are pice-wise linear. 8.5.1 Prediction given Partitioning Estimation of the parameters \\(\\beta_1, ..., \\beta_M\\) is easy if the partitioning is known. For Regression, it is simply the average of the response variables for the subset of the data that lays within the partition. Mathematically speaking \\[ \\hat{\\beta}_r = \\sum\\limits_{i = 1}^n 1_{[x_i \\in \\mathcal{P}_r]} Y_i / \\sum\\limits_{i = 1}^n 1_{[x_i \\in \\mathcal{P}_r]}\\] For classification, the class of a partition _j is determined by the largest group within that partition). We can estimate the class probabilities directly (also for J &gt; 2) for the r-th partition as follows: \\[ \\hat{\\pi}_j(x) = \\frac{\\# \\text{from class j in}\\; \\mathcal{P}_r}{\\#\\text{total in}\\; \\mathcal{P}_r} = \\sum\\limits_{i = 1}^n 1_{[Y_i = j]} 1_{[x_i \\in \\mathcal{P}_r]}/ \\sum\\limits_{i = 1}^n 1_{[x_i \\in \\mathcal{P}_r]}\\] 8.5.2 Assumptions on the Patritions As we saw above, obtaining predictions given the partitioning is not hard. The more difficult problem is to obtain the partitions. By imposing some restrictions on the shape of the partitions and the strategy to choose them, we can limit the complexity of the question at hand. Namely, we assume partitions that are axes parallel rectangles, just as depicted in the pictuer above. Note that this is a stronger limitation than just assuming linear (decision) boundaries since these boundaries also need to be parallel to the axis. For example, decision trees would not do well on a classification problem like this (unless there is a lot of data and we can have many splits: we use a greedy algorithm since the space of possible partitioning schemes is still huge. 8.5.3 Algorithm The algorithm now looks as follows: Start with \\(M = 1\\) and \\(\\mathcal{P} = \\{\\mathcal{R}\\} = \\mathbb{R}^p\\). Redefine \\(\\mathcal{R}\\) as \\(\\mathcal{R_{left}} \\cup \\mathcal{R_{right}}\\) where \\(\\mathcal{R}_{left} \\;= \\mathbb{R}\\times\\mathbb{R}\\;...\\; \\times(-\\infty, d]\\times \\mathbb{R} ...\\times\\mathbb{R}\\) \\(\\mathcal{R}_{right} = \\mathbb{R}\\times\\mathbb{R}\\;...\\; \\times(d, \\infty)\\times \\mathbb{R} ...\\times\\mathbb{R}\\) where \\(d\\) is a value from the finite set of midpoints between the data points with regard to the dimension currently considered. We search over all dimensions \\(j \\in \\{1, ..., p\\}\\) and within each dimension over all potential split points \\(d\\) such that the negative log-likelihood is decreased the most. The new partition is \\(\\mathcal{P} = \\{R_{left}, R_{right}\\}\\) We again refine the current partition as in step 2 by splitting up one partition into two parts. Then, we update the partition \\[\\mathcal{P} = \\mathcal{P}_{old} \\setminus \\mathcal{P}_{to\\;refine} \\;\\cup\\{R_{left}, R_{right}\\} \\] Iterate over the step 3 \\(M\\) times. Prune the tree by reverting some of the partitioning steps above until the optimal size of the tree is found (e.g via cross-validation). You can fit a tree in R with the rpart package, which stands for recursive partitioning. tree &lt;- rpart::rpart( upo3~., data = ozone, control = list( minsplit = 20, cp = 0.003 ) ) 8.5.4 Backward Deletion / Pruning After \\(M\\) steps, there will be \\(M + 1\\) partitions. This can also be visualized nicely in a tree structure. rpart::prune(tree, cp = 0.05) %&gt;% # prune tree for illustrative purposes rpart.plot::prp(extra = 1, box.col=c(&#39;pink&#39;, &#39;palegreen3&#39;, &#39;lightsteelblue 2&#39;,&#39;lightgoldenrod 1&#39;)[tree$frame$yval]) The idea is now to subsequently remove the leaves from the tree such that the negative log-likeihood increases the least. If we do that until no leaf is left, we end up with a sequence of trees. \\[\\begin{equation} \\mathcal{T}_M \\supset \\mathcal{T}_{M-1} \\;\\;...\\;\\;\\supset \\mathcal{T}_{\\emptyset} \\tag{8.1} \\end{equation}\\] Just as with Mallow’s \\(C_p\\), we can compute a score for every model that is increasing in the fit of the model but also has a complexity penality \\[R_{\\alpha}(\\mathcal{T}) = R(\\mathcal{T}) + \\alpha \\times \\text{size}(\\mathcal{T})\\] Now, we only need to find the right alpha. We can set a few alpha values, then find the best tree for this alpha \\(\\mathcal{T}(\\alpha) = \\arg\\min\\limits_{\\mathcal{T} \\subset \\mathcal{T}_M}R_{\\alpha}(\\mathcal{T})\\) and then do cross-validation for these alpha values to find the optimal alpha. It can be shown that the set \\(\\{\\mathcal{T}(\\alpha)| \\alpha \\in (0, \\infty]\\}\\) is nested and the same or a subeset of the set in equation (8.1). Use rpart::plotcp() to plot the size of the optimal trees for each alpha against the cross-validation score. Then, use the one-standard error rule to select the idal tree size. That is first find the tree with the lowest relative error. Then add one standard error to it’s error and find the smallest tree that does not execced this relative error. The idea behind this approach is to choose good model that performs similar to the best (and potentially complex) model but is as simple as possible. rpart::plotcp(tree) 8.5.5 Pros and Cons of Trees Pros are: Straightforward interpretation. Show it your grand mother and she will understand it. Allow for interaction effects. Competitive performance. Can deal with missing values thanks to the surrogate split. For each node the tree algorithm tries to find variables that are highly correlated with the selected splitter. Then, if this variable is not available for a new observation to be classified, the surrogate is used to classifiy the observation on that split node so subsequent nodes can further process the observation. The variable selection is done automatically and variables that are higher up in the hierarchy are considered to be more imporatant for prediction. We will have a look at ridge regression and LASSO which also do variable selection automatically, but the feature is not present in any method we looked at before. There are some cons also: First and foremost, trees yield piece-wise constant predictions, which is typically not what we assume the true underlaying function to look like. Subsequent splits depend on previous splits. Therefore, if an early split is wrong, everything following afterwards is wrong. This means the algorithm may not be very stable. ? question how would you use mars for classification? 8.5.6 Random Forests Random forests are made up of three main ingredients: regression (or classification) trees boostrapping aggregating The algorithm is as follows: draw \\(n_{tree}\\) boostrap samples (of size n obviously). build for each of them an unpruned tree. However, instead of searching over all \\(p\\) variables for the best split at each node, just consider a random sample of \\(m_{try}\\) variables for the split at each node. Obviously, \\(m_{try} = p\\) is the tree solution introduced before and corresponds to the bagging (which stands for boostrap aggregating, introduced later). Predict a new data point by aggregating the \\(n_{tree}\\) prediction (majority vote for classification, averaging for regression). To obtain an estimate for the generalization error, you can use the out-of-bag approach, that is At each boostrap iteration, make predictions with the data that is not in the boostrap sample. aggregate the predictions for all \\(n_{tree}\\) trees and compute the error rate and call it out-of-bag estimate of the error rate. The only drawback of trees is that interpretability is lower than for trees. For regression you need only one index since you will have just one ouput layer.↩ "],
["variable-selection-ridge-regression-an-lasso.html", "Chapter 9 Variable Selection - Ridge Regression an Lasso 9.1 Ridge Regression 9.2 Lasso 9.3 Extensions", " Chapter 9 Variable Selection - Ridge Regression an Lasso Except for trees, none of the techniques introduced so far perform variable selection. Variable selection is particularly important when working with high-dimensional data, namely for two reasons: In the case of highly correlated predictors, regression coefficient estimates become ill-determined, which means - loosly speaking - that there are a lot of different values for each predictor such that the model leads to the same prediction if the other coefficients are set wisely. This is undesirable. Note that the fitted values are not ill-determined since they don’t vary when changing the coefficients. In cases where we have \\(\\mathbf{n}&gt; \\mathbf{p}\\), we cannot estimate many models, so for example OLS. That means we first need to select some predictors before we can proceed. In addition, a model with few predictors is often easier to interpret than a model with many predictors. Ride and Lasso - which is which? Ridge regression - two words, is the method with an L2-penalty. Lasso is just one word - and has an L1-penalty. 9.1 Ridge Regression Consider the OLS Problem \\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\\] We can demean the predictors and obtain \\[y_i = \\beta_0 + \\beta_1(x_1 - \\bar{x}_1) + \\beta_2(x_2 - \\bar{x}_2) + ... + \\beta_p(x_p - \\bar{x}_p) + \\epsilon\\] In which case \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} = \\bar{Y}\\) and can take it to the other side and end up with the model. \\[\\tilde{y} = \\hat{\\beta}_1 \\tilde{x}_1 + \\hat{\\beta}_2 \\tilde{x}_2 + ... \\hat{\\beta}_p \\tilde{x}_p +\\epsilon\\] With all variables having a mean of zero. Hence we got rid of the intercept. In order to compare the different \\(\\beta\\)s, we should also scale the predictors. Hence, if there are two variables that are highly correlated, we can expres this part of the model equation as follows \\[ \\beta_j x^{(j)} + \\beta_k x^{(k)} \\approx (\\beta_j + c \\beta_j ) x^{(j)}\\] You can see that for each \\(\\beta_j\\), we can find an appropriate \\(\\beta_k\\) so that the sum does not change. That’s what we meant above when we said the coefficients are not stable but the sum is. One way to make the coefficients better-determined is to impose further conditions on them. For example, we can restrict \\(\\sum\\limits_{j = 1}^p \\beta_j &lt; s\\). The regression problem becomes \\[\\beta = \\arg\\min\\limits_{\\|\\beta\\| &lt; s}\\|X\\beta\\|\\] which is equivalent to the lagrangian problem \\(\\arg\\min\\limits_{\\beta} \\{ \\|X \\beta \\|+ \\lambda \\| \\beta \\|_2^2 \\}\\) With a one-to-one mapping of \\(s \\rightarrow \\lambda\\) Note that this is a generalization of the least square solution since it contains the least squares solution for \\(\\lambda = 0\\). The resulting normal equations are \\[(X&#39;X + \\lambda I)^{-1}\\hat{\\beta}^* = X&#39;Y\\] Where one can see that the matrix to invert will be non-singular \\(\\lambda &gt; 0\\), even if \\(X&#39;X\\) is singular. Due to this shrinking, it is intuitive that the \\(E[\\hat{\\beta}] \\neq \\beta\\), that is, the coefficient will be biased. That can be seen easily if we sneak in the ols solution for which we know it is unbiased. \\[\\begin{equation} \\begin{split} (E[\\hat{\\beta}] = &amp; E[(X&#39;X + \\lambda I)^{-1}X&#39;y]) \\\\ &amp; E[(X&#39;X + \\lambda I)^{-1} (X&#39;X)(X&#39;X)^{-1}X&#39;y] \\\\ &amp; E[(X&#39;X + \\lambda I)^{-1} (X&#39;X)\\beta^{ols}] \\\\ &amp; (X&#39;X + \\lambda I)^{-1} (X&#39;X)\\beta^{ols} \\end{split} \\end{equation}\\] Whereas \\(E[\\hat{\\beta}] \\neq \\beta\\) for \\(\\lambda &gt; 0\\) Also, we can see that for \\(\\lambda \\rightarrow \\infty\\), \\(\\beta \\rightarrow 0\\). However, since by introducing a bias, we at the same time decrease the variance of our estimator. Therefore, we can optimize the bias-variance trade-off by finding an appropriate \\(\\lambda\\), e.g. by (generalized) cross-valdiation. The regularization parameter lamda here is similar to the one we saw in the chapter about smoothing splines. To estimate the model, we can use the MASS package. fitted &lt;- MASS::lm.ridge( GNP.deflator ~., # it&#39;s lamBda, not lamda! R won&#39;t complain due to ... ! lambda = seq(0, 0.1, by = 0.001), data = longley ) We can plot the coefficients for different values of \\(\\lambda\\) in a so-called trace plot. plot(fitted) We can also get the coeficients for each lamda we estimated and select the best model and get the usual lm summary for it. coef(fitted) %&gt;% as_data_frame() ## # A tibble: 101 x 7 ## `` GNP Unemployed Armed.Forces Population Year ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2946.85636 0.2635272 0.03648291 0.011161050 -1.737030 -1.41879853 ## 2 1895.97527 0.2392348 0.03100610 0.009372158 -1.643803 -0.87657471 ## 3 1166.33337 0.2209952 0.02719073 0.008243201 -1.565026 -0.50108472 ## 4 635.78843 0.2066111 0.02440554 0.007514565 -1.496246 -0.22885815 ## 5 236.65772 0.1948539 0.02230066 0.007043302 -1.434886 -0.02473192 ## 6 -71.53274 0.1849806 0.02066688 0.006744636 -1.379323 0.13231532 ## 7 -314.43247 0.1765137 0.01937157 0.006565392 -1.328460 0.25560068 ## 8 -509.05648 0.1691312 0.01832674 0.006470736 -1.281519 0.35395451 ## 9 -667.11647 0.1626072 0.01747181 0.006437042 -1.237922 0.43345188 ## 10 -796.92303 0.1567781 0.01676376 0.006447832 -1.197224 0.49840118 ## # ... with 91 more rows, and 1 more variables: Employed &lt;dbl&gt; MASS::select(fitted) ## modified HKB estimator is 0.006836982 ## modified L-W estimator is 0.05267247 ## smallest value of GCV at 0.006 As pointed out above, \\(\\lambda = 0\\) corresponds to the OLS solution. 9.2 Lasso The Lasso is essentially just a variant of ridge regression whereas the penalty is the L1-norm instead of the squared L2 norm. \\[\\arg\\min\\limits_{\\beta} \\{ \\|X \\beta \\|+ \\lambda \\| \\beta \\|_1 \\}\\] Where \\(\\| \\beta \\|_1 = \\sum\\limits_{j = 1}^p|\\beta_j|\\), i.e. the sum of the absolute values of the coefficients. The mathematical properties of the L1-norm imply that some coefficients actually will become exactly zero, which is not the case for ridge regression. In the latter case, a decrease of a coefficient from \\(0.1\\) to \\(0\\) will decrease the penalty from \\(0.001\\) to \\(0\\), whereas a decrease from a coefficient of \\(10\\) to \\(9\\) wil decrease the penalty from \\(100\\) to \\(81\\). Hence, Lasso will tend to shrink larger coefficients more since the loss function rewards this more. The above plot shows the Lasso traces for six coefficients. On the x-axis we don’t have lamda, but the norm of the Beta vector. \\(max |\\beta|\\) corresponds to the OLS solution with \\(\\lambda = 0\\) since for any other value of \\(\\lambda\\), the norm is smaller. Hence, \\(x = 1\\) corresponds to the OLS solution. On the other hand, \\(x = 0\\) corresponds to \\(\\lambda = \\infty\\), since \\(\\|\\beta\\|\\) equals zero. In this case, all coefficients are zero obviously. Note that both Lasso and Ridge regression have worse in-sample performance than OLS, since the coefficients are biased. However, the out-of-sample performance is better since the bias variance trade-off is optimized. 9.3 Extensions 9.3.1 Elastic Net The elastic net combines the penalties used in ridge regression and lasso. \\[ \\hat{\\beta} = \\|Y- X\\beta\\|^2 \\;\\; \\text{subject to}\\; (1 - \\alpha) \\|\\beta\\|_1 + \\alpha \\|\\beta\\|^2 &lt;t\\] The lasso penalty \\((1 - \\alpha) \\|\\beta\\|_1 + \\alpha \\|\\beta\\|^2\\) is a sum of a structky convex function and a convex function and hence strictly convex. 9.3.2 Adaptive Lasso The idea is to use a different weight for each coefficient in the penalty term: \\[ \\hat{\\beta} = \\arg\\min\\|Y- X\\beta\\|^2 \\;\\; + \\lambda \\sum\\limits_{j = 1}^p w_j|\\beta_j|\\] The idea is now to use consistent esitimates for all coefficients, e.g. through least squares. Then, do a lasso and set \\(w_j = \\frac{1}{\\hat{\\beta_j}}\\). This means that we now penalize small coefficients more, so they get shrunken to zero more quickly, whereas large coefficients (which are clearly non-zero) are given a low weight so their bias after the shrinkage is small 9.3.3 Relaxed Lasso The relaxed lasso is also based on the idea of doing variable shrinkage isolated from variable selection. Hence, the idea is doing a lasso first to obtain the relevant variables. Then, a combination between lasso and OLS is used to obtain the final model \\[\\hat{\\beta}^{\\lambda, \\phi} = \\arg\\min\\limits_{\\beta} \\sum\\limits_{i = 1}^n(yi - \\sum\\limits_{j \\in \\mathcal{M}_\\lambda} \\beta_j x_{i, j})^2 + \\lambda \\phi \\sum\\limits_{j = 1}^p|\\beta_j|\\] For \\(\\phi = 0\\), the final estimates are just the OLS estimates with \\(\\mathcal{M}_\\lambda\\), \\(\\phi = 1\\) just reproduces the lasso that was used for variable selection. 9.3.4 (Sparse) Group Lasso When dealing with categorical data with $ J&gt;2$, the techniques introduced above have the drawback that they may select just a few of the dummy variables to be non-zero. However, for interpretability, we want to keep all or none of the dummies in the model instead of selecting each variable independently. This can be achieved with a group lasso. We first split the design matrix into \\(L\\) design matrices where each of them contains a block of \\(p_l\\) predictors and all observations and it holds that \\(\\sum\\limits_{p = 1}^L p_l = p\\). We also split the corresponding coefficient vector \\(\\beta\\) into pieces. Then, we group-wise scaled L2 penalties. \\[ \\|Y-\\sum\\limits_{l = 1}^L X_l\\beta_l \\|_2^2 + \\lambda \\sum\\limits_{l = 1}^L \\sqrt{p_l} \\|\\beta_l\\|_2\\] Each continuous predictor forms its own group, categorical predictors are put in one group. Note that for \\(L = p\\), the problem reduces to a lasso although we have an L2-penalty. This is because the L2 norm and the L1 norm of a scalar concide, or mathematically speaking \\(\\||s\\|_2 = \\sqrt{s^2} = |s| = \\|s\\|_1\\). This acts like a lasso on the group level, i.e. it sets all coefficients of a group to zero - or none of them. There is an extension called sparse group lasso which can bring sprasity within a group. This is not helpful for dummy variables, but for situations where you have predictors that form a group since they are highly correlated or otherwise connected. It also applies the idea of an elastic net penalty to the group lasso. The formula is \\[ \\|Y-\\sum\\limits_{l = 1}^L X_l\\beta_l \\|_2^2 + \\lambda_1 \\sum\\limits_{l = 1}^L \\sqrt{p_l} \\|\\beta_l\\|_2 + \\lambda_2 \\|\\beta_l \\|_1\\] 9.3.5 Oracle Properties The adaptive lasso (unlike all other lasso-related techniques presented here posseses so-called oracle properties). Consider the set $, which contains the all non-zero perdictor variabiables from a large number of available predictors. \\[ \\mathcal{M} = \\{j \\in \\{1, ..., p\\}; \\; \\beta_j \\neq 0\\}\\] The set \\(\\mathcal{M}_\\lambda\\) contains all predictors that were estimated to be non-zero via a Lasso estimate \\[ \\mathcal{M}_\\lambda = \\{j \\in \\{1, ..., p\\}; \\; \\hat{\\beta}_j \\neq 0\\}\\] An estimator has oracle properties if \\[ \\mathcal{M}_\\lambda =\\mathcal{M} \\; \\text{for} \\; n \\rightarrow \\infty\\] Lasso typically produces too large of non-zero predictors, but all non-zero predictors are in that set, that is \\[ \\mathcal{M}_{\\lambda}^{Lasso} \\supset \\mathcal{M} \\] "],
["bagging-and-boosting.html", "Chapter 10 Bagging and Boosting 10.1 Bagging 10.2 Subagging 10.3 \\(L_2\\)-Boosting 10.4 Some unfinished stuff", " Chapter 10 Bagging and Boosting Bagging stands for bootstrapping aggregating and 10.1 Bagging Bagging works as follows: Consider a base procedure \\[ \\hat{g}(\\cdot): \\mathbb{R}^p \\rightarrow \\mathbb{R}\\] Draw a bootstrap sample \\((Y_1^*, X_1^*), ..., (Y_1^*, X_n^*)\\) and compute the bootstrap estimator \\(\\hat{g(\\cdot)}^*\\). Repeat the above step \\(B\\) times yielding \\(\\hat{g}(\\cdot)^{*1}, ..., \\hat{g}(\\cdot)^{*B}\\) bootstrap estimators Average the estimates to construct the bagged estimator: \\[ \\hat{g}_{Bag}(x) = n^{-1} \\sum\\limits_{i = 1}^B\\hat{g}(\\cdot)^{*i}\\] Then, \\(\\hat{g}(\\cdot)\\) is nothing else than an approximation of the bootstrap expectation. \\[\\hat{g}(\\cdot) \\approx \\mathbb{E}^*[g(x)^{*}]\\] Note that the novel point is that we now use this approximation as an estimate of \\(g(\\cdot)\\). However, \\[\\begin{equation} \\begin{split} \\hat{g}(\\cdot)_{Bag} &amp; = \\hat{g}(\\cdot) + \\mathbb{E}^*[g(x)^{*}] - \\hat{g}(\\cdot) \\\\ &amp; = \\hat{g}(\\cdot) + \\text{bootstrap bias estimate} \\end{split} \\end{equation}\\] Instead of subtracting the bootstrap bias estimate, we are adding it! However, for trees for example, bagging reduces the variance of the estimate so much that the bias increase will not be strong enough to push the mean square error up. Let us consider a one-dimensional example. The variance of an indicator function (which is a Bernoulli experiment) is given by \\[ Var(1_{X&gt;d}) = \\mathbb{P}(X &gt; d)(1 - (\\mathbb{P}(X &gt; d))\\] If we assume \\(X \\sim N(0, 1)\\) and \\(d = 0\\), \\(\\mathbb{P}(X &gt; d) = 1 - \\mathbb{P}(X&lt;d) = 0.5\\), so the above quantity is \\(1/4\\). For bagged trees, it turns out that the estimator is a product of probit functions \\(\\phi(d-X)\\). Since it holds that \\[ \\text{if}\\;\\; X \\sim F, \\;\\;F(X) \\sim U\\] the variance of random forest is \\[Var(\\phi(d-X)) = Var(U) = 1/12\\] So the variance was reduced by a factor of 3. 10.2 Subagging Subagging stands for sub-sampling and aggregation. It is different from bagging in that instead of drawing bootstrap samples of size \\(n\\), we only draw samples of size \\(m &lt; n\\), that is \\((X_1^*, Y_1^*), ..., (X_m^*, Y_m^*)\\) but without replacement. It can be shown that for some situations, subagging with \\(m = n/2\\) is equivalent to bagging, hence, subagging is a cheap version of bagging. Bagging (and subagging) have one drawback which is the lack of interpretability. In the script, there is a comparison between MARS and trees. Both subagging and bagging helps for trees, but does not help for MARS. How come? Remember that fitted values of trees are piece-wise constant. Bagging makes them smoother (and hence more like MARS). MARS yield a piece-wise linear function by nature. Trees have a high variance and hence the bias-variance trade-off can be optimized with bagging. MARS don’t have such a high variance, hence, optimization is not possible to the same extend. 10.3 \\(L_2\\)-Boosting We saw bagging was a variance reduction technique. Boosting is a bias reduction technique. As with bagging there is a base estimator \\(\\hat{g}(\\cdot)\\). Then, you basically refit it to the residuals to reduce them many times. The concrete implementation is: Fit an estimator to the data and compute the residuals \\[U_i = Y_i - \\nu\\hat{g}_1(x_i)\\] where $&lt; 1 $ is a tuning parameter. The smaller \\(\\nu\\), the more explainable residuals you leave for suceeding estimators. Denote \\(\\hat{f}_1(x) = \\nu g_1(x)\\) For m = 2, 3, …, M: Fit the residuals to the data, i.e \\[ (U_i, X_i) \\rightarrow \\hat{g}(x)\\] Set \\[ \\hat{f}_m(x) = \\hat{f}_{m-1}(x) + \\nu \\hat{g}_m(x)\\] Compute the current residuals: \\[U_i = Y_i - \\hat{f}_m(x)\\] A small \\(\\nu\\) can be interpreted as follows: You go into the right direction, but you do that slowly and you allow yourself to be corrected later. Boosting can be useful in the context of thinning-out with trees, where you don’t have enough observations in the terminal nodes to continue. Boosting then comes up with a more complex solution by taking linear combinations of trees. For classificcation trees, you can also extend the tree by taking Boosting can also be used for varible selection. For example, let \\(\\hat{g}\\) be a GAM with one predictor. Then, for each variable fit such a gam and set \\(f_1(x) = arg\\min\\limits_{\\hat{g}_j(x) \\; j =\\{1, ..., p\\}}\\|Y - \\hat{g}_j(x)\\|\\), i.e. take the the additive model that had the smallest residual sum of squares. Then, use the adjusted residuals from it and fit another \\(p\\) GAMs and select again the gam with the smallest RSS. 10.4 Some unfinished stuff Let’s consider a simple one-dimensional example. If you have a cut-off at \\(d\\) (so for example for classification, \\(X &gt; d\\) is classified as \\(1\\), \\(X &lt; d\\) as \\(0\\)) and you take the mean over many bootstrap samples (which you do with random forests, where each tree has a potentially different \\(d\\)), with \\(n \\rightarrow \\infty\\) you get a smooth function (probably some form of a transformed binomial that is asymptotically normal) one_mean &lt;- function(d = 0, ...) { x &lt;- rnorm(...) x_boolean &lt;- x &gt; d mean(x_boolean) } mean_sim &lt;- rerun(1000, one_mean(n = 200)) %&gt;% flatten_dbl() ggplot(data_frame(mean = mean_sim), aes(x = mean)) + stat_ecdf(geom = &quot;step&quot;) Here you can see that if \\(x \\sim F\\), \\(F(X) \\sim Unif\\). x &lt;- rnorm(1000) plot.ecdf(x) y &lt;- pnorm(x) plot.ecdf(y) "],
["round-up.html", "Chapter 11 Round up 11.1 Comparing models 11.2 Exercises Take-aways", " Chapter 11 Round up This chapter consists of some thoughts I considered relevant for this course but which did not seem to fit well in any of the other chapters. 11.1 Comparing models The package plotmo provides nice functionality to plot diffent classes of models. We use it to compare a few models introduced in the previous chapters: I think this graph is a nice summary of what we leared in this course. We can characterize the models by: How smooth the fitted surface is. Linear models are smooth by nature (if there are no dummy variables involved), random forest (rpart) are just piecewise constant. Random forest are smoother than trees because they are aggregated trees. Ignore gbm for now and note that neural nets are also rather smooth. How flexible they are: Linear models are not very flexible, gams are only additive, so no interaction effects, trees can have interaction effects, so do random forests. This is a rough classification, but it grasps the general pattern. 11.2 Exercises Take-aways Creating Formulas Task (Series 11): Generate a R-formula for a cubic penalized regression model that accounts for all 3-way interactions. From ?formula The ^ operator indicates crossing to the specified degree. For example (a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turn expands to a formula containing the main effects for a, b and c together with their second-order interactions. library(sfsmisc) formula &lt;- wrapFormula( logO3 ~., data = data, wrapString = &quot;poly(*, degree = 3&quot; # polynomial ) update(formula, logO3 ~ .^3) # three way interaction Matrix Multiplication You can’t multiply data frames. No way. Use as.matrix to convert. don’t forget the intercept in the model frame. Other R is vectorized. Before writing complicated map() stuff, think about whether it is possible to do all calculations vectorised. Compare exercise series 8. Scaling and Transformations: If the response or in fact any variable is highly skewed, use the log as a first aid transformation to for efficiency. Also, some methods such as neural networks work better with scaled predictors. Another category are methods such as lasso or ridge regression, where it is beneficial for interpretability to scale the variables. Accessing model parameters When calculating the degrees of freedom, e.g. for the mallows cp, never use someghing like `length(coef(fit))`` as \\(p\\), since this will only work for linear models. For additive models for example, this will give you the number of soothing terms used, which is not equivalent to the degrees of freedom. Instead, use fit$df.residual Which is available for many classes of fitted models, i.e. also GAMs. Also, often it is not necessary to compute something like the residual standard error, since you can conveniently access it via fit$sigma Also, do sanity checks for your results. A qick plot(unlist(cps)) for example can show you the various values cp takes for different model vs. their index. As with cross validation, you know that the curve must be somehow convex. "],
["intro.html", "Chapter 12 Introduction", " Chapter 12 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 12. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 12.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 12.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 12.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 12.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package [@R-bookdown] in this sample book, which was built on top of R Markdown and knitr [@xie2015]. "],
["references.html", "References", " References "]
]
