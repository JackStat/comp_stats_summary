[
["index.html", "Computational Statistics - Summary Chapter 1 Introduction", " Computational Statistics - Summary Lorenz Walthert Chapter 1 Introduction This is a summary of the class computational statistics at ETH Zurich. "],
["multiple-linear-regression.html", "Chapter 2 Multiple Linear Regression", " Chapter 2 Multiple Linear Regression "],
["nonparametric-density-estimation.html", "Chapter 3 Nonparametric Density Estimation", " Chapter 3 Nonparametric Density Estimation "],
["nonparametric-regression.html", "Chapter 4 Nonparametric Regression", " Chapter 4 Nonparametric Regression "],
["cross-validation.html", "Chapter 5 Cross Validation 5.1 Motivation and Core Idea 5.2 Loss Function 5.3 Implementation 5.4 K-fold Cross-Validation 5.5 Properties of the different schemes 5.6 Shortcuts for (some) linear fitting operators 5.7 Parameter Tuning", " Chapter 5 Cross Validation 5.1 Motivation and Core Idea Cross-validation is a tool for estimating the performance of an algorithm on new data points, the so-called the generalization error. An estimate of the generalization allows us to do two important things: Tuning the parameters of a statistical technique. Comparing statistical techniques with regard to their accuracy. If we use the training data to evaluate the performance of an algorithm, this estimate will be over-optimistic because an estimator is usually obtained by minimizing some sort of error in the training data. Therefore, we use a separate data pool, called the test data to evaluate the performance out of sample. Consider the regression function estimate \\(\\hat{m}\\) based on a sample \\((X_1, ..., X_n.)\\). By increasing the number of parameters in the model and by allowing for interactions between them, we can make the regression model fitting arbitrarily well to the data. However, such an extremely complex model will not perform as well with new data, that is, will not generalize well to other situations, since we essentially modeled also a lot of noise. We use the following notation: \\[ l^{-1}\\sum\\limits_{i = 1}^l\\rho(Y_{new, i}, \\hat{g}(X_{new, i}))\\] Where \\(\\rho\\) is a loss function to be evaluated on the new data points \\((Y_{new, 1}, ..., Y_{new, l})\\) and the prediction made for \\((X_{new, 1}, ..., X_{new, l})\\) with the function \\(\\hat{g}\\), which was estimated from the training data \\((X_1, ..., X_n)\\). When \\(l\\) gets large, this approximates the test error \\[\\mathbb{E}_{(X_{new}, Y_{new})}[\\rho(Y_{new}, \\hat{m}(X_{new})]\\] which is still a function of the training data (since it is conditional on the training data). Note that the test error is not the same as the generalization error. The latter is an expectation over both the training and the test data. The typical relationship between the test error and the training error is depicted in the figure below. 5.2 Loss Function Depending on the application, one can imagine different loss-functions. For example the squared deviance from the true value is often used, i.e. \\[n^{-1}\\sum\\limits_{i = 1}^n\\rho(Y_i, \\hat{m}(X_i)) = n^{-1}\\sum\\limits_{i = 1}^n(Y_i - \\hat{m}(X_i))^2\\] Hence, larger deviance is penalized over-proportionally. For classification, one often uses the zero-one error, i.e. \\[n^{-1}\\sum\\limits_{i = 1}^n1_{\\hat{m}(X_i) = Y_i}\\]. However, it might also be appropriate to use asymmetric loss functions if false negatives are worse than false positives (i.e. for cancer tests). 5.3 Implementation There are different ways to do cross validation while adhering to the principles introduced above. 5.3.1 Leave-one-out Use all but one data point to construct a model and predict on the remaining data point. Do that \\(n\\) times until all \\(n\\) points were used for prediction once. Compute the test error as an average over all n errors measured, i.e \\[n^{-1}\\sum\\limits_{i = 1}^n \\rho{(Y_{i}, \\hat{m}_{n-1}^{-i}(X_i)})\\] And use that as an approximation of the generalization error. 5.4 K-fold Cross-Validation This method is best explained with a picture. knitr::include_graphics(&quot;figures/k_fold_cv.png&quot;) Here, one splits the data set into k equally sized fold. Then, we use all \\(k-1\\) folds to build a model and the remaining fold to evaluate the model. Then, we average the \\(k\\) estimates of the generalization error. Or in mathematical notation: \\[K^{-1} \\sum\\limits_{k = 1}^K |B_k|^{-1} \\sum\\limits_{i \\in B_k}\\rho({Y_{i}, \\hat{m}^{-B_k}_{n-|B_k|}(X_i))}\\] Note that leave-one out cv is the same as k-fold cross validation with \\(=n\\). 5.4.1 Random Division into test and training data set The problem of K-fold cross-validation is that it depends on one realization of the split into k folds. Instead, we can generalize leave-one-out to leave-d-out. That means, we remove \\(d\\) observations from our initial data, apply our estimation procedure and evaluate on the \\(d\\) observations. \\[\\hat{\\theta}^{-C_k}_{n-k} \\;\\;\\; \\text{for all possible subsets}\\;\\; C_k, \\;\\; k=1, ..., {\\binom{n}{d}}\\] The generalization error can be estimated with \\[{\\binom{n}{d}}^{-1}\\sum\\limits_{k = 1}^{\\binom{n}{d}} d^{-1}\\sum\\limits_{i \\in C_k} \\rho(Y_i, \\hat{m}^{-C_k}_{n-d}(X_i))\\] For \\(d &gt; 3\\), the computational burden becomes immense. For that reason, instead of considering all \\({\\binom{n}{d}}\\) sets, we can uniformly draw \\(B\\) sets (\\(C_1^*, ... C_B^*\\)) from \\(C_1, ..., C_{\\binom{n}{d}}\\) without replacement. For \\(B=\\binom{n}{d}\\), we obviously get the full leave-d-out solution. The computational cost for computing such an approximation to the leave-d-out is linear in \\(B\\) (since evaluating is almost for free). For leave-one-out, the cost is linear in \\(n\\) in the same way. Hence, the stochastic approximation for leave-d-out can be even smaller than for leave-one-out if \\(B &lt; n\\). 5.5 Properties of the different schemes leave-one-out is an asymptotically unbiased estimator for the generalization error and the true prediction. However, we use a sample size \\(n-1\\) instead of \\(n\\), which causes a slight bias (meaning we have less data as we do in a real world scenario, which most likely makes the CV score a tiny little bit worse than it should be). Because the training sets are very similar to each other the leave-one-out scheme has a large variance. leave-d-out has a higher bias than leave-one-out because the sample size is even smaller than \\(n-1\\) (for \\(d&gt;1\\)). However, since we aggregate over more ( instead of \\(n\\)) cv scores, which can be shown to decrease the variance of the final cv estimator. k-fold cv has a higher bias than both leave one out. 5.6 Shortcuts for (some) linear fitting operators Leave-one-out cv score for some linear fitting procedures such as least squares or smoothing spline can be computed via a shortcut when our loss function is \\(\\rho(y, \\hat{y}) = |y-\\hat{y}|^2\\). In particular, we can compute the estimator for such a linear fitting procedure once, compute the linear fitting operator \\(S\\), which satisfies \\(\\mathbf{Y} = \\mathbf{SY}\\) and plug it in this formula: \\[n^{-1}\\sum\\limits_{i = 1}^n \\Bigg(\\frac{Y_i - \\hat{m}(X_i)}{1-S_{ii}}\\Bigg)^2\\] Computing \\(\\mathbf{S}\\) requires \\(O(n)\\) operations (see exercises). Historically, it has been computationally easier to compute the trace of \\(\\mathbf{S}\\) so there is also a quantity called generalized cross validation (which is a misleading terminology), which coincides with the formula above in certain cases. 5.7 Parameter Tuning Example To use leave-one-out cross-validation for parameter tuning of a lasso. Calculate the test set error for one value of lamda. Then, change the value of lamda and recompute the model and then test set error, so that the test set error becomes a function of lamda, as depicted below. "],
["bootstrap.html", "Chapter 6 Bootstrap 6.1 Motivation 6.2 The Bootstrap Distribution 6.3 Bootstrap Consistency 6.4 Boostrap Confidence Intervals 6.5 Boostrap Estimator of the Generalization Error 6.6 Out-of-Boostrap sample for estimating the GE 6.7 Double Boostrap Confidence Intervals 6.8 Three Versions of Boostrap 6.9 Conclusion", " Chapter 6 Bootstrap Bootstrap can be summarized as “simulating from an estimated model” It is used for inference (confidence intervals / hypothesis testing) It can also be used for estimating the predictive power of a model (similarly to cross validation) via out-of-bootstrap generalization error 6.1 Motivation Consider i.i.d. data. \\[ Z_1, .. Z_n \\sim\\ P \\;\\; with \\; \\;Z_i = (X_i, Y_i)\\] And assume a statistical procedure \\[ \\hat{\\theta} = g(Z_1, ..., Z_n) \\] \\(g(\\cdot)\\) can be a point estimator for a regression coefficient, a non-parametric curve estimator or a generalization error estimator based on one new observation, e.g. \\[ \\hat{\\theta}_{n+1} = g(Z_1, ..., Z_{n}, Z_{new}) = (Y_{new} - m_{Z_1, ..., Z_{n}}(X_{new})^2 \\] To make inference, we want to know the distribution of \\(\\hat{\\theta}\\). For some cases, we can derive the distribution analytically if we know the distribution \\(P\\). The central limit theorem states that the sum of random variables approximates a normal distribution with \\(n \\rightarrow \\infty\\). Therefore, we know that the an estimator for the mean of the random variables follows the normal distribution. \\[ \\hat{\\theta}_{n} = n^{-1}\\sum x_i \\sim N(\\mu_x, \\sigma_x^2 / n) \\; \\; \\; n \\rightarrow \\infty \\] for any \\(P\\). However, if \\(\\hat{\\theta}\\) does not involve the sum of random variables, and the CLT does not apply, it’s not as straightforward to obtain the distribution of \\(\\hat{\\theta}\\). Also, if \\(P\\) is not the normal distribution, but some other distribution, we can’t find the distribution of \\(\\hat{\\theta}\\) easily. The script mentions the median estimator as an example for which the variance already depends on the density of \\(P\\). Hence, deriving properties of estimators analytically, even the asymptotic ones only, is a pain. Therefore, if we knew \\(P\\), we could simply simulate many times and get the distribution of \\(\\hat{\\theta}\\) this way. That is, draw many \\(({X_i}^*, {Y_i}^*)\\) from that distribution and compute \\(\\hat{\\theta}\\) for each draw. The problem is that we don’t know \\(P\\). But we have a data sample that was generated from \\(P\\). Hence, we can instead take the empirical distribution \\(\\hat{P}\\)that places probability mass of \\(1/n\\) on each observation, draw a sample from this distribution (which is simply drawing uniformly from our sample with replacement) and compute our estimate of interest from this sample. \\[ \\hat{\\theta}^{*} = g({Z_1}^{*}, ..., {Z_{new}}^{*})\\] We can do that many times to get an approximate distribution for \\(\\hat{\\theta}\\). A crucial assumption is that \\(\\hat{P}\\) reassembles \\(P\\). If our data is not i.i.d, this may not be the case and hence bootstrapping might be misleading. Below, we can see that i.i.d. sampling (red line) reassembles the true distribution (black line) quite well, whereas biased sampling (blue line) obviously does not. We produce a sample that places higher probability mass on the large (absolute) values. library(&quot;tidyverse&quot;) pop &lt;- data_frame(pop = rnorm(10000) * 1:10000) iid &lt;- sample(pop$pop, 1000) # sample iid # sample non-iid: sample is biased towards high absolute values ind &lt;- rbinom(10000, size = 1, prob = seq(0, 1, length.out = 10000)) not_iid &lt;- pop$pop[as.logical(ind)] # get sample not_iid &lt;- sample(not_iid, 1000) # reduce sample size to 1000 out &lt;- data_frame(iid = iid, not_iid = not_iid) %&gt;% gather(type, value, iid, not_iid) ggplot(out, aes(x = value, color = type)) + geom_density() + geom_density(aes(x = pop, color = NULL), data = pop) We can summarize the bootstrap procedure as follows. draw a bootstrap sample \\({Z_1}^{*}, ..., {Z_{n}}^{*}\\) compute your estimator \\({\\hat{\\theta}}^*\\) based on that sample. repeat the first two steps \\(B\\) times to get bootstrap estimators \\({\\hat{\\theta}_1}^*, ..., {\\hat{\\theta}_B}^*\\) and therefore an estimate of the distribution of \\(\\hat{\\theta}\\). Use the \\(B\\) estimated bootstrap estimators as approximations for the bootstrap expectation, quantiles and so on. \\(\\mathbb{E}[\\hat{\\theta}^*_n] \\approx B^{-1}\\sum\\limits_{j = 1}^n \\hat{\\theta}^{* j}_n\\) 6.2 The Bootstrap Distribution With \\(P^*\\), we denote the bootstrap distribution, which is the conditional probability distribution introduced by sampling i.i.d. from the empirical distribution \\(\\hat{P}\\). Hence, \\(P^*\\) of \\({\\hat{\\theta}}^*\\) is the distribution that arises from sampling i.i.d. from \\(\\hat{P}\\) and applying the transformation \\(g(\\cdot)\\) to the data. Conditioning on the data allows us to treat \\(\\hat{P}\\) as fixed. 6.3 Bootstrap Consistency The bootstrap is is called consistent if \\[ \\mathbb{P}[a_n(\\hat{\\theta} - \\theta) \\leq x ] - \\mathbb{P}[a_n(\\hat{\\theta}^* - \\hat{\\theta}) \\leq x ] \\rightarrow 0 \\;\\; (n \\rightarrow \\infty)\\] Consistency of the bootstrap typically holds if the limiting distribution is normal and the samples \\(Z_1, .., Z_n\\) are i.i.d. Consistency of the bootstrap implies consistent variance and bias estimation: \\[ \\frac{Var^* (\\hat{\\theta}^*)}{Var(\\hat{\\theta})} \\rightarrow 1\\] \\[ \\frac{\\mathbb{E}^* (\\hat{\\theta}^*) - \\hat{\\theta}}{\\mathbb{E}(\\hat{\\theta}) - \\theta} \\rightarrow 1\\] You can think of \\(\\theta\\) as the real parameter and \\(\\hat{\\theta}\\) as the estimate based on a sample. Similarly, in the bootstrap world, \\(\\hat{\\theta}\\) is the real parameter, and \\(\\hat{\\theta}^*_i\\) as an estimator of the real parameter \\(\\hat{\\theta}\\). The bootstrap world is an analogue of the real world. So in our bootstrap simulation, we know the true parameter \\(\\hat{\\theta}\\). From our simulation, we get many \\(\\hat{\\theta}^*_i\\) and can find the bootstrap expectation \\(\\mathbb{E}[\\hat{\\theta}^*_n] \\approx B^{-1}\\sum\\limits_{j = 1}^n \\hat{\\theta}^{* j}_n\\). The idea is now to generalize from the boostrap world to the real world, i.e. by saying that the relationship between \\(\\hat{\\theta}^*\\) and \\(\\hat{\\theta}\\) is similar to the one between \\(\\hat{\\theta}\\) and \\(\\theta\\). A simple trick to remember all of this is: if there is no hat, add one if there is a hat, add a star. 6.4 Boostrap Confidence Intervals Note that there confidence intervals are not simply taking the quantiles of the bootstrap distribution. The trick is really to make use of the analogy between the real world and the boostrap world. So when we see our bootstrap expectation \\(\\mathbb{E}[\\hat{\\theta}^*_n]\\) is way higher than \\(\\hat{\\theta}\\), then we also should believe that our \\(\\hat{\\theta}\\) is higher than \\(\\theta\\). The above procedure accounts for that. 6.5 Boostrap Estimator of the Generalization Error We can also use the bootstrap to estimate the generalization error. \\[ \\mathbb{E}[\\rho(Y_{new}, m^*(X_{new}))] \\] We draw a sample \\(({Z_1}^*, ..., {Z_n}^*, Z_{new})\\) from \\(\\hat{P}\\) We compute the bootstrapped estimator \\({m(\\cdot)}^*\\) based on the sample We estimate \\(\\mathbb{E}[\\rho(Y_{new}, {m^*(X_{new})}^*)]\\), which is with respect to both training and test data. We can rewrite the generalization error as follows: \\[ \\mathbb{E}[\\rho(Y_{new}, m^*({X_{new}}^*))] = \\mathbb{E}_{train}[E_{test}[\\rho({Y_{new}}^*, m^*({X_{new}}^*))| train]]\\] Conditioning on the training data in the inner expectation, \\(m(\\cdot)\\) is non-random / fixed. The only random component is \\({Y_{new}}^*\\). Since we draw from the empirical distribution and place a probability mass of \\(1/n\\) on every data point. we can calculate the inner (discrete) expectation easily via \\(\\mathbb{E}(X) = \\sum\\limits_{j = 1}^n p_j * x_j = n^{-1} \\sum\\limits_{j = 1}^n x_j\\). The expectation becomes \\[ \\mathbb{E}_{train}[n^{-1}\\sum\\rho(Y_{i}, m^*(X_{i}))] = n^{-1}\\sum\\mathbb{E}[\\rho(Y_{i}, m^*(X_{i}))]\\] We can see that there is no need to draw \\(Z_{new}\\) from the data. The final algorithm looks as follows: Draw \\(({Z_1}^*, ..., {Z_n}^*)\\) compute bootstrap estimator \\({\\hat{\\theta}}^*\\) Evaluate this estimator on all data points and average over them, i.e \\(err^* = n^{-1} \\sum \\rho(Y_i, m^*(X_i))\\) Repeat steps above B times and average all error estimates to get the bootstrap GE estimate, i.e. \\(GE^* = B^{-1} \\sum {err_i}^*\\) 6.6 Out-of-Boostrap sample for estimating the GE One can criticize the GE estimate above because some samples are used in the test as well as in the training set. This leads to over-optimistic estimations and can be avoided by using the out-of-bootstrap approach. With this technique, we first generate a bootstrap sample to compute our estimator and then use the remaining observations not used in the bootstrap sample to evaluate the estimator. We do that \\(B\\) times and the size of the test set may vary. You can see this as some kind of cross-validation with about 30% of the data used as the test set. The difference is that some observations were used multiple times in the training data, yielding a training set always of size n (instead of - for example \\(n*0.9\\) for 10-fold-CV). 6.7 Double Boostrap Confidence Intervals Confidence intervals are almost never exact, meaning that \\[\\mathbb{P}[\\theta \\in I^{**}(1-\\alpha)] = 1-\\alpha + \\Delta\\] Where \\(I^{**}(1-\\alpha)\\) is a \\(\\alpha\\)-confidence interval. However, by changing the nominal coverage of the confidence interval, it is possible to make the actual coverage equal to an arbitrary value, i.e \\[ \\mathbb{P}[\\theta \\in I^{**}(1-\\alpha&#39;)] = 1-\\alpha\\] The problem is that \\(\\alpha&#39;\\) is unknown. But another level of bootstrap can be used to estimate \\(\\alpha\\), denoted by \\(\\hat{\\alpha}\\), which typically achieves \\[\\mathbb{P}[\\theta \\in I^{**}(1-\\hat{\\alpha}&#39;)] = 1-\\alpha + \\Delta&#39;\\] with \\(\\Delta&#39; \\lt {\\Delta}\\) To implement a double bootstrap confidence interval, proceed as follows: Draw a bootstrap sample \\(({Z_1}^*, ..., {Z_n}^*)\\). From this sample, draw B second-level bootstrap samples and compute the estimator of interest and one confidence interval \\(I^{**}(1-\\alpha)\\) based on B second-level bootstrap samples. evaluate whether \\(\\hat{\\theta}\\) lays within the bootstrap confidence interval from a. \\(cover^*(1-\\alpha) = 1_{[\\hat{\\theta} \\in I^{**}(1-\\alpha)]}\\) Repeat the above M times to get \\(cover^{* 1}, ..., cover^{* M}\\) and hence approximate \\(\\mathbb{P}[\\theta \\in I^{**}(1-\\alpha)]\\) with \\[ p^*(\\alpha) = M^{-1} \\sum\\limits_{m = 1}^M cover^{* m}\\] Vary \\(\\alpha\\) in all of the steps above to find \\(\\alpha&#39;\\) so that \\(p^*(\\alpha&#39;) = 1- \\alpha\\) Question here (see Google docs) 6.8 Three Versions of Boostrap So far, we discussed the fully non-parametric bootstrap, which is simulating from the empirical distribution. On the other extreme of the scale, there is the parametric bootstrap. The middle way is the model-based bootstrap 6.8.1 Non-parametric Regression We draw a bootstrap sample \\(({Z_1}^*, ..., {Z_n}^*) \\sim \\hat{P}\\), i.e. we sample from the empirical distribution data with replacement. 6.8.2 Parametric Boostrap Here, we assume the data are realizations from a known distribution \\(P\\), which is is determined up to some unknown parameter (vector) \\(\\theta\\). That means we sample \\((Z_1, ..., Z_n) \\sim P_{\\hat{\\theta}}\\). For example, take the following regression model \\(y = X\\beta + \\epsilon\\) where we know the errors are Gaussian. We can estimate our regression model, obtain residuals and compute the mean (which is zero) and the standard deviation of them. To generate a bootstrap sample, we simulate residuals \\(\\epsilon^*\\) from \\(N(0, \\hat{\\mu})\\) and Add them to our observed data, i.e. we obtain \\(({Y_1}^*, ..., {Y_n}^*)\\) from \\(X\\beta + \\epsilon^*\\). Hence, the final bootstrap sample we use is \\((x_1, {Y_1}^*), ..., (x_1, {Y_n}^*))\\) where the \\(x_i\\) are just the observed data. We can compute bootstrap estimates \\(\\hat{\\beta}^*\\), the bootstrap expectation \\(\\mathbb{E}^*[\\beta*]\\) as well as confidence intervals for the regression coefficients or generalization errors just as shown in detail above. The difference is only how the bootstrap sample is obtained. Similarly, for time series data, we may assume an AR(p) model. Initializing \\({X_0}^*, ..., {X_{-p+1}}^*\\) with \\(0\\). Generate random noise \\({\\epsilon_1}^*, ..., {\\epsilon_{n+m}}^*\\) according to \\(P_{\\hat{\\theta}}\\). Construct our time series \\({X_t}^* = \\sum\\limits_{j = 1}^p \\hat{\\theta}{X_{t-j}}^* + {\\epsilon_t}^*\\) \\((X_1, ..., X_{n+m})\\) Throw away the first \\(M\\) observations that were used as fade-in. Proceed with the B bootstrap samples \\(({X_1}^*, ..., {X_n}^*)\\) as outlined above for the non-parametric bootstrap to obtain coefficient estimates for \\(\\theta\\), confidence intervals or estimating the generalization error of the model. 6.8.3 Model-Based Bootstrap The middle way is the model based bootstrap. As with the parametric bootstrap, we assume to know the model, e.g. \\(y = m(x) + \\epsilon\\) (where \\(m(\\cdot)\\) might be a non-parametric curve estimator), but we do not make an assumption about the error distribution. Instead of generating \\(\\epsilon^*\\) from the known distribution with unknown parameters (\\(\\epsilon^* \\sim P_{\\hat{\\theta}}\\), as in the parametric bootstrap)), we draw them with replacement from the empirical distribution. To sum it up, these are the steps necessary: Estimate \\(\\hat{m(\\cdot)}\\) from all data. Simulate \\({\\epsilon_1}^*, ..., {\\epsilon_n}^*\\) by drawing from \\(\\hat{P}\\) with replacement. Obtain \\(({Y_1}^*, ..., {Y_n}^*)\\) from \\(\\hat{m}(x) + \\epsilon^*\\). As for the parametric bootstrap, the final bootstrap sample we use is \\((x_1, {Y_1}^*), ..., (x_n, {Y_n}^*))\\) where the \\(x_i\\) are just the observed data. Again, you can use the bootstrap samples as for the other two methods. 6.9 Conclusion Which version of the bootstrap should I use? The answer is classical. If the parametric model fits the data very well, there is no need to estimate the distribution explicitly. Also, if there is very little data, it might be very difficult to estimate \\(P\\). On the other hand, the non-parametric bootstrap is less sensitive to model-misspecification and can deal with arbitrary distributions (? is that true?). "],
["classification.html", "Chapter 7 Classification 7.1 Indirect Classification - The Bayes Classifier 7.2 Direct Classification - The Discriminant View 7.3 Indirect Classification - The View of Logistic Regression", " Chapter 7 Classification 7.1 Indirect Classification - The Bayes Classifier In classification, the goal is to assign observations to a group. Similar to regression, where we have \\(m(x) = E[Y | X = x]\\), we want to assign class abc probabilities to the observations \\[\\pi_j (x) = P[Y = j | X = x] \\;\\;\\; (j = 0,1, ..., J-1) \\] Def: A classifier maps A multidimensional input vector to a class label. Or mathematically: \\(C: \\mathbb{R}^p \\rightarrow \\{0, ..., J-1\\}\\) The quality of a classifier is measured via the zero-one test-error. \\[\\mathbb{P}[C(X_{new}) \\neq Y_{new}] \\] The optimal classifier with respect to the zero-one Error is the Bayes Classifier. It classifies an observation to the group for which the predicted probability was the highest. \\[ C_{bayes}(x) = \\arg\\max_{0&lt;j&lt;J-1}\\pi_j(x)\\] Hence, the Bayes Classifier is a point-wise classifier. For the Bayes Classifier, the zero-one test error is known as the Bayes Risk. \\[ \\mathbb{P}[C_{Bayes}(X_{new}) \\neq Y_{new}]\\] In practice, \\(\\pi_j(\\cdot)\\) is unknown (just as the MSE in regression is unknown) and hence, the the Bayes Classifier and Risk is unknown too. However, we can estimate \\(\\pi_j(\\cdot)\\) from the data and plug it in the Bayes Classifier. \\[\\hat{C}(X) = \\arg\\max_{0&lt;j&lt;J-1}\\hat{\\pi}_j(x)\\] This is an indirect estimator, since we first estimate the class probabilities \\(\\pi_j(\\cdot)\\) for each observation \\(x\\) and then assign the class to it for which the probability was the highest. Question how is that more indirect than Discriminant analysis? Don’t we use the Bayes classifier in the end? 7.2 Direct Classification - The Discriminant View 7.2.1 LDA One example for a direct classification is discriminant analysis. Using Bayes Theorem \\[ \\mathbb{P}[Y = j | X] = \\frac{\\mathbb{P}[X = x | y = j]}{\\mathbb{P}[X = x]}*\\mathbb{P}[Y = j] \\] And assuming \\[ (X| Y) \\sim N_p(\\mu_j, \\Sigma); \\;\\; \\sum\\limits_{k = 0}^{J-1}p_k = 1\\] We can write \\[ \\mathbb{P}[Y = j | X = x] = \\frac{f_{ x | Y = j } * p_j}{\\sum\\limits_{k = 0}^{J-1} f_{x | Y = k} * p_k} \\] Note that there is no distributional assumption on \\(Y\\) so far. You can estimate \\[\\mu_j = \\sum\\limits_{i = 1}^n{x_i*1_{Y_i = j}} / 1_{Y_i = j}\\] and \\[\\Sigma = \\frac{1}{n-j}\\sum\\limits_{j = 0}^{J-1}\\sum\\limits_{i = 1}^n(x_i - \\mu_j)(x_i - \\mu_j)&#39;\\;1_{Y_i = j} \\] Note that the means of the response of the groups are different, but the covariance structure is the same for all of them. We now also need to estimate \\(p_j\\). A straight-forward way is \\[ \\hat{p}_j = n^{-1}\\sum\\limits_{i = 1}^n{1_{[Y_i = j]}} = \\frac{n_j}{n} \\] From here, you can easily compute the classifier (as done in the exercise) by maximizing the log-likelihood. Then, you can derive the decision boundary by using \\(\\delta_j - \\delta_k = 0\\). In a two dimensional predictor space with two classes, the decision boundary is a line. Every combination of the two predictors on one side of the line will result in a prediction of class one, everything on the other side of the line of class two. Note that both the decision function (and hence the decision boundary) are linear in x. 7.2.2 QDA Quadratic discriminant analysis loosens the assumption of shared covariance matrices, namely each group has their own covariance matrix. This leads to quadratic decisions functions \\(\\delta\\) and hence to non-linear decision boundaries. QDA is more flexible but for high \\(p\\), the problem of over-fitting can occur, since the number of variables to estimate is \\(J*p(p+1)\\) variable for the covariance matrix only (instead of \\(p*(p+1)\\) for LDA. 7.3 Indirect Classification - The View of Logistic Regression There are also ways to come up with indirect assignment of the class label, namely via the Bayes classifier \\(\\hat{C}(X) = \\arg\\max_{0&lt;j&lt;J-1}\\hat{\\pi}_j(x)\\). The logistic model for \\(\\pi_j(x) = \\mathbb{P}(Y = j | X = x)\\) is \\(\\log(\\frac{\\pi_j(x)}{1-\\pi_j(x)}) = g(\\cdot)\\) or \\(\\pi_j(x) = \\frac{e^{g(\\cdot)}}{1+ e^{g(\\cdot)}}\\) equivalently. That model maps the real value \\(g(\\cdot)\\) can take to the interval \\((0, 1)\\), which gives a natural interpretation of the reponse as a probability. The response variable \\(Y_1, ..., Y_n\\) are distributed according to a Bernulli distribution, ie. \\(Y_1, ..., Y_n \\sim \\textrm{Bernulli}(\\pi(x_i))\\). The logistic model belongs to the class of generalized linear models. These models have three characteristics: A link function (in our case the logit function \\(\\log(\\frac{\\pi_j(x)}{1-\\pi_j(x)}) = g(x)\\)). A response distribution (i.e. Bernulli) The concrete form of \\(g(\\cdot)\\) (in logistic regression most often just a linear model \\(g(x) = \\sum\\limits_{j = 1}^p \\beta_j x_j\\)). This allows us to write down the likelihood of the logistic model \\[\\begin{equation} \\begin{split} L(\\beta, (x_1, Y_1), ..., (x_n, Y_n))&amp; = \\prod\\limits_{i = 1}^n \\mathbb{P}(Y_i = y_i)\\\\ &amp; =\\prod\\limits_{i = 1}^n \\pi(x_i)^{Y_i-1}(1-\\pi(x_i))^{Y_i-1}\\\\ \\log(L(\\cdot) &amp; = \\sum\\limits_{i = 1}^n Y_i \\pi(x_i) + (1 - Y_i) (1 - \\pi(x_i)) \\end{split} \\tag{7.1} \\end{equation}\\] "],
["flexible-regression-and-classification-methods.html", "Chapter 8 Flexible regression and classification methods", " Chapter 8 Flexible regression and classification methods "],
["bagging-and-boosting.html", "Chapter 9 Bagging and Boosting", " Chapter 9 Bagging and Boosting "],
["intro.html", "Chapter 10 Introduction", " Chapter 10 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 10. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 10.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 10.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 10.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 10.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package [@R-bookdown] in this sample book, which was built on top of R Markdown and knitr [@xie2015]. "],
["references.html", "References", " References "]
]
