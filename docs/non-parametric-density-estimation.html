<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="multiple-linear-regression.html">
<link rel="next" href="non-parametric-regression.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
<li class="chapter" data-level="2.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>2.9</b> Model Selection</a><ul>
<li class="chapter" data-level="2.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mallows-cp-statistic"><i class="fa fa-check"></i><b>2.9.1</b> Mallow’s cp statistic</a></li>
<li class="chapter" data-level="2.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#search-strategies"><i class="fa fa-check"></i><b>2.9.2</b> Search strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
<li class="chapter" data-level="4.7" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#smoothing-splines"><i class="fa fa-check"></i><b>4.7</b> Smoothing splines</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression and Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
<li class="chapter" data-level="11.3" data-path="round-up.html"><a href="round-up.html#cheatsheet"><i class="fa fa-check"></i><b>11.3</b> Cheatsheet</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-parametric-density-estimation" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Non-parametric Density Estimation</h1>
<div id="the-histogramm" class="section level2">
<h2><span class="header-section-number">3.1</span> The Histogramm</h2>
<p>The simplest density estimator is the histogram. The drawback is that we need to specify two parameters. The origin <span class="math inline">\(x_0\)</span> and the bandwidth <span class="math inline">\(h\)</span>. At least the former is highly arbitrary and both affect the resulting histogram remarkably.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">30</span>, <span class="dt">center =</span> <span class="dv">0</span>) <span class="co"># center is what we called x_0</span></code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="kernels" class="section level2">
<h2><span class="header-section-number">3.2</span> Kernels</h2>
<div id="the-naive-estimator" class="section level3">
<h3><span class="header-section-number">3.2.1</span> The naive Estimator</h3>
<p>We can compute the relative frequency of observations falling into some region and use these frequencies as estimators for the probabilities.</p>
<p>Let’s consider some example data distributed uniformly at random between <span class="math inline">\(-0.5\)</span> and <span class="math inline">\(+0.5\)</span>. Let us try to estimate the densities via the relative frequencies. We consider three points in x: <span class="math inline">\(-0.5, 0\)</span> and <span class="math inline">\(+0.5\)</span>. We set <span class="math inline">\(h\)</span> to 1/2 for now. For x equal zero, we know that all points are comprised in the range, so the relative frequency is one, for the two other x values, it is a half each. We can interpolate and then the a triangle.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">10000</span>, <span class="dt">min =</span> <span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">max =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)
dens &lt;-<span class="st"> </span><span class="kw">density</span>(x, <span class="dt">kernel =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">bw =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)


eval_one &lt;-<span class="st"> </span><span class="cf">function</span>(e_eval_one, x, h) {
  <span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>h)<span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(e_eval_one <span class="op">+</span><span class="st"> </span>h <span class="op">&gt;</span><span class="st"> </span>x <span class="op">&amp;</span><span class="st"> </span>e_eval_one <span class="op">-</span><span class="st"> </span>h <span class="op">&lt;</span><span class="st"> </span>x)
}

naive_density &lt;-<span class="st"> </span><span class="cf">function</span>(x, h, x_eval) {
  <span class="kw">data_frame</span>(
    <span class="dt">x_eval =</span> x_eval, 
    <span class="dt">y_est =</span> <span class="kw">map_dbl</span>(x_eval, eval_one, <span class="dt">x =</span> x, <span class="dt">h =</span> h),
    <span class="dt">h =</span> h
  )
}

one_est &lt;-<span class="st"> </span><span class="kw">naive_density</span>(x, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">3</span>))

<span class="kw">ggplot</span>(one_est, <span class="kw">aes</span>(<span class="dt">x =</span> x_eval, <span class="dt">y =</span> y_est)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_path</span>()</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We don’t need to normalize anything since the area has size one already, which we need for it to be a density. However, if we use a larger <span class="math inline">\(h\)</span>, for example <span class="math inline">\(h = 1\)</span>, we need to divide by <span class="math inline">\(2\)</span> to get an area of size one since at all three points, the relative frequency is one and it becomes zero only when the absolute value of x is larger than 3/2. Hence, we found the formula to obtain densities. <span class="math display">\[ f(x) = \frac{1}{2h}\mathbb{P}[x-h&lt; X &lt; x+h] \;\;\; \text{for}\;\; h \rightarrow 0\]</span> Which translates into frequencies for our estimator <span class="math display">\[ \hat{f}(x) = \frac{1}{2hn}\#(X_i \in [x-h, x+h))\]</span> This is quite wrong, as we know the true distribution is uniform between minus one and one. However, as you can see by playing around with the shiny app further down the page, it seems that with <span class="math inline">\(h \rightarrow \infty\)</span>, we are approaching the true distribution. The nice thing about this estimator is that there is no need to set the tuning parameter <span class="math inline">\(x_0\)</span> anymore.</p>
<p>You can represent the naive kernel differently, namely as a kernel. We have <span class="math display">\[
\begin{split}
f(x) = &amp; \frac{1}{2nh}\#\{X_i \in [x- d, x+ d)\} \\
     = &amp; \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[x_i \in [x- d, x+ d)]} \\
     = &amp; \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[(x_i - x)/h \in [-1, 1)]} \\
     = &amp; \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[(x - x_i)/h \in [-1, 1)]} | \text{You can flip signs} \\
     = &amp; \frac{1}{2nh} \sum\limits_{i = 1}^n w&#39;\big(\frac{x -x_i}{h}\big) \\
\end{split}
\]</span></p>
<p><span class="math display">\[
w&#39;\big(\frac{x - x_i}{h}\big) = \begin{cases}
      1 \;\;\ \text{if} \;\;\ | \frac{x - x_i}{h} | &gt; 1
      \\
      0 \;\;\ \text{else}.
      \end{cases}
\]</span> Where we can collapse <span class="math inline">\(2\)</span> and our weight function <span class="math inline">\(w&#39;(x)\)</span>, so we end up with the following.</p>
<p><span class="math display">\[
\begin{split}
f(x) = &amp; \frac{1}{nh} \sum\limits_{i = 1}^n w\big(\frac{x - x_i}{h}\big) \\
\end{split}
\]</span></p>
<p><span class="math display">\[
w\big(\frac{x - x_i}{h}\big) = \begin{cases}
      \frac{1}{2} \;\;\ \text{if} \;\;\ | \frac{x - x_i}{h} | &gt; 1
      \\
      0 \;\;\ \text{else}.
      \end{cases}
\]</span></p>
</div>
<div id="other-kernels" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Other Kernels</h3>
<p>The function <span class="math inline">\(w\)</span> from above is an example of a kernel function since it satisfies the condition of a kernel, which are</p>
<ul>
<li>integrates to one, <span class="math inline">\(\int\limits_{-\infty}^\infty K(X)dx = 1\)</span></li>
<li>is symmetric, <span class="math inline">\(K(x) = K(-X)\)</span></li>
<li>is strictly positive, <span class="math inline">\(K(X)\geqslant 0\)</span></li>
</ul>
<p><span class="math inline">\(w(\cdot)\)</span> is the rectangular kernel, which becomes obvious when plotting it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rectangular &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="kw">if_else</span>(<span class="kw">abs</span>(x) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>)
}

normal &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  (<span class="dv">2</span><span class="op">*</span>pi)<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)
}

cosine &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="kw">if_else</span>(<span class="kw">abs</span>(x) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>, pi <span class="op">/</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x), <span class="dv">0</span>)
}

<span class="kw">ggplot</span>(<span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="dv">0</span>), <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> rectangular, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;rectangular&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> normal, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;normal&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> cosine, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;cosine&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>))</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>It is <em>piece-wise</em> constant, since every point is either in or out of the interval. The density estimate is not smooth since the kernel is not smooth. <span class="math inline">\(\hat{f}(x)\)</span> inherits also other properties from the kernel, e.g. it is strictly positive if the kernel is strictly positive and it integrates to one if the kernel does so. Therefore, to obtain a smooth density estimate, we need to choose a smooth kernel. The Gaussian kernel seems to be a natural choice. Instead of giving an observation a weight of either one or zero, values in between are also possible, depending on the distance of the point relative to the point for which we want to obtain a density estimate and depending on <span class="math inline">\(h\)</span>. Some kernels such as the normal kernel have the drawback that they their weights are never getting to zero, which implies computational cost at limited gain. We can instead choose a kernel like the cosine kernel, which also smooth but reaches zero relatively quickly, which speeds up calculations.</p>
</div>
</div>
<div id="the-bandwidth" class="section level2">
<h2><span class="header-section-number">3.3</span> The Bandwidth</h2>
<p>More important than the kernel is the bandwidth. A small bandwidth yields a very wiggly function (since only the points in the very close neighborhood are given a significant weight), a large bandwidth will produce a function that is varying slowly as a function of x. The bandwidth is closely related to the bias-variance trade-off.</p>
<p><strong>A large bandwidth means a large bias and a low variance, a small bandwidth the opposite.</strong></p>
<p>For the extreme cases of h, we have:</p>
<ul>
<li>For a very large bandwidth, the curve estimate becomes the kernel.</li>
<li>For a very small bandwidth, the deity takes the shape of many small kernels. Around each point, a kernel with density <span class="math inline">\(1/n\)</span> forms. Hence, they integrate to one. Points may be close to each other, so the mini kernels overlap.</li>
</ul>
<p>We can also use locally varying bandwidths. In regions where data is sparse we want to use larger bandwidths. A simple approach is to use the <strong>k nearest neighbour</strong> of each x as the local bandwidth for each x. Note that with such bandwidths the density will <strong>not necessarily integrate to one</strong> anymore.</p>
<p>We can decompose the mean squared error of an estimator evaluated at a point x similar to the one of a random variable (since the estimator is derived from realizations of random variables).</p>
<span class="math display">\[\begin{equation}
\begin{split}

Var(\hat{f(x)}) &amp; = Var(\hat{f(x)} - f(x)) \\
             &amp; = \mathbb{E}[(\hat{f(x)} - f(x))^2] - \mathbb{E}[(\hat{f(x)} - f(x))]^2 \\
             &amp; = \text{MSE(f(x))} - \text{bias}^2 \\
\text{MSE(f(x))} &amp; = \text{bias}^2 + Var(\hat{f(x)}) \\
\end{split}
\end{equation}\]</span>
<p>Instead of minimizing the mean squared error at <em>one</em> point, we minimize the integrated mean square error over the support of x.</p>
<p><span class="math display">\[ \text{IMSE}(x)  = \int \text{MSE}(x)dx\]</span></p>
</div>
<div id="bringing-it-all-together" class="section level2">
<h2><span class="header-section-number">3.4</span> Bringing it all together</h2>
<p>Here is a shiny app that lets you try different parameters. <iframe src="https://lorenzwalthert.shinyapps.io/naive_density/?showcase=0" width="672" height="1000px"></iframe></p>
</div>
<div id="other-density-estimators" class="section level2">
<h2><span class="header-section-number">3.5</span> Other Density Estimators</h2>
<p>In regions where data is sparse, your estimate will typically be close to zero, but it cannot become negative. This constraint is a bit nasty. If you take logs, you no longer have such constraints. The constraint is just that the exp of your estimate has to integrate to one. So you estimate the log of <span class="math inline">\(f\)</span> and exponentiate to get <span class="math inline">\(f\)</span>.</p>
</div>
<div id="higher-dimensions" class="section level2">
<h2><span class="header-section-number">3.6</span> Higher Dimensions</h2>
<p>Very similar to one-dimensional density estimation, you can estimate densities of multi-dimensional data. The main difference is that the kernel is now multivariate.</p>
<p><span class="math display">\[ \hat{f}(\mathbf{x}) = \frac{1}{nh^d}\sum\limits_{j = 1}^n 
K\Big(\frac{\mathbf{x}- \mathbf{X_i}}{h}\Big)\]</span></p>
<p>A simple multivariate kernel is the product of uni-variate kernels. <span class="math display">\[K(\mathbf{u}) = \prod\limits_{j = 1}^dK_{univ}(u_j)\]</span></p>
<p>If in addition, the kernel should be <em>radially symmetric</em> (which means a function that is constant in circles around the origin, which also means that the the value the kernel takes only depends on the distance to the origin), it can be shown that the only kernel that meets this two conditions, i.e. that is</p>
<ul>
<li>is a product kernel</li>
<li>is radially symmetric</li>
</ul>
<p>is the Gaussian kernel <span class="math inline">\(K(\mathbf{u}) = ce^{-\frac{1}{2}u_1^2} * ce^{-\frac{1}{2}u_2^2} ... = ce^{-\frac{1}{2}\mathbf{u}&#39;\mathbf{u}} = ce^{-\frac{1}{2}\|u\|^2}\)</span></p>
<div id="the-curse-of-dimensionality" class="section level3">
<h3><span class="header-section-number">3.6.1</span> The Curse of Dimensionality</h3>
<p>The curse of dimensionality is the phenomena that data becomes sparse if the number of dimensions increase. That means that for every other dimension we add, we must add over-proportionally many data points to keep the distance between the data points at a reasonable (low) level.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-parametric-regression.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
