<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cross-validation.html">
<link rel="next" href="classification.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
<li class="chapter" data-level="2.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>2.9</b> Model Selection</a><ul>
<li class="chapter" data-level="2.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mallows-cp-statistic"><i class="fa fa-check"></i><b>2.9.1</b> Mallow’s cp statistic</a></li>
<li class="chapter" data-level="2.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#search-strategies"><i class="fa fa-check"></i><b>2.9.2</b> Search strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression and Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
<li class="chapter" data-level="11.3" data-path="round-up.html"><a href="round-up.html#cheatsheet"><i class="fa fa-check"></i><b>11.3</b> Cheatsheet</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bootstrap" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Bootstrap</h1>
<ul>
<li>Bootstrap can be summarized as “simulating from an estimated model”</li>
<li>It is used for inference (confidence intervals / hypothesis testing)</li>
<li>It can also be used for estimating the predictive power of a model (similarly to cross validation) via out-of-bootstrap generalization error</li>
</ul>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">6.1</span> Motivation</h2>
<p>Consider i.i.d. data. <span class="math display">\[ Z_1, .. Z_n \sim\ P \;\; with \; \;Z_i = (X_i, Y_i)\]</span> And assume a statistical procedure <span class="math display">\[ \hat{\theta} = g(Z_1, ..., Z_n) \]</span> <span class="math inline">\(g(\cdot)\)</span> can be a point estimator for a regression coefficient, a non-parametric curve estimator or a generalization error estimator based on one new observation, e.g. <span class="math display">\[ \hat{\theta}_{n+1} = g(Z_1, ..., Z_{n}, Z_{new}) = (Y_{new} - m_{Z_1, ..., Z_{n}}(X_{new}))^2 \]</span> To make inference, we want to know the distribution of <span class="math inline">\(\hat{\theta}\)</span>. For some cases, we can derive the distribution analytically if we know the distribution <span class="math inline">\(P\)</span>. The central limit theorem states that the sum of random variables approximates a normal distribution with <span class="math inline">\(n \rightarrow \infty\)</span>. Therefore, we know that the an estimator for the mean of the random variables follows the normal distribution. <span class="math display">\[ \hat{\theta}_{n} = n^{-1}\sum x_i \sim N(\mu_x, \sigma_x^2 / n) \; \; \; n \rightarrow \infty \]</span> for <em>any</em> <span class="math inline">\(P\)</span>. However, if <span class="math inline">\(\hat{\theta}\)</span> does not involve the sum of random variables, and the CLT does not apply, it’s not as straightforward to obtain the distribution of <span class="math inline">\(\hat{\theta}\)</span>. Also, if <span class="math inline">\(P\)</span> is not the normal distribution, but some other distribution, we can’t find the distribution of <span class="math inline">\(\hat{\theta}\)</span> easily. The script mentions the median estimator as an example for which the variance already depends on the density of <span class="math inline">\(P\)</span>. Hence, deriving properties of estimators analytically, even the asymptotic ones only, is a pain. Therefore, if we knew <span class="math inline">\(P\)</span>, we could simply simulate many times and get the distribution of <span class="math inline">\(\hat{\theta}\)</span> this way. That is, draw many <span class="math inline">\(({X_i}^*, {Y_i}^*)\)</span> from that distribution and compute <span class="math inline">\(\hat{\theta}\)</span> for each draw.</p>
<p>The problem is that we don’t know <span class="math inline">\(P\)</span>. But we have a data sample that was generated from <span class="math inline">\(P\)</span>. Hence, we can instead take the <strong>empirical</strong> distribution <span class="math inline">\(\hat{P}\)</span> that places probability mass of <span class="math inline">\(1/n\)</span> on each observation, draw a sample from this distribution (which is simply drawing uniformly at random from our sample with replacement) and compute our estimate of interest from this sample. <span class="math display">\[ \hat{\theta}^{*} = g({Z_1}^{*}, ..., {Z_{new}}^{*})\]</span> We can do that many times to get an approximate distribution for <span class="math inline">\(\hat{\theta}\)</span>. A crucial assumption is that <span class="math inline">\(\hat{P}\)</span> reassembles <span class="math inline">\(P\)</span>. If our data is not i.i.d, this may not be the case and hence bootstrapping might be misleading. Below, we can see that i.i.d. sampling (red line) reassembles the true distribution (black line) quite well, whereas biased sampling (blue line) obviously does not. We produce a sample that places higher probability mass on the large (absolute) values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
pop &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">pop =</span> <span class="kw">rnorm</span>(<span class="dv">10000</span>) *<span class="st"> </span><span class="dv">1</span>:<span class="dv">10000</span>) 
iid &lt;-<span class="st"> </span><span class="kw">sample</span>(pop$pop, <span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="co"># sample iid</span>

<span class="co"># sample non-iid: sample is biased towards high absolute values</span>
ind &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">10000</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">10000</span>)) 
not_iid &lt;-<span class="st"> </span>pop$pop[<span class="kw">as.logical</span>(ind)] <span class="co"># get sample</span>
not_iid &lt;-<span class="st"> </span><span class="kw">sample</span>(not_iid, <span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="co"># reduce sample size to 1000</span>

out &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">iid =</span> iid, <span class="dt">not_iid =</span> not_iid) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(type, value, iid, not_iid)
<span class="kw">ggplot</span>(out, <span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">color =</span> type)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="kw">aes</span>(<span class="dt">x =</span> pop, <span class="dt">color =</span> <span class="ot">NULL</span>), <span class="dt">data =</span> pop)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>We can summarize the bootstrap procedure as follows.</p>
<ul>
<li>draw a bootstrap sample <span class="math inline">\({Z_1}^{*}, ..., {Z_{n}}^{*}\)</span></li>
<li>compute your estimator <span class="math inline">\({\hat{\theta}}^*\)</span> based on that sample.</li>
<li>repeat the first two steps <span class="math inline">\(B\)</span> times to get bootstrap estimators <span class="math inline">\({\hat{\theta}_1}^*, ..., {\hat{\theta}_B}^*\)</span> and therefore an estimate of the distribution of <span class="math inline">\(\hat{\theta}\)</span>.</li>
</ul>
<p>Use the <span class="math inline">\(B\)</span> estimated bootstrap estimators as approximations for the bootstrap expectation, quantiles and so on. <span class="math inline">\(\mathbb{E}[\hat{\theta}^*_n] \approx B^{-1}\sum\limits_{j = 1}^n \hat{\theta}^{* j}_n\)</span></p>
</div>
<div id="the-bootstrap-distribution" class="section level2">
<h2><span class="header-section-number">6.2</span> The Bootstrap Distribution</h2>
<p>With <span class="math inline">\(P^*\)</span>, we denote the bootstrap distribution, which is the conditional probability distribution introduced by sampling i.i.d. from the empirical distribution <span class="math inline">\(\hat{P}\)</span>. Hence, <span class="math inline">\(P^*\)</span> of <span class="math inline">\({\hat{\theta}}^*\)</span> is the distribution that arises from sampling i.i.d. from <span class="math inline">\(\hat{P}\)</span> and applying the transformation <span class="math inline">\(g(\cdot)\)</span> to the data. Conditioning on the data allows us to treat <span class="math inline">\(\hat{P}\)</span> as fixed.</p>
</div>
<div id="bootstrap-consistency" class="section level2">
<h2><span class="header-section-number">6.3</span> Bootstrap Consistency</h2>
<p>The bootstrap is is called consistent if <span class="math display">\[ \mathbb{P}[a_n(\hat{\theta} - \theta) \leq x ] - \mathbb{P}[a_n(\hat{\theta}^* - \hat{\theta}) \leq x ] \rightarrow 0 \;\; (n \rightarrow \infty)\]</span> Consistency of the bootstrap typically holds if the limiting distribution is normal and the samples <span class="math inline">\(Z_1, .., Z_n\)</span> are i.i.d. Consistency of the bootstrap implies consistent variance and bias estimation:</p>
<p><span class="math display">\[ \frac{Var^* (\hat{\theta}^*)}{Var(\hat{\theta})} \rightarrow 1\]</span> <span class="math display">\[ \frac{\mathbb{E}^* (\hat{\theta}^*) - \hat{\theta}}{\mathbb{E}(\hat{\theta}) - \theta} \rightarrow 1\]</span> You can think of <span class="math inline">\(\theta\)</span> as the real parameter and <span class="math inline">\(\hat{\theta}\)</span> as the estimate based on a sample. Similarly, in the bootstrap world, <span class="math inline">\(\hat{\theta}\)</span> is the <em>real</em> parameter, and <span class="math inline">\(\hat{\theta}^*_i\)</span> as an estimator of the <em>real</em> parameter <span class="math inline">\(\hat{\theta}\)</span>. The bootstrap world is an analogue of the real world. So in our bootstrap simulation, we know the <em>true</em> parameter <span class="math inline">\(\hat{\theta}\)</span>. From our simulation, we get many <span class="math inline">\(\hat{\theta}^*_i\)</span> and can find the bootstrap expectation <span class="math inline">\(\mathbb{E}[\hat{\theta}^*_n] \approx B^{-1}\sum\limits_{j = 1}^n \hat{\theta}^{* j}_n\)</span>. The idea is now to generalize from the <em>boostrap</em> world to the <em>real</em> world, i.e. by saying that the relationship between <span class="math inline">\(\hat{\theta}^*\)</span> and <span class="math inline">\(\hat{\theta}\)</span> is similar to the one between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta\)</span>.</p>
<p>A simple trick to remember all of this is:</p>
<ul>
<li>if there is no hat, add one</li>
<li>if there is a hat, add a star.</li>
</ul>
</div>
<div id="boostrap-confidence-intervals" class="section level2">
<h2><span class="header-section-number">6.4</span> Boostrap Confidence Intervals</h2>
<p><img src="figures/pivot_ci.png" width="579" /></p>
<p>Note that there confidence intervals are not simply taking the quantiles of the bootstrap distribution. The trick is really to make use of the analogy between the <em>real</em> world and the <em>boostrap</em> world. So when we see our bootstrap expectation <span class="math inline">\(\mathbb{E}[\hat{\theta}^*_n]\)</span> is way higher than <span class="math inline">\(\hat{\theta}\)</span>, then we also should believe that our <span class="math inline">\(\hat{\theta}\)</span> is higher than <span class="math inline">\(\theta\)</span>. The above procedure accounts for that.</p>
</div>
<div id="boostrap-estimator-of-the-generalization-error" class="section level2">
<h2><span class="header-section-number">6.5</span> Boostrap Estimator of the Generalization Error</h2>
<p>We can also use the bootstrap to estimate the generalization error. <span class="math display">\[ \mathbb{E}[\rho(Y_{new}, m^*(X_{new}))] \]</span></p>
<ul>
<li>We draw a sample <span class="math inline">\(({Z_1}^*, ..., {Z_n}^*, Z_{new})\)</span> from <span class="math inline">\(\hat{P}\)</span></li>
<li>We compute the bootstrapped estimator <span class="math inline">\({m(\cdot)}^*\)</span> based on the sample</li>
<li>We estimate <span class="math inline">\(\mathbb{E}[\rho(Y_{new}, {m^*(X_{new})}^*)]\)</span>, which is with respect to both training and test data.</li>
</ul>
<p>We can rewrite the generalization error as follows: <span class="math display">\[ \mathbb{E}[\rho(Y_{new}, m^*({X_{new}}^*))] = \mathbb{E}_{train}[E_{test}[\rho({Y_{new}}^*, m^*({X_{new}}^*))| train]]\]</span> Conditioning on the training data in the inner expectation, <span class="math inline">\(m(\cdot)\)</span> is non-random / fixed. The only random component is <span class="math inline">\({Y_{new}}^*\)</span>. Since we draw from the empirical distribution and place a probability mass of <span class="math inline">\(1/n\)</span> on every data point. we can calculate the inner (discrete) expectation easily via <span class="math inline">\(\mathbb{E}(X) = \sum\limits_{j = 1}^n p_j * x_j = n^{-1} \sum\limits_{j = 1}^n x_j\)</span>. The expectation becomes <span class="math display">\[ \mathbb{E}_{train}[n^{-1}\sum\rho(Y_{i}, m^*(X_{i}))] =  n^{-1}\sum\mathbb{E}[\rho(Y_{i}, m^*(X_{i}))]\]</span></p>
<p>We can see that there is no need to draw <span class="math inline">\(Z_{new}\)</span> from the data. The final algorithm looks as follows:</p>
<ul>
<li>Draw <span class="math inline">\(({Z_1}^*, ..., {Z_n}^*)\)</span></li>
<li>compute bootstrap estimator <span class="math inline">\({\hat{\theta}}^*\)</span></li>
<li>Evaluate this estimator on all data points and average over them, i.e <span class="math inline">\(err^* = n^{-1} \sum \rho(Y_i, m^*(X_i))\)</span></li>
<li>Repeat steps above B times and average all error estimates to get the bootstrap GE estimate, i.e. <span class="math inline">\(GE^* = B^{-1} \sum {err_i}^*\)</span></li>
</ul>
</div>
<div id="out-of-boostrap-sample-for-estimating-the-ge" class="section level2">
<h2><span class="header-section-number">6.6</span> Out-of-Boostrap sample for estimating the GE</h2>
<p>One can criticize the GE estimate above because some samples are used in the <strong>test as well as in the training set</strong>. This leads to <strong>over-optimistic estimations</strong> and can be avoided by using the out-of-bootstrap approach. With this technique, we first generate a bootstrap sample to compute our estimator and then use the remaining observations not used in the bootstrap sample to evaluate the estimator. We do that <span class="math inline">\(B\)</span> times and the size of the test set may vary. You can see this as some kind of cross-validation with about 30% of the data used as the test set. The difference is that some observations were used multiple times in the training data, yielding a <strong>training set always of size n</strong> (instead of - for example <span class="math inline">\(n*0.9\)</span> for 10-fold-CV).</p>
</div>
<div id="double-boostrap-confidence-intervals" class="section level2">
<h2><span class="header-section-number">6.7</span> Double Boostrap Confidence Intervals</h2>
<p>Confidence intervals are almost never exact, meaning that <span class="math display">\[\mathbb{P}[\theta \in I^{**}(1-\alpha)] = 1-\alpha + \Delta\]</span> Where <span class="math inline">\(I^{**}(1-\alpha)\)</span> is a <span class="math inline">\(\alpha\)</span>-confidence interval. However, by changing the <em>nominal</em> coverage of the confidence interval, it is possible to make the actual coverage equal to an arbitrary value, i.e</p>
<p><span class="math display">\[ \mathbb{P}[\theta \in I^{**}(1-\alpha&#39;)] = 1-\alpha\]</span> The problem is that <span class="math inline">\(\alpha&#39;\)</span> is unknown. But another level of bootstrap can be used to <strong>estimate</strong> <span class="math inline">\(\alpha\)</span>, denoted by <span class="math inline">\(\hat{\alpha}\)</span>, which typically achieves <span class="math display">\[\mathbb{P}[\theta \in I^{**}(1-\hat{\alpha}&#39;)] = 1- \alpha + \Delta&#39;\]</span> with <span class="math inline">\(\Delta&#39; &lt; \Delta\)</span></p>
<p>To implement a double bootstrap confidence interval, proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>Draw a bootstrap sample <span class="math inline">\(({Z_1}^*, ..., {Z_n}^*)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>From this sample, draw B second-level bootstrap samples and compute the estimator of interest and <em>one</em> confidence interval <span class="math inline">\(I^{**}(1-\alpha)\)</span> based on B second-level bootstrap samples.</li>
<li>evaluate whether <span class="math inline">\(\hat{\theta}\)</span> lays within the bootstrap confidence interval from a. <span class="math inline">\(cover^*(1-\alpha) = 1_{[\hat{\theta} \in I^{**}(1-\alpha)]}\)</span></li>
</ol></li>
<li>Repeat the above M times to get <span class="math inline">\(cover^{* 1}, ..., cover^{* M}\)</span> and hence approximate <span class="math inline">\(\mathbb{P}[\theta \in I^{**}(1-\alpha)]\)</span> with <span class="math display">\[ p^*(\alpha) = M^{-1} \sum\limits_{m = 1}^M cover^{* m}\]</span></li>
<li>Vary <span class="math inline">\(\alpha\)</span> in all of the steps above to find <span class="math inline">\(\alpha&#39;\)</span> so that <span class="math inline">\(p^*(\alpha&#39;) = 1- \alpha\)</span></li>
</ol>
<p>Question here (see Google docs)</p>
</div>
<div id="three-versions-of-boostrap" class="section level2">
<h2><span class="header-section-number">6.8</span> Three Versions of Boostrap</h2>
<ul>
<li>So far, we discussed the <strong>fully non-parametric</strong> bootstrap, which is simulating from the empirical distribution.</li>
<li>On the other extreme of the scale, there is the <strong>parametric bootstrap</strong>.</li>
<li>The middle way is the model-based bootstrap</li>
</ul>
<div id="non-parametric-regression-1" class="section level3">
<h3><span class="header-section-number">6.8.1</span> Non-parametric Regression</h3>
<p>We draw a bootstrap sample <span class="math inline">\(({Z_1}^*, ..., {Z_n}^*) \sim \hat{P}\)</span>, i.e. we sample from the <em>empirical distribution</em> data with replacement.</p>
</div>
<div id="parametric-boostrap" class="section level3">
<h3><span class="header-section-number">6.8.2</span> Parametric Boostrap</h3>
<p>Here, we assume the data are realizations from a <em>known</em> distribution <span class="math inline">\(P\)</span>, which is is determined up to some unknown parameter (vector) <span class="math inline">\(\theta\)</span>. That means we sample <span class="math inline">\((Z_1, ..., Z_n) \sim P_{\hat{\theta}}\)</span>. For example, take the following regression model <span class="math inline">\(y = X\beta + \epsilon\)</span> where we know the errors are Gaussian.</p>
<ol style="list-style-type: decimal">
<li>We can estimate our regression model, obtain residuals and compute the mean (which is zero) and the standard deviation of them.</li>
<li>To generate a bootstrap sample, we simulate residuals <span class="math inline">\(\epsilon^*\)</span> from <span class="math inline">\(N(0, \hat{\mu})\)</span> and</li>
<li>Add them to our observed data, i.e. we obtain <span class="math inline">\(({Y_1}^*, ..., {Y_n}^*)\)</span> from <span class="math inline">\(X\beta + \epsilon^*\)</span>. Hence, the final bootstrap sample we use is <span class="math inline">\((x_1, {Y_1}^*), ..., (x_1, {Y_n}^*))\)</span> where the <span class="math inline">\(x_i\)</span> are just the observed data.</li>
<li>We can compute bootstrap estimates <span class="math inline">\(\hat{\beta}^*\)</span>, the bootstrap expectation <span class="math inline">\(\mathbb{E}^*[\beta*]\)</span> as well as confidence intervals for the regression coefficients or generalization errors just as shown in detail above. The difference is only <em>how</em> the bootstrap sample is obtained.</li>
</ol>
<p>Similarly, for time series data, we may assume an AR(p) model.</p>
<ol style="list-style-type: decimal">
<li>Initializing <span class="math inline">\({X_0}^*, ..., {X_{-p+1}}^*\)</span> with <span class="math inline">\(0\)</span>.</li>
<li>Generate random noise <span class="math inline">\({\epsilon_1}^*, ..., {\epsilon_{n+m}}^*\)</span> according to <span class="math inline">\(P_{\hat{\theta}}\)</span>.</li>
<li>Construct our time series <span class="math inline">\({X_t}^* = \sum\limits_{j = 1}^p \hat{\theta}{X_{t-j}}^* + {\epsilon_t}^*\)</span> <span class="math inline">\((X_1, ..., X_{n+m})\)</span></li>
<li>Throw away the first <span class="math inline">\(M\)</span> observations that were used as fade-in.</li>
<li>Proceed with the B bootstrap samples <span class="math inline">\(({X_1}^*, ..., {X_n}^*)\)</span> as outlined above for the non-parametric bootstrap to obtain coefficient estimates for <span class="math inline">\(\theta\)</span>, confidence intervals or estimating the generalization error of the model.</li>
</ol>
</div>
<div id="model-based-bootstrap" class="section level3">
<h3><span class="header-section-number">6.8.3</span> Model-Based Bootstrap</h3>
<p>The middle way is the model based bootstrap. As with the parametric bootstrap, we assume to know the model, e.g. <span class="math inline">\(y = m(x) + \epsilon\)</span> (where <span class="math inline">\(m(\cdot)\)</span> might be a non-parametric curve estimator), but we do not make an assumption about the error distribution. Instead of generating <span class="math inline">\(\epsilon^*\)</span> from the known distribution with unknown parameters (<span class="math inline">\(\epsilon^* \sim P_{\hat{\theta}}\)</span>, as in the parametric bootstrap)), we draw them with replacement from the <em>empirical</em> distribution. To sum it up, these are the steps necessary:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{m(\cdot)}\)</span> from all data.</li>
<li>Simulate <span class="math inline">\({\epsilon_1}^*, ..., {\epsilon_n}^*\)</span> by drawing from <span class="math inline">\(\hat{P}\)</span> with replacement.</li>
<li>Obtain <span class="math inline">\(({Y_1}^*, ..., {Y_n}^*)\)</span> from <span class="math inline">\(\hat{m}(x) + \epsilon^*\)</span>. As for the parametric bootstrap, the final bootstrap sample we use is <span class="math inline">\((x_1, {Y_1}^*), ..., (x_n, {Y_n}^*))\)</span> where the <span class="math inline">\(x_i\)</span> are just the observed data.</li>
<li>Again, you can use the bootstrap samples as for the other two methods.</li>
</ol>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">6.9</span> Conclusion</h2>
<p>Which version of the bootstrap should I use? The answer is classical. If the parametric model fits the data very well, there is no need to estimate the distribution explicitly. Also, if there is very little data, it might be very difficult to estimate <span class="math inline">\(P\)</span>. On the other hand, the non-parametric bootstrap is less sensitive to model-misspecification and can deal with arbitrary distributions (? is that true?).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cross-validation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
