<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="classification.html">
<link rel="next" href="bagging-and-boosting.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3" data-path="nonparametric-density-estimation.html"><a href="nonparametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Density Estimation</a></li>
<li class="chapter" data-level="4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>4</b> Nonparametric Regression</a></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementation"><i class="fa fa-check"></i><b>5.3</b> Implementation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#examples"><i class="fa fa-check"></i><b>5.7</b> Examples</a><ul>
<li class="chapter" data-level="5.7.1" data-path="cross-validation.html"><a href="cross-validation.html#practical-cv-in-r"><i class="fa fa-check"></i><b>5.7.1</b> Practical CV in R</a></li>
<li class="chapter" data-level="5.7.2" data-path="cross-validation.html"><a href="cross-validation.html#parameter-tuning"><i class="fa fa-check"></i><b>5.7.2</b> Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.3</b> Example</a></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.4</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.4.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.5</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.5.1</b> Proejction Pursuit Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>9</b> Bagging and Boosting</a></li>
<li class="chapter" data-level="10" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>10</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="flexible-regression-and-classification-methods" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Flexible regression and classification methods</h1>
<p>The curse of dimensionality makes it very hard to estimate fully nonparametric regression function <span class="math inline">\(\hat{m} = \mathbb{E}[Y|X = x]\)</span> or classification function <span class="math inline">\(\hat{\pi}_j = \mathbb{P}[Y = j | X = x]\)</span>. Hence, by making some (reasonable) structural assumptions, we can improve our models significantly. Generally, we consider the mapping <span class="math inline">\(\mathbb{R}^p \rightarrow \mathbb{P}\)</span> via the function <span class="math inline">\(g(\cdot)\)</span> for both the regression and the classification problem.</p>

<div id="additive-models" class="section level2">
<h2><span class="header-section-number">8.1</span> Additive Models</h2>
<div id="structure" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Structure</h3>
<p>One assumption we can make is to assume a particular functional form of <span class="math inline">\(g(\cdot)\)</span>, namely an <em>additive</em>. That is <span class="math display">\[g_{add} = \mu + \sum\limits_{j = 1}^pg(x_j) \]</span> <span class="math inline">\(E[g(x_j)] = 0\)</span> is required to make the model identifiable only. Note that we have not placed any assumption on <span class="math inline">\(g(x_j)\)</span> yet, that is, <span class="math inline">\(g(x_j)\)</span> can be fully non-parametric, but each dimension is mapped separately. In other words every <span class="math inline">\(g(x_j) \;\; j = 1, ..., p\)</span> models one input dimension and mapping of input to output is obtained by summing the transformed inputs up. This eliminates the possibility of interaction effects.</p>
</div>
<div id="fitting-procedure" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Fitting Procedure</h3>
<p>Additive models can be estimated with a technique called back-fitting. However, the model can be estimated with any nonparametric method for one-dimensional smoothing. Here is the receipt:</p>
<ul>
<li>since we assume an additive model, we need to initialize all <span class="math inline">\(p\)</span> components of it with zero, that is setting <span class="math inline">\(g_j(\cdot) = 0 \;\; j = 1,..., p\)</span> plus setting <span class="math inline">\(\mu = n^{-1}\sum Y_i\)</span>. * Then we fit one-dimensional smoother repeatedly, that is solving the one-dimensional smoothing problem <span class="math inline">\(Y - \mu - \sum\limits_{j \neq k}\hat{g}_j = \hat{g}_j(x_j)\)</span>, or put differently <span class="math inline">\(\hat{g}_j = S_j(Y - \mu1 - \sum\limits_{j \neq k}g_j)\)</span>. This has to be done repeatedly for <span class="math inline">\(j = 1, ..., p, 1, ..., p \text{etc.}\)</span>.</li>
<li>Stop iterating when functions don’t change much anymore, that is, when the following quantity is less than a certain tolerance. <span class="math display">\[\frac{|\hat{g}_{i, new} - \hat{g}_{i, old}|}{|\hat{g}_{i, old}|}\]</span></li>
<li>Normalize the functions by subtracting the mean from them: <span class="math display">\[\tilde{g}_j = \hat{g}_j - n^{-1} \sum\limits_{i = 1}^n \hat{g}_j(x_{ij})\]</span></li>
</ul>
<p>Back-fitting is a <strong>coordinate-wise</strong> optimization method that optimizes one coordinate at the time (one <span class="math inline">\(g(\cdot)\)</span>, but can be more than one parameter), which may be slower in convergence than a general gradient descent that optimizes all directions simultaneously but also more robust.</p>
<p><img src="figures/coordinate_wise.png" width="244" /></p>
</div>
<div id="additive-models-in-r" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Additive Models in R</h3>
<p>You can use the package <strong>mgcv</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;ozone&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;gss&quot;</span>) 
fit &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(upo3 ~<span class="st"> </span><span class="kw">s</span>(vdht) +<span class="st"> </span><span class="kw">s</span>(wdsp) +<span class="st"> </span><span class="kw">s</span>(hmdt) +<span class="st"> </span><span class="kw">s</span>(sbtp), 
           <span class="dt">data =</span> ozone) 

<span class="kw">plot</span>(fit, <span class="dt">pages =</span> <span class="dv">1</span>) </code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-16-1.png" width="672" /> You can see that vdht enters the model almost linearly. That is, with an increase of one unit of vdht, the predicted 03 value increases linearly. sbtp is different. Depending on the value of sbtp, the increase in the predicted value is different. Low sbtp values hardly have an impact on the response, higher values do.</p>

</div>
</div>
<div id="mars" class="section level2">
<h2><span class="header-section-number">8.2</span> MARS</h2>
<p>MARS stands for multivariate adaptive regression splines and allows pice-wise linear curve fitting. In contrast to GAMs, they allow for interactions between the variables. MARS is very similar to regression trees, but it has a continuous response. It uses reflected pairs of basis functions</p>
<span class="math display">\[\begin{equation} 
  (x_j -d)_{+} = \begin{cases} x_j - d \;\;\ \text{if} \;\;\ x_j - d &gt; 0 \\ 
  0 \;\;\ \text{else}. 
  \end{cases} 
\end{equation}\]</span>
<p>and it counterpart <span class="math inline">\((d - x_j)_{+}\)</span>. The index <span class="math inline">\(j\)</span> refers to the j-th predictor variable, d is a knot at one of the <span class="math inline">\(x_js\)</span>. The pool of basis functions to consider is called <span class="math inline">\(\mathbf{B}\)</span> and contains all variables with all potential knots, that is <span class="math display">\[
\mathcal{B} = \{(x_j - d)_+ \;\;\; (d - x_j)_+ \;\;\;j = \{1, ..., p\} \;\; d =
\{x_{1j}, ..., x_{nj}\}\}\]</span> The model then is <span class="math display">\[ g(x)  = \mu + \sum\limits_{m =
1}^M\beta_m h_m(x) = \sum\limits_{m = 0}^M\beta_m h_m(x)\]</span> The model uses forward selection and backward deletion and optimizes some (generalized) cross-validation criterion. Here is the receipt:</p>
<ol style="list-style-type: decimal">
<li>initialize <span class="math inline">\(\mathcal{M} = \{h_0(\cdot) = 1\}\)</span> and estimate <span class="math inline">\(\beta_0\)</span> as the data average of the response <span class="math inline">\(n^{-1}\sum\limits_{i = 1}^n Y_i\)</span>. 2. for <span class="math inline">\(r = 1, 2, ...\)</span> search for a new pair of function <span class="math inline">\((h_{2 r-1} \;\; h_{2 r})\)</span> which are of the form <span class="math display">\[h_{2 r-1} = (x_j -d)_+ \times h_l\]</span> <span class="math display">\[h_{2 r} = (d - x_j)_+ \times
h_l\]</span> that reduce the residual sum of squares the most with some <span class="math inline">\(h_l\)</span> from <span class="math inline">\(\mathcal{M}\)</span> that some basis functions from <span class="math inline">\(\mathcal{B}\)</span> does <em>not</em> contain<br />
<span class="math inline">\(x_j\)</span>. The model becomes <span class="math display">\[\hat{g}(x) = \sum\limits_{m = 0}^{2r}\hat{\beta}_m
h_m(x)\]</span> which can be estimated by least squares. Update the set <span class="math inline">\(\mathcal{M}\)</span> to be <span class="math inline">\(\mathcal{M} = \mathcal{M}_{old} \cup \{h_{2r-1}, h_{2r}\}\)</span> 3. Iterate the above step until the set <span class="math inline">\(\mathcal{M}\)</span> becomes <em>large enough</em>. 4. Do backward deletion (<em>pruning</em>) by removing the <em>one</em> function from a reflected pair that increases the residual sum of squares the least. 5. Stop the pruning when some GCV score reaches its minimum.</li>
</ol>
<div id="details-for-dummies" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Details for Dummies</h3>
<p>Note that by using reflective pairs <span class="math inline">\(\big\{(x_j - d)_+ \;\;\; (d - x_j)_{+}\; \}\)</span>, we construct a piece-wise linear function with one knot at <span class="math inline">\(d\)</span>, since we sum the functions (which both a have a zero part that does not overlap) up. This hinge function (or better the two parts of individually) <span class="math inline">\(\beta\)</span> will be multiplied with a respective <span class="math inline">\(\beta\)</span>, so the slope is adaptive. Also, since each of the functions have its own <span class="math inline">\(d\)</span>, the kink of the function must not occur at <span class="math inline">\(y = 0\)</span>.</p>
<p>From the receipt above, we can see that <span class="math inline">\(d\)</span>-way interactions are only possible if a <span class="math inline">\(d-1\)</span>-way interaction with a subset of the <span class="math inline">\(d\)</span>-way interaction is already in the model. For interpretability and other reasons, restricting the number of interactions to three or two is beneficial. Restricting the degree of interaction to one (which is actually no interaction) will yield an additive model.</p>
</div>
</div>
<div id="example" class="section level2">
<h2><span class="header-section-number">8.3</span> Example</h2>
<p>Let us consider the simple case of a one dimensional predictor space. We add noise to data that is piece-wise linear up to <span class="math inline">\(x= 100\)</span> and then follows a sine-wave. We then fit three mars models with different number of maximal knots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;earth&quot;</span>) 
sim &lt;-<span class="st"> </span><span class="kw">data_frame</span>( <span class="dt">x =</span> -<span class="dv">25</span>:<span class="dv">75</span>, 
                   <span class="dt">y =</span> <span class="kw">pmax</span>(<span class="dv">0</span>, x -<span class="st"> </span><span class="dv">40</span>) +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">3</span>)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="kw">data_frame</span>( <span class="dt">x =</span> <span class="dv">75</span>:<span class="dv">100</span>, <span class="dt">y =</span> -<span class="dv">1</span>*<span class="kw">sqrt</span>(x)*<span class="st"> </span><span class="kw">sin</span>(x/<span class="dv">3</span>) +<span class="st"> </span><span class="dv">30</span>))

fitted &lt;-<span class="st"> </span><span class="kw">data_frame</span>( 
  <span class="dt">nk3  =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">3</span>), 
  <span class="dt">nk5  =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">5</span>), 
  <span class="dt">nk100 =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">100</span>)
)


sim2 &lt;-<span class="st"> </span>fitted %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">map_df</span>(~<span class="kw">predict</span>(.x)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">bind_cols</span>(sim) %&gt;%<span class="st"> </span><span class="kw">gather</span>(key, value, -x, -y)


<span class="kw">ggplot</span>(sim2) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) </code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fitted$nk3) </code></pre></div>
<pre><code>## Call: earth(formula=y~x, data=sim, nk=3)
## 
##             coefficients
## (Intercept)    1.1007390
## h(x-34)        0.5496082
## 
## Selected 2 of 3 terms, and 1 of 1 predictors
## Termination condition: Reached nk 3
## Importance: x
## Number of terms at each degree of interaction: 1 1 (additive model)
## GCV 29.6829    RSS 3593.734    GRSq 0.8285947    RSq 0.8339929</code></pre>
<p>The example shows what we expected. The green line with a maximum of three knots uses just one knot around 30, and only the right part of the reflected pair is used. It cannot distinguish between the linear segment between 40 and 75 and the sine-wave afterwards. By allowing more knots, we can see that the red line fits the data quite well. Note that the default of <code>degree</code> is just <span class="math inline">\(1\)</span>, so we don’t have interaction terms in the model. This does not matter for a one-dimensional example anyways.</p>

</div>
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">8.4</span> Neural Networks</h2>
<p>Neural networks are high-dimensional non-linear regression models. The way it works is best illustrated with a picture.</p>
<p><img src="figures/nn.png" width="605" /></p>
<p>This is a neural network with one hidden layer, <span class="math inline">\(p\)</span> input layers and <span class="math inline">\(q\)</span> output layers. Mathematically speaking, the model is: <span class="math display">\[ g_k(x) = f_0\Big(\alpha_k +
\sum\limits_{h = 1}^q w_{ij} \phi(\tilde{\alpha}_h + \sum\limits_{j = 1}^p
w_{jh}x_j)\Big)\]</span> Where <span class="math inline">\(\phi(x)\)</span> is the sigmoid function <span class="math inline">\(\frac{exp(x)}{1 + exp(x)}\)</span>, <span class="math inline">\(f_0\)</span> is the sigmoid function for classification and the identity for regression. In the case of regression <span class="math inline">\(k = 1\)</span> is used, for classification we use <span class="math inline">\(g_0, ..., g_{J-1}\)</span> and then use the Bayes classifier <span class="math inline">\(\mathcal{\hat{C}(x)} = \arg\max\limits_{0&lt;j&lt;J-1} g_j(x)\)</span> (is that correct?), which is called the softmax in the neural network literature.</p>
<div id="fitting-neural-networks-in-r" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Fitting Neural Networks (in R)</h3>
<p>The <code>nnet</code> function from the package with the same name basically uses gradient descent to maximize the likelihood. It is important to first scale the data so the gradient descent does not get stuck in flat regions of the sigmoid function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22</span>) 
<span class="kw">data</span>(<span class="st">&quot;ozone&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;gss&quot;</span>) 
<span class="kw">unloadNamespace</span>(<span class="st">&quot;MASS&quot;</span>)
scaled &lt;-<span class="st"> </span>ozone %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(-upo3) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">scale</span>() %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">as_data_frame</span>() %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_upo3 =</span> <span class="kw">log</span>(ozone$upo3))

fit &lt;-<span class="st"> </span>nnet::<span class="kw">nnet</span>( log_upo3 ~., <span class="dt">data =</span> scaled, 
                   <span class="dt">size =</span> <span class="dv">3</span>, <span class="co"># how many nodes in the *one* hidden layer. </span>
                   <span class="dt">decay =</span> <span class="fl">4e-4</span>, <span class="co"># regularization. Multiply weights by 1 - decay after </span>
                   <span class="co"># gradient step. </span>
                   <span class="dt">linout =</span> <span class="ot">TRUE</span>, <span class="co"># linear output units (refers to f0?). </span>
                   <span class="dt">skip =</span> <span class="ot">TRUE</span>, <span class="co"># add skip-layer connections between output and input. </span>
                   <span class="dt">maxit =</span> <span class="dv">500</span> ) </code></pre></div>
<p>The weights between the nodes are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit) </code></pre></div>
<pre><code>## a 9-3-1 network with 43 weights
## options were - skip-layer connections  linear output units  decay=4e-04
##  b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 
##  -2.28   1.27  -0.34  -2.57   1.46   0.03   0.10  -1.02  -0.39  -0.33 
##  b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 
## -12.43   5.09   2.04   8.19  -7.66  -7.01   2.40  -0.31   3.59  -1.19 
##  b-&gt;h3 i1-&gt;h3 i2-&gt;h3 i3-&gt;h3 i4-&gt;h3 i5-&gt;h3 i6-&gt;h3 i7-&gt;h3 i8-&gt;h3 i9-&gt;h3 
## -19.77  -6.64   1.49  -4.53  -3.95   2.28   6.05   5.19  10.05  -0.20 
##   b-&gt;o  h1-&gt;o  h2-&gt;o  h3-&gt;o  i1-&gt;o  i2-&gt;o  i3-&gt;o  i4-&gt;o  i5-&gt;o  i6-&gt;o 
##   2.50  -1.81   0.68   0.71   0.11  -0.09  -0.49   0.72   0.01  -0.03 
##  i7-&gt;o  i8-&gt;o  i9-&gt;o 
##   0.04  -0.29  -0.15</code></pre>
<p>The in-sample MSE for the regression case is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">residuals</span>(fit)^<span class="dv">2</span>) </code></pre></div>
<pre><code>## [1] 0.1036706</code></pre>

</div>
</div>
<div id="projection-pursuit-regression" class="section level2">
<h2><span class="header-section-number">8.5</span> Projection Pursuit Regression</h2>
<p>The model takes the form <span class="math display">\[g_{PPR} = \mu + \sum\limits_{k = 1}^q f_k(\sum \limits_{j = 1}^p
\alpha_{jk}x_j) \]</span> With <span class="math inline">\(\sum \limits_{j = 1}^p \alpha_j = 1\)</span> and <span class="math inline">\(E[f_k(\sum \limits_{j = 1}^p \alpha_{jk}x_j)] = 0 \;\; \text{for all k}\)</span>.</p>
<p><span class="math inline">\(\mathbf{\alpha}_k x_j\)</span> is the projection of the j-th column in the design matrix onto <span class="math inline">\(\alpha_k\)</span>. The functions <span class="math inline">\(f_k\)</span> only vary along one direction and are hence called ridge functions.</p>
<p>Projection pursuit regression is similar to both neural nets and additive models. It is similar to GAMs because</p>
<ul>
<li>it can be seen as an additive model whereas the predictors were first projected into the optimal direction.</li>
</ul>
<p>And it is similar neural nets because</p>
<ul>
<li>if you assume the identity for <span class="math inline">\(f_0\)</span> in the neral net (which you typically do for regression) the models become very similar up to the term <span class="math inline">\(w_{hk} \phi\)</span>, which is just <span class="math inline">\(f_k\)</span> in projection pursuit regression. Hence, instead of assuming a parapetric form (the sigmoid function) and multiplying that transformation with a weight <span class="math inline">\(w_{hk}\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> we don’t make any assumption on the form of the function, that is, let <span class="math inline">\(f_k\)</span> be fully non-parametric.</li>
</ul>
<p>Probably fot that very reason, the model requires much smaller <span class="math inline">\(q\)</span> than a neural net requires hidden units, at the expense of estimating the ridge functions (which is not necessary for neural nets).</p>
<div id="proejction-pursuit-example" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Proejction Pursuit Example</h3>
<p>In the following, we illustrate how optimal projections of the initial predictor space can allow us to use an additive functional form to deal with interaction terms. Let us consider the following data-generating model <span class="math display">\[ Y = X_1 \times X_2 + \epsilon \; \text{with} \;\epsilon \sim N(0, 1) \; \text{and}\; X_1, X_2 \sim \text{Unif}(-1,1)\]</span></p>
<p>Where <span class="math inline">\(X \in \mathbb{R}^2\)</span>, i.e. a two-dimensional predictor space with the predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Using elementary calculus, this can be rewritten as <span class="math display">\[ Y = \frac{1}{4} (X_1 + X_2)^2  - \frac{1}{4}(X_1 - X_2)^2\]</span> Hence, we rewrote a multiplicative model as an additive model. As we are using arbitrary <em>smooth</em> functions <span class="math inline">\(f_k\)</span>, we can easily fit the quadratic terms in the equation above, so the problem to solve becomes <span class="math display">\[Y = \mu + f_1(X_1 + X_2) - f_2(X_1 - X_2)\]</span> Therefore, the remaining question is how can we choose the two vectors <span class="math inline">\(\mathbf{\alpha}_1\)</span> and <span class="math inline">\(\mathbf{\alpha}_1\)</span> such that the result of the projection is <span class="math inline">\(X_1 + X_2 \;\text{and}\;X_1 - X_2\)</span>. With the restriction <span class="math inline">\(|\alpha| = 1\)</span>, it turns out we can preceded as follows: We project the first predictor onto <span class="math inline">\((\alpha_{11}, \alpha_{12}) = (0.7, 0.7)\)</span> and the second predictor onto <span class="math inline">\((\alpha_{11}, \alpha_{12}) = (0.7, -0.7)\)</span>. This yields <span class="math inline">\(0.7(X_1 + X_2)\)</span> and <span class="math inline">\(0.7(X_1 - X_2)\)</span>.</p>
<p>Let’s implement that with R</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(
  <span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">500</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
  <span class="dt">x2 =</span> <span class="kw">runif</span>(<span class="dv">500</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
  <span class="dt">y =</span> x1*x2 +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">500</span>, <span class="dv">0</span>, <span class="fl">0.005</span>)
)
all &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()</code></pre></div>
<p>We can see the obvious pattern, but we can also see that an additive model would not do well on that.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1y &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()


<span class="kw">grid.arrange</span>(all, x1y)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>How about using the aforementioned projection?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">projected_x1 =</span> <span class="fl">0.7</span>*(x1 +<span class="st"> </span>x2),
    <span class="dt">projected_x2 =</span> -<span class="fl">0.7</span>*(x1 -<span class="st"> </span>x2)
  )

projected_all &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x1, <span class="dt">y =</span> projected_x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

projected_x1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x1, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

projected_x2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x2, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

fitted_x1 &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(y~<span class="kw">s</span>(projected_x1), <span class="dt">data =</span> data)
fitted_x2 &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(y~<span class="kw">s</span>(projected_x2), <span class="dt">data =</span> data)

data &lt;-<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">predict</span>(fitted_x1) +<span class="st"> </span><span class="kw">predict</span>(fitted_x2))

fitted &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> fitted), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()
<span class="kw">grid.arrange</span>(projected_all, projected_x1, projected_x2, fitted, <span class="dt">nrow =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>The bottom right picture shows the predictions with the projection pursuit approach, which resembles the original data pretty well. Again, the idea is to use an additive model to account for the interactions properly by first projecting the predictors optimally.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>For regression you need only one index since you will have just one ouput layer.<a href="flexible-regression-and-classification-methods.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bagging-and-boosting.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
