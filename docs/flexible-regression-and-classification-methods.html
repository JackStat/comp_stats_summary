<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="classification.html">
<link rel="next" href="variable-selection-ridge-regression-an-lasso.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
<li class="chapter" data-level="2.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>2.9</b> Model Selection</a><ul>
<li class="chapter" data-level="2.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mallows-cp-statistic"><i class="fa fa-check"></i><b>2.9.1</b> Mallow’s cp statistic</a></li>
<li class="chapter" data-level="2.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#search-strategies"><i class="fa fa-check"></i><b>2.9.2</b> Search strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression an Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-an-lasso.html"><a href="variable-selection-ridge-regression-an-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
<li class="chapter" data-level="11.3" data-path="round-up.html"><a href="round-up.html#cheatsheet"><i class="fa fa-check"></i><b>11.3</b> Cheatsheet</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="flexible-regression-and-classification-methods" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Flexible regression and classification methods</h1>
<p>The curse of dimensionality makes it very hard to estimate fully non-parametric regression function <span class="math inline">\(\hat{m} = \mathbb{E}[Y|X = x]\)</span> or classification function <span class="math inline">\(\hat{\pi}_j = \mathbb{P}[Y = j | X = x]\)</span>. Hence, by making some (reasonable) structural assumptions, we can improve our models significantly. Generally, we consider the mapping <span class="math inline">\(\mathbb{R}^p \rightarrow \mathbb{P}\)</span> via the function <span class="math inline">\(g(\cdot)\)</span> for both the regression and the classification problem.</p>

<div id="additive-models" class="section level2">
<h2><span class="header-section-number">8.1</span> Additive Models</h2>
<div id="structure" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Structure</h3>
<p>One assumption we can make is to assume a particular functional form of <span class="math inline">\(g(\cdot)\)</span>, namely an <em>additive</em>. That is <span class="math display">\[g_{add}(x) = \mu + \sum\limits_{j = 1}^pg(x_j) \]</span> <span class="math inline">\(E[g(x_j)] = 0\)</span> is required to make the model identifiable only. Note that we have not placed any assumption on <span class="math inline">\(g(x_j)\)</span> yet, that is, <span class="math inline">\(g(x_j)\)</span> can be fully non-parametric, but each dimension is mapped separately. In other words every <span class="math inline">\(g(x_j) \;\; j = 1, ..., p\)</span> models one input dimension and mapping of input to output is obtained by summing the transformed inputs up. This eliminates the possibility of <strong>interaction effects</strong>.</p>
</div>
<div id="fitting-procedure" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Fitting Procedure</h3>
<p>Additive models can be estimated with a technique called back-fitting. However, the model can be estimated with any non-parametric method for one-dimensional smoothing. Here is the receipt:</p>
<ul>
<li>since we assume an additive model, we need to initialize all <span class="math inline">\(p\)</span> components of it with zero, that is setting <span class="math inline">\(g_j(\cdot) = 0 \;\; j = 1,..., p\)</span> plus setting <span class="math inline">\(\mu = n^{-1}\sum Y_i\)</span>. * Then we fit one-dimensional smoother repeatedly, that is solving the one-dimensional smoothing problem <span class="math inline">\(Y - \mu - \sum\limits_{j \neq k}\hat{g}_j = \hat{g}_j(x_j)\)</span>, or put differently <span class="math inline">\(\hat{g}_j = S_j(Y - \mu1 - \sum\limits_{j \neq k}g_j)\)</span>. This has to be done repeatedly for <span class="math inline">\(j = 1, ..., p, 1, ..., p\)</span> etc.</li>
<li>Stop iterating when functions don’t change much anymore, that is, when the following quantity is less than a certain tolerance. <span class="math display">\[\frac{|\hat{g}_{i, new} - \hat{g}_{i, old}|}{|\hat{g}_{i, old}|}\]</span></li>
<li>Normalize the functions by subtracting the mean from them: <span class="math display">\[\tilde{g}_j = \hat{g}_j - n^{-1} \sum\limits_{i = 1}^n \hat{g}_j(x_{ij})\]</span></li>
</ul>
<p>Back-fitting is a <strong>coordinate-wise</strong> optimization method that optimizes one coordinate at the time (one <span class="math inline">\(g(\cdot)\)</span>, but can be more than one parameter), which may be slower in convergence than a general gradient descent that optimizes all directions simultaneously but also more robust.</p>
<p><img src="figures/coordinate_wise.png" width="244" /></p>
</div>
<div id="additive-models-in-r" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Additive Models in R</h3>
<p>You can use the package <strong>mgcv</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;ozone&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;gss&quot;</span>) 
fit &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(upo3 ~<span class="st"> </span><span class="kw">s</span>(vdht) +<span class="st"> </span><span class="kw">s</span>(wdsp) +<span class="st"> </span><span class="kw">s</span>(hmdt) +<span class="st"> </span><span class="kw">s</span>(sbtp), 
           <span class="dt">data =</span> ozone) 

<span class="kw">plot</span>(fit, <span class="dt">pages =</span> <span class="dv">1</span>) </code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>You can see that vdht enters the model almost linearly. That is, with an increase of one unit of vdht, the predicted 03 value increases linearly. sbtp is different. Depending on the value of sbtp, the increase in the predicted value is different. Low sbtp values hardly have an impact on the response, higher values do.</p>

</div>
</div>
<div id="mars" class="section level2">
<h2><span class="header-section-number">8.2</span> MARS</h2>
<p>MARS stands for multivariate adaptive regression splines and allows pice-wise linear curve fitting. In contrast to GAMs, they allow for interactions between the variables. MARS is very similar to regression trees, but it has a continuous response. It uses reflected pairs of basis functions</p>
<span class="math display">\[\begin{equation} 
  (x_j -d)_{+} = \begin{cases} x_j - d \;\;\ \text{if} \;\;\ x_j - d &gt; 0 \\ 
  0 \;\;\ \text{else}. 
  \end{cases} 
\end{equation}\]</span>
<p>and it counterpart <span class="math inline">\((d - x_j)_{+}\)</span>. The index <span class="math inline">\(j\)</span> refers to the j-th predictor variable, d is a knot at one of the <span class="math inline">\(x_js\)</span>. The pool of basis functions to consider is called <span class="math inline">\(\mathcal{B}\)</span> and contains all variables with all potential knots, that is <span class="math display">\[
\mathcal{B} = \{(x_j - d)_+ \;\;\; (d - x_j)_+ \;\;\;j = \{1, ..., p\} \;\; d =
\{x_{1j}, ..., x_{nj}\}\}\]</span> The model then is <span class="math display">\[ g(x)  = \mu + \sum\limits_{m =
1}^M\beta_m h_m(x) = \sum\limits_{m = 0}^M\beta_m h_m(x)\]</span> The model uses forward selection and backward deletion and optimizes some (generalized) cross-validation criterion. Here is the receipt:</p>
<ol style="list-style-type: decimal">
<li>initialize <span class="math inline">\(\mathcal{M} = \{h_0(\cdot) = 1\}\)</span> and estimate <span class="math inline">\(\beta_0\)</span> as the data average of the response <span class="math inline">\(n^{-1}\sum\limits_{i = 1}^n Y_i\)</span>.</li>
<li>for <span class="math inline">\(r = 1, 2, ...\)</span> search for a new pair of function <span class="math inline">\((h_{2 r-1} \;\;  h_{2 r})\)</span> which are of the form <span class="math display">\[h_{2 r-1} = (x_j -d)_+ \times h_l\]</span> <span class="math display">\[h_{2 r} = (d -
   x_j)_+ \times h_l\]</span> that reduce the residual sum of squares the most with some <span class="math inline">\(h_l\)</span> from <span class="math inline">\(\mathcal{M}\)</span> that some basis functions from <span class="math inline">\(\mathcal{B}\)</span> does <em>not</em> contain <span class="math inline">\(x_j\)</span>. The model becomes <span class="math display">\[\hat{g}(x) = \sum\limits_{m = 0}^{2r}\hat{\beta}_m
   h_m(x)\]</span> which can be estimated by least squares. Update the set <span class="math inline">\(\mathcal{M}\)</span> to be <span class="math inline">\(\mathcal{M} = \mathcal{M}_{old} \cup \{h_{2r-1}, h_{2r}\}\)</span></li>
<li>Iterate the above step until the set <span class="math inline">\(\mathcal{M}\)</span> becomes <em>large enough</em>.</li>
<li>Do backward deletion (<em>pruning</em>) by removing the <em>one</em> function from a reflected pair that increases the residual sum of squares the least.</li>
<li>Stop the pruning when some GCV score reaches its minimum.</li>
</ol>
<p>From the receipt above, we can see that <span class="math inline">\(d\)</span>-way interactions are only possible if a <span class="math inline">\(d-1\)</span>-way interaction with a subset of the <span class="math inline">\(d\)</span>-way interaction is already in the model. For interpretability and other reasons, restricting the number of interactions to three or two is beneficial. Restricting the degree of interaction to one (which is actually no interaction) will yield an additive model.</p>
<div id="details-for-dummies" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Details for Dummies</h3>
<p>Note that by using reflective pairs <span class="math inline">\(\big\{(x_j - d)_+ \;\;\; (d - x_j)_{+}\; \}\)</span>, we construct a piece-wise linear function with one knot at <span class="math inline">\(d\)</span>, since we sum the functions (which both a have a zero part that does not overlap) up. This hinge function (or better the two parts of individually) will be multiplied with a respective <span class="math inline">\(\beta\)</span>, so the slope is adaptive. Also, since each of the functions have its own <span class="math inline">\(d\)</span>, the kink of the function must not occur at <span class="math inline">\(y = 0\)</span> (this is wrong).</p>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Example</h3>
<p>Let us consider the simple case of a one dimensional predictor space. We add noise to data that is piece-wise linear up to <span class="math inline">\(x= 100\)</span> and then follows a sine-wave. We then fit three mars models with different number of maximal knots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;earth&quot;</span>) 
sim &lt;-<span class="st"> </span><span class="kw">data_frame</span>( <span class="dt">x =</span> -<span class="dv">25</span>:<span class="dv">75</span>, 
                   <span class="dt">y =</span> <span class="kw">pmax</span>(<span class="dv">0</span>, x -<span class="st"> </span><span class="dv">40</span>) +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">3</span>)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="kw">data_frame</span>( <span class="dt">x =</span> <span class="dv">75</span>:<span class="dv">100</span>, <span class="dt">y =</span> -<span class="dv">1</span>*<span class="kw">sqrt</span>(x)*<span class="st"> </span><span class="kw">sin</span>(x/<span class="dv">3</span>) +<span class="st"> </span><span class="dv">30</span>))

fitted &lt;-<span class="st"> </span><span class="kw">data_frame</span>( 
  <span class="dt">nk3  =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">3</span>), 
  <span class="dt">nk5  =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">5</span>), 
  <span class="dt">nk100 =</span> <span class="kw">earth</span>(y~x, <span class="dt">data =</span> sim, <span class="dt">nk =</span> <span class="dv">100</span>)
)


sim2 &lt;-<span class="st"> </span>fitted %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">map_df</span>(~<span class="kw">predict</span>(.x)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">bind_cols</span>(sim) %&gt;%<span class="st"> </span><span class="kw">gather</span>(key, value, -x, -y)


<span class="kw">ggplot</span>(sim2) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) </code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fitted$nk3) </code></pre></div>
<pre><code>## Call: earth(formula=y~x, data=sim, nk=3)
## 
##             coefficients
## (Intercept)   0.06140576
## h(x-30)       0.53378814
## 
## Selected 2 of 3 terms, and 1 of 1 predictors
## Termination condition: Reached nk 3
## Importance: x
## Number of terms at each degree of interaction: 1 1 (additive model)
## GCV 36.0235    RSS 4361.396    GRSq 0.8112337    RSq 0.8171787</code></pre>
<p>The example shows what we expected. The green line with a maximum of three knots uses just one knot around 30, and only the right part of the reflected pair is used. It cannot distinguish between the linear segment between 40 and 75 and the sine-wave afterwards. By allowing more knots, we can see that the red line fits the data quite well. Note that the default of <code>degree</code> is just <span class="math inline">\(1\)</span>, so we don’t have interaction terms in the model. This does not matter for a one-dimensional example anyways.</p>

</div>
</div>
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">8.3</span> Neural Networks</h2>
<p>Neural networks are high-dimensional non-linear regression models. The way it works is best illustrated with a picture.</p>
<p><img src="figures/nn.png" width="605" /></p>
<p>This is a neural network with one hidden layer, <span class="math inline">\(p\)</span> input layers and <span class="math inline">\(q\)</span> output layers. Mathematically speaking, the model is: <span class="math display">\[ g_k(x) = f_0\Big(\alpha_k +
\sum\limits_{h = 1}^q w_{hk} \phi(\tilde{\alpha}_h + \sum\limits_{j = 1}^p
w_{jh}x_j)\Big)\]</span> Where <span class="math inline">\(\phi(x)\)</span> is the sigmoid function <span class="math inline">\(\frac{exp(x)}{1 + exp(x)}\)</span>, <span class="math inline">\(f_0\)</span> is the sigmoid function for classification and the identity for regression. In the case of regression <span class="math inline">\(k = 1\)</span> is used, for classification we use <span class="math inline">\(g_0, ..., g_{J-1}\)</span> and then use the Bayes classifier <span class="math inline">\(\mathcal{\hat{C}(x)} = \arg\max\limits_{0&lt;j&lt;J-1} g_j(x)\)</span> (is that correct?), which is called the softmax in the neural network literature.</p>
<div id="fitting-neural-networks-in-r" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Fitting Neural Networks (in R)</h3>
<p>The <code>nnet</code> function from the package with the same name basically uses gradient descent to maximize the likelihood. It is important to first scale the data so the gradient descent does not get stuck in flat regions of the sigmoid function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22</span>) 
<span class="kw">data</span>(<span class="st">&quot;ozone&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;gss&quot;</span>) 
<span class="kw">unloadNamespace</span>(<span class="st">&quot;MASS&quot;</span>)
scaled &lt;-<span class="st"> </span>ozone %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(-upo3) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">scale</span>() %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">as_data_frame</span>() %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">log_upo3 =</span> <span class="kw">log</span>(ozone$upo3))

fit &lt;-<span class="st"> </span>nnet::<span class="kw">nnet</span>( log_upo3 ~., <span class="dt">data =</span> scaled, 
                   <span class="dt">size =</span> <span class="dv">3</span>, <span class="co"># how many nodes in the *one* hidden layer. </span>
                   <span class="dt">decay =</span> <span class="fl">4e-4</span>, <span class="co"># regularization. Multiply weights by 1 - decay after </span>
                   <span class="co"># gradient step. </span>
                   <span class="dt">linout =</span> <span class="ot">TRUE</span>, <span class="co"># linear output units (refers to f0?). </span>
                   <span class="dt">skip =</span> <span class="ot">TRUE</span>, <span class="co"># add skip-layer connections between output and input. </span>
                   <span class="dt">maxit =</span> <span class="dv">500</span> ) </code></pre></div>
<p>The weights between the nodes are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit) </code></pre></div>
<pre><code>## a 9-3-1 network with 43 weights
## options were - skip-layer connections  linear output units  decay=4e-04
##  b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 
##  -2.28   1.27  -0.34  -2.57   1.46   0.03   0.10  -1.02  -0.39  -0.33 
##  b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 
## -12.43   5.09   2.04   8.19  -7.66  -7.01   2.40  -0.31   3.59  -1.19 
##  b-&gt;h3 i1-&gt;h3 i2-&gt;h3 i3-&gt;h3 i4-&gt;h3 i5-&gt;h3 i6-&gt;h3 i7-&gt;h3 i8-&gt;h3 i9-&gt;h3 
## -19.77  -6.64   1.49  -4.53  -3.95   2.28   6.05   5.19  10.05  -0.20 
##   b-&gt;o  h1-&gt;o  h2-&gt;o  h3-&gt;o  i1-&gt;o  i2-&gt;o  i3-&gt;o  i4-&gt;o  i5-&gt;o  i6-&gt;o 
##   2.50  -1.81   0.68   0.71   0.11  -0.09  -0.49   0.72   0.01  -0.03 
##  i7-&gt;o  i8-&gt;o  i9-&gt;o 
##   0.04  -0.29  -0.15</code></pre>
<p>The in-sample MSE for the regression case is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">residuals</span>(fit)^<span class="dv">2</span>) </code></pre></div>
<pre><code>## [1] 0.1036706</code></pre>

</div>
</div>
<div id="projection-pursuit-regression" class="section level2">
<h2><span class="header-section-number">8.4</span> Projection Pursuit Regression</h2>
<p>The model takes the form <span class="math display">\[g_{PPR} = \mu + \sum\limits_{k = 1}^q f_k(\sum \limits_{j = 1}^p
\alpha_{jk}x_j) \]</span> With <span class="math inline">\(\sum \limits_{j = 1}^p \alpha_j = 1\)</span> and <span class="math inline">\(E[f_k(\sum \limits_{j = 1}^p \alpha_{jk}x_j)] = 0 \;\; \text{for all k}\)</span>.</p>
<p><span class="math inline">\(\mathbf{\alpha}_k x_j\)</span> is the projection of the j-th column in the design matrix onto <span class="math inline">\(\alpha_k\)</span>. The functions <span class="math inline">\(f_k\)</span> only vary along one direction and are hence called ridge functions.</p>
<p>Projection pursuit regression is similar to both neural nets and additive models. It is similar to GAMs because</p>
<ul>
<li>it can be seen as an additive model whereas the predictors were first projected into the optimal direction.</li>
</ul>
<p>And it is similar neural nets because</p>
<ul>
<li>if you assume the identity for <span class="math inline">\(f_0\)</span> in the neral net (which you typically do for regression) the models become very similar up to the term <span class="math inline">\(w_{hk} \phi\)</span>, which is just <span class="math inline">\(f_k\)</span> in projection pursuit regression. Hence, instead of assuming a parapetric form (the sigmoid function) and multiplying that transformation with a weight <span class="math inline">\(w_{hk}\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> we don’t make any assumption on the form of the function, that is, let <span class="math inline">\(f_k\)</span> be fully non-parametric.</li>
</ul>
<p>Probably for that very reason, the model requires much smaller <span class="math inline">\(q\)</span> than a neural net requires hidden units, at the expense of estimating the ridge functions (which is not necessary for neural nets).</p>
<div id="proejction-pursuit-example" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Proejction Pursuit Example</h3>
<p>In the following, we illustrate how optimal projections of the initial predictor space can allow us to use an additive functional form to deal with interaction terms. Let us consider the following data-generating model <span class="math display">\[ Y = X_1 \times X_2 + \epsilon \; \text{with} \;\epsilon \sim N(0, 1) \; \text{and}\; X_1, X_2 \sim \text{Unif}(-1,1)\]</span></p>
<p>Where <span class="math inline">\(X \in \mathbb{R}^2\)</span>, i.e. a two-dimensional predictor space with the predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Using elementary calculus, this can be rewritten as <span class="math display">\[ Y = \frac{1}{4} (X_1 + X_2)^2  - \frac{1}{4}(X_1 - X_2)^2\]</span> Hence, we rewrote a multiplicative model as an additive model. As we are using arbitrary <em>smooth</em> functions <span class="math inline">\(f_k\)</span>, we can easily fit the quadratic terms in the equation above, so the problem to solve becomes <span class="math display">\[Y = \mu + f_1(X_1 + X_2) - f_2(X_1 - X_2)\]</span> Therefore, the remaining question is how can we choose the two vectors <span class="math inline">\(\mathbf{\alpha}_1\)</span> and <span class="math inline">\(\mathbf{\alpha}_2\)</span> such that the result of the projection is <span class="math inline">\(X_1 + X_2 \;\text{and}\;X_1 - X_2\)</span>. With the restriction <span class="math inline">\(|\alpha| = 1\)</span>, it turns out we can proceed as follows: We first project predictor onto <span class="math inline">\((\alpha_{11}, \alpha_{12}) = (0.7, 0.7)\)</span> and then onto <span class="math inline">\((\alpha_{11}, \alpha_{12}) = (0.7, -0.7)\)</span>. This yields <span class="math inline">\(0.7(X_1 + X_2)\)</span> and <span class="math inline">\(0.7(X_1 - X_2)\)</span>.</p>
<p>Let’s implement that in R</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(
  <span class="dt">x1 =</span> <span class="kw">runif</span>(<span class="dv">500</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
  <span class="dt">x2 =</span> <span class="kw">runif</span>(<span class="dv">500</span>, -<span class="dv">1</span>, <span class="dv">1</span>),
  <span class="dt">y =</span> x1*x2 +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">500</span>, <span class="dv">0</span>, <span class="fl">0.005</span>)
)
all &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1y &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()


<span class="kw">grid.arrange</span>(all, x1y)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-31-1.png" width="672" /> We can see the obvious pattern, but we can also see that an additive model would not do well on that.</p>
<p>How about using the aforementioned projection?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">projected_x1 =</span> <span class="fl">0.7</span>*(x1 +<span class="st"> </span>x2),
    <span class="dt">projected_x2 =</span> -<span class="fl">0.7</span>*(x1 -<span class="st"> </span>x2)
  )

projected_all &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x1, <span class="dt">y =</span> projected_x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

projected_x1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x1, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

projected_x2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> projected_x2, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> y), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()

fitted_x1 &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(y~<span class="kw">s</span>(projected_x1), <span class="dt">data =</span> data)
fitted_x2 &lt;-<span class="st"> </span>mgcv::<span class="kw">gam</span>(y~<span class="kw">s</span>(projected_x2), <span class="dt">data =</span> data)

data &lt;-<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">fitted =</span> <span class="kw">predict</span>(fitted_x1) +<span class="st"> </span><span class="kw">predict</span>(fitted_x2))

fitted &lt;-<span class="st"> </span><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> fitted), <span class="dt">size =</span> <span class="dv">3</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_gradient2</span>()
<span class="kw">grid.arrange</span>(projected_all, projected_x1, projected_x2, fitted, <span class="dt">nrow =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>The bottom right picture shows the predictions with the projection pursuit approach, which resembles the original data pretty well. Again, the idea is to use an additive model to account for the interactions properly by first projecting the predictors optimally.</p>
<p>Suppose we did not know the optimal projection. We could use the build-in <code>ppr()</code> command to fit a projection pursuit regression and then show us the projections used.</p>
<p>First, we fit the model and check out the projections</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">ppr</span>(y~<span class="st"> </span>x1+x2, <span class="dt">nterms =</span> <span class="dv">2</span>, <span class="dt">data =</span> data)
sfsmisc::<span class="kw">mult.fig</span>(<span class="dv">2</span>)
<span class="kw">plot</span>(fit)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Now, let us look at the <span class="math inline">\(\alpha\)</span>s.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit$alpha</code></pre></div>
<pre><code>##        term 1     term 2
## x1  0.7795395 -0.7788790
## x2 -0.6263530 -0.6271742</code></pre>
<p>It’s pretty much the model we came up with before.</p>
<p>There is also an interpretation of the projection vectors. Since the above example is not particularly interesting, we will look at the output of exercise series 9, problem 1g.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_from_series9$alpha
##          term 1 term 2 term 3 term 4
## vdht       0.48  -0.09  -0.06  -0.02
## wind      -0.25  -0.11   0.09   0.12
## humidity  -0.03  -0.54   0.16   0.22
## temp       0.48   0.43  -0.04  -0.37
## ibht      -0.06  -0.17  -0.05   0.01
## dgpg       0.47  -0.45  -0.11  -0.66
## ibtp       0.07  -0.10  -0.10   0.32
## vsty      -0.44  -0.10   0.05   0.26
## day       -0.23  -0.49  -0.97   0.44</code></pre></div>
<p>If you take the mean over the absolute valeus in the rows, you can see that the variable <code>ibht</code> has relatively low average weights over the four terms. That means the variable does not get a high weight in general. You can also look at colums individually to find out which varialbes were important for a certain term, almost like in principle component analysis. You can see that term 3 is dominated by <code>day</code>.</p>

</div>
</div>
<div id="classification-and-regression-trees" class="section level2">
<h2><span class="header-section-number">8.5</span> Classification and Regression Trees</h2>
<p>The model function for trees is <span class="math display">\[g_{tree}(x) = \sum\limits_{r = 1}^M \beta_r1_{[x \in \mathcal{P}_r]}\]</span> Where <span class="math inline">\(\mathcal{P} = \cup_{j = 1}^M \mathcal{P}_j\)</span>, that is, the space <span class="math inline">\(\mathcal{P} \in \mathbb{R}^p\)</span> is devided into <span class="math inline">\(M\)</span> disjoint partitions. Hence, note that in the sum above, <span class="math inline">\(x\)</span> can only be in one of the <span class="math inline">\(M\)</span> martitions and hence, all but one indicator functions are zero in the sum. The model yields a <strong>pice-wise constant</strong> response, that is, the prediction is the same for all <span class="math inline">\(x \in \mathcal{P}_r\)</span>. That can be visualized nicely in a two-dimensional predictor space.</p>
<div class="figure"><span id="fig:treepart"></span>
<img src="figures/tree-partitioning.png" alt="Partition with `rpart()`. Color indicate result of the majority voting Source: course script p. 73." width="438" />
<p class="caption">
Figure 8.1: Partition with <code>rpart()</code>. Color indicate result of the majority voting Source: course script p. 73.
</p>
</div>
<p>Trees are similar to multivariate adaptive regression splines MARS, as mentioned in section <a href="flexible-regression-and-classification-methods.html#mars">8.2</a> in the sense that they allow for interaction effects. This can be seen well in figure <a href="flexible-regression-and-classification-methods.html#fig:treepart">8.1</a>. Going form age 50 to age 100 has different effects depending on the start. Trees are different from MARS as they are piece-wise constant, whereas MARS are pice-wise linear.</p>
<div id="prediction-given-partitioning" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Prediction given Partitioning</h3>
<p>Estimation of the parameters <span class="math inline">\(\beta_1, ..., \beta_M\)</span> is easy if the partitioning is known. For Regression, it is simply the average of the response variables for the subset of the data that lays within the partition. Mathematically speaking <span class="math display">\[ \hat{\beta}_r = \sum\limits_{i = 1}^n 1_{[x_i \in \mathcal{P}_r]} Y_i / 
\sum\limits_{i = 1}^n 1_{[x_i \in \mathcal{P}_r]}\]</span></p>
<p>For classification, the class of a partition _j is determined by the largest group within that partition). We can estimate the class probabilities directly (also for J &gt; 2) for the r-th partition as follows:</p>
<p><span class="math display">\[ \hat{\pi}_j(x) =
\frac{\# \text{from class j in}\; \mathcal{P}_r}{\#\text{total in}\; \mathcal{P}_r} = 
\sum\limits_{i = 1}^n 1_{[Y_i = j]} 
1_{[x_i \in \mathcal{P}_r]}/ 
\sum\limits_{i = 1}^n 1_{[x_i \in \mathcal{P}_r]}\]</span></p>
</div>
<div id="assumptions-on-the-patritions" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Assumptions on the Patritions</h3>
<p>As we saw above, obtaining predictions <em>given</em> the partitioning is not hard. The more difficult problem is to obtain the partitions. By imposing some restrictions on the shape of the partitions and the strategy to choose them, we can limit the complexity of the question at hand. Namely, we</p>
<ul>
<li>assume partitions that are <strong>axes parallel rectangles</strong>, just as depicted in the pictuer above. Note that this is a stronger limitation than just assuming linear (decision) boundaries since these boundaries also need to be parallel to the axis. For example, decision trees would not do well on a classification problem like this (unless there is a lot of data and we can have many splits:</li>
</ul>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<ul>
<li>we use a <strong>greedy</strong> algorithm since the space of possible partitioning schemes is still huge.</li>
</ul>
</div>
<div id="algorithm" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Algorithm</h3>
<p>The algorithm now looks as follows:</p>
<ol style="list-style-type: decimal">
<li>Start with <span class="math inline">\(M = 1\)</span> and <span class="math inline">\(\mathcal{P} = \{\mathcal{R}\} = \mathbb{R}^p\)</span>.</li>
<li>Redefine <span class="math inline">\(\mathcal{R}\)</span> as <span class="math inline">\(\mathcal{R_{left}} \cup \mathcal{R_{right}}\)</span> where</li>
</ol>
<p><span class="math inline">\(\mathcal{R}_{left} \;= \mathbb{R}\times\mathbb{R}\;...\; \times(-\infty, d]\times  \mathbb{R} ...\times\mathbb{R}\)</span></p>
<p><span class="math inline">\(\mathcal{R}_{right} = \mathbb{R}\times\mathbb{R}\;...\; \times(d, \infty)\times  \mathbb{R} ...\times\mathbb{R}\)</span></p>
<p>where <span class="math inline">\(d\)</span> is a value from the <em>finite</em> set of midpoints between the data points with regard to the dimension currently considered. We search over all dimensions <span class="math inline">\(j \in \{1, ..., p\}\)</span> and within each dimension over all potential split points <span class="math inline">\(d\)</span> such that the negative log-likelihood is decreased the most. The new partition is <span class="math inline">\(\mathcal{P} = \{R_{left}, R_{right}\}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>We again refine the current partition as in step 2 by splitting up <em>one</em> partition into two parts. Then, we update the partition <span class="math display">\[\mathcal{P} = \mathcal{P}_{old} \setminus \mathcal{P}_{to\;refine} \;\cup\{R_{left}, R_{right}\} \]</span></li>
<li>Iterate over the step 3 <span class="math inline">\(M\)</span> times.</li>
<li>Prune the tree by reverting some of the partitioning steps above until the optimal size of the tree is found (e.g via cross-validation).</li>
</ol>
<p>You can fit a tree in R with the <code>rpart</code> package, which stands for recursive partitioning.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree &lt;-<span class="st"> </span>rpart::<span class="kw">rpart</span>(
  upo3~., 
  <span class="dt">data =</span> ozone,
  <span class="dt">control =</span> <span class="kw">list</span>(
    <span class="dt">minsplit =</span> <span class="dv">20</span>,
    <span class="dt">cp =</span> <span class="fl">0.003</span>
  )
)</code></pre></div>
</div>
<div id="backward-deletion-pruning" class="section level3">
<h3><span class="header-section-number">8.5.4</span> Backward Deletion / Pruning</h3>
<p>After <span class="math inline">\(M\)</span> steps, there will be <span class="math inline">\(M + 1\)</span> partitions. This can also be visualized nicely in a tree structure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rpart::<span class="kw">prune</span>(tree, <span class="dt">cp =</span> <span class="fl">0.05</span>) %&gt;%<span class="st"> </span><span class="co"># prune tree for illustrative purposes</span>
rpart.plot::<span class="kw">prp</span>(<span class="dt">extra =</span> <span class="dv">1</span>,
    <span class="dt">box.col=</span><span class="kw">c</span>(<span class="st">&#39;pink&#39;</span>, <span class="st">&#39;palegreen3&#39;</span>, <span class="st">&#39;lightsteelblue 2&#39;</span>,<span class="st">&#39;lightgoldenrod 1&#39;</span>)[tree$frame$yval])</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-39-1.png" width="650px" /></p>
The idea is now to subsequently remove the leaves from the tree such that the negative log-likeihood increases the least. If we do that until no leaf is left, we end up with a sequence of trees.
<span class="math display" id="eq:treeset">\[\begin{equation}
\mathcal{T}_M \supset \mathcal{T}_{M-1} \;\;...\;\;\supset \mathcal{T}_{\emptyset}
\tag{8.1}
\end{equation}\]</span>
<p>Just as with Mallow’s <span class="math inline">\(C_p\)</span>, we can compute a score for every model that is increasing in the fit of the model but also has a complexity penality</p>
<p><span class="math display">\[R_{\alpha}(\mathcal{T}) = R(\mathcal{T}) + \alpha \times \text{size}(\mathcal{T})\]</span></p>
<p>Now, we only need to find the right alpha. We can set a few alpha values, then find the best tree for this alpha <span class="math inline">\(\mathcal{T}(\alpha) = \arg\min\limits_{\mathcal{T} \subset \mathcal{T}_M}R_{\alpha}(\mathcal{T})\)</span> and then do cross-validation for these alpha values to find the optimal alpha. It can be shown that the set <span class="math inline">\(\{\mathcal{T}(\alpha)| \alpha \in (0, \infty]\}\)</span> is <em>nested</em> and the same or a subeset of the set in equation <a href="flexible-regression-and-classification-methods.html#eq:treeset">(8.1)</a>. Use <code>rpart::plotcp()</code> to plot the size of the optimal trees for each alpha against the cross-validation score. Then, use the one-standard error rule to select the idal tree size. That is first find the tree with the lowest relative error. Then add one standard error to it’s error and find the smallest tree that does not execced this relative error. The idea behind this approach is to choose good model that performs similar to the best (and potentially complex) model but is as simple as possible.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rpart::<span class="kw">plotcp</span>(tree)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-40-1.png" width="650px" /></p>
</div>
<div id="pros-and-cons-of-trees" class="section level3">
<h3><span class="header-section-number">8.5.5</span> Pros and Cons of Trees</h3>
<p>Pros are:</p>
<ul>
<li>Straightforward interpretation. Show it your grand mother and she will understand it.</li>
<li>Allow for interaction effects.</li>
<li>Competitive performance.</li>
<li>Can deal with missing values thanks to the <em>surrogate split</em>. For each node the tree algorithm tries to find variables that are highly correlated with the selected splitter. Then, if this variable is not available for a new observation to be classified, the surrogate is used to classifiy the observation on that split node so subsequent nodes can further process the observation.</li>
<li>The variable selection is done automatically and variables that are higher up in the hierarchy are considered to be more imporatant for prediction. We will have a look at ridge regression and LASSO which also do variable selection automatically, but the feature is not present in any method we looked at before.</li>
</ul>
<p>There are some cons also:</p>
<ul>
<li>First and foremost, trees yield <strong>piece-wise</strong> constant predictions, which is typically not what we assume the true underlaying function to look like.</li>
<li>Subsequent splits depend on previous splits. Therefore, if an early split is <em>wrong</em>, everything following afterwards is <em>wrong</em>. This means the algorithm may not be very stable.</li>
</ul>
<p>? question how would you use mars for classification?</p>
</div>
<div id="random-forests" class="section level3">
<h3><span class="header-section-number">8.5.6</span> Random Forests</h3>
<p>Random forests are made up of three main ingredients:</p>
<ul>
<li>regression (or classification) trees</li>
<li>boostrapping</li>
<li>aggregating</li>
</ul>
<p>The algorithm is as follows:</p>
<ul>
<li>draw <span class="math inline">\(n_{tree}\)</span> boostrap samples (of size n obviously).</li>
<li>build for each of them an <em>unpruned</em> tree. However, instead of searching over all <span class="math inline">\(p\)</span> variables for the best split at each node, just consider a random sample of <span class="math inline">\(m_{try}\)</span> variables for the split at each node. Obviously, <span class="math inline">\(m_{try} = p\)</span> is the tree solution introduced before and corresponds to the bagging (which stands for boostrap aggregating, introduced later).</li>
<li>Predict a new data point by aggregating the <span class="math inline">\(n_{tree}\)</span> prediction (majority vote for classification, averaging for regression).</li>
</ul>
<p>To obtain an estimate for the generalization error, you can use the <strong>out-of-bag</strong> approach, that is</p>
<ul>
<li>At each boostrap iteration, make predictions with the data that is not in the boostrap sample.</li>
<li>aggregate the predictions for all <span class="math inline">\(n_{tree}\)</span> trees and compute the error rate and call it <em>out-of-bag estimate of the error rate</em>.</li>
</ul>
<p>The only drawback of trees is that interpretability is lower than for trees.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>For regression you need only one index since you will have just one ouput layer.<a href="flexible-regression-and-classification-methods.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variable-selection-ridge-regression-an-lasso.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
