<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics - Summary</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Computational Statistics - Summary">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics - Summary" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics - Summary" />
  
  
  

<meta name="author" content="Lorenz Walthert">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="non-parametric-density-estimation.html">
<link rel="next" href="cross-validation.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-the-linear-model"><i class="fa fa-check"></i><b>2.1</b> Assumptions of the linear model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#geometric-interpretation"><i class="fa fa-check"></i><b>2.2</b> Geometric interpretation</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#hat-matrix"><i class="fa fa-check"></i><b>2.3</b> Hat matrix</a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multiple-regression-vs.simple-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple regression vs. simple regression</a></li>
<li class="chapter" data-level="2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#properties"><i class="fa fa-check"></i><b>2.5</b> Properties</a></li>
<li class="chapter" data-level="2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#tests"><i class="fa fa-check"></i><b>2.6</b> Tests</a></li>
<li class="chapter" data-level="2.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#diagnostics"><i class="fa fa-check"></i><b>2.7</b> Diagnostics</a></li>
<li class="chapter" data-level="2.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>2.8</b> Generalized least squares</a></li>
<li class="chapter" data-level="2.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-selection"><i class="fa fa-check"></i><b>2.9</b> Model Selection</a><ul>
<li class="chapter" data-level="2.9.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mallows-cp-statistic"><i class="fa fa-check"></i><b>2.9.1</b> Mallow’s cp statistic</a></li>
<li class="chapter" data-level="2.9.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#search-strategies"><i class="fa fa-check"></i><b>2.9.2</b> Search strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html"><i class="fa fa-check"></i><b>3</b> Non-parametric Density Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-histogramm"><i class="fa fa-check"></i><b>3.1</b> The Histogramm</a></li>
<li class="chapter" data-level="3.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#kernels"><i class="fa fa-check"></i><b>3.2</b> Kernels</a><ul>
<li class="chapter" data-level="3.2.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-naive-estimator"><i class="fa fa-check"></i><b>3.2.1</b> The naive Estimator</a></li>
<li class="chapter" data-level="3.2.2" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-kernels"><i class="fa fa-check"></i><b>3.2.2</b> Other Kernels</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-bandwidth"><i class="fa fa-check"></i><b>3.3</b> The Bandwidth</a></li>
<li class="chapter" data-level="3.4" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#bringing-it-all-together"><i class="fa fa-check"></i><b>3.4</b> Bringing it all together</a></li>
<li class="chapter" data-level="3.5" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#other-density-estimators"><i class="fa fa-check"></i><b>3.5</b> Other Density Estimators</a></li>
<li class="chapter" data-level="3.6" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#higher-dimensions"><i class="fa fa-check"></i><b>3.6</b> Higher Dimensions</a><ul>
<li class="chapter" data-level="3.6.1" data-path="non-parametric-density-estimation.html"><a href="non-parametric-density-estimation.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.6.1</b> The Curse of Dimensionality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html"><i class="fa fa-check"></i><b>4</b> Non-parametric Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#alternative-interpretation"><i class="fa fa-check"></i><b>4.1</b> Alternative Interpretation</a></li>
<li class="chapter" data-level="4.2" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#the-bandwidth-1"><i class="fa fa-check"></i><b>4.2</b> The Bandwidth</a></li>
<li class="chapter" data-level="4.3" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#hat-matrix-1"><i class="fa fa-check"></i><b>4.3</b> Hat Matrix</a></li>
<li class="chapter" data-level="4.4" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>4.4</b> Degrees of Freedom</a><ul>
<li class="chapter" data-level="4.4.1" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#applications"><i class="fa fa-check"></i><b>4.4.1</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#inference"><i class="fa fa-check"></i><b>4.5</b> Inference</a></li>
<li class="chapter" data-level="4.6" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#local-polynomial-estimator"><i class="fa fa-check"></i><b>4.6</b> Local Polynomial Estimator</a></li>
<li class="chapter" data-level="4.7" data-path="non-parametric-regression.html"><a href="non-parametric-regression.html#smoothing-splines"><i class="fa fa-check"></i><b>4.7</b> Smoothing splines</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>5</b> Cross Validation</a><ul>
<li class="chapter" data-level="5.1" data-path="cross-validation.html"><a href="cross-validation.html#motivation-and-core-idea"><i class="fa fa-check"></i><b>5.1</b> Motivation and Core Idea</a></li>
<li class="chapter" data-level="5.2" data-path="cross-validation.html"><a href="cross-validation.html#loss-function"><i class="fa fa-check"></i><b>5.2</b> Loss Function</a></li>
<li class="chapter" data-level="5.3" data-path="cross-validation.html"><a href="cross-validation.html#implementations"><i class="fa fa-check"></i><b>5.3</b> Implementations</a><ul>
<li class="chapter" data-level="5.3.1" data-path="cross-validation.html"><a href="cross-validation.html#leave-one-out"><i class="fa fa-check"></i><b>5.3.1</b> Leave-one-out</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.4</b> K-fold Cross-Validation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="cross-validation.html"><a href="cross-validation.html#random-division-into-test-and-training-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Random Division into test and training data set</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="cross-validation.html"><a href="cross-validation.html#properties-of-the-different-schemes"><i class="fa fa-check"></i><b>5.5</b> Properties of the different schemes</a></li>
<li class="chapter" data-level="5.6" data-path="cross-validation.html"><a href="cross-validation.html#shortcuts-for-some-linear-fitting-operators"><i class="fa fa-check"></i><b>5.6</b> Shortcuts for (some) linear fitting operators</a></li>
<li class="chapter" data-level="5.7" data-path="cross-validation.html"><a href="cross-validation.html#isolation-of-each-cross-validation-sample"><i class="fa fa-check"></i><b>5.7</b> Isolation of each cross validation sample</a></li>
<li class="chapter" data-level="5.8" data-path="cross-validation.html"><a href="cross-validation.html#examples-with-r"><i class="fa fa-check"></i><b>5.8</b> Examples with R</a><ul>
<li class="chapter" data-level="5.8.1" data-path="cross-validation.html"><a href="cross-validation.html#application-1-estimating-the-generalization-error"><i class="fa fa-check"></i><b>5.8.1</b> Application 1: Estimating the generalization error</a></li>
<li class="chapter" data-level="5.8.2" data-path="cross-validation.html"><a href="cross-validation.html#application-2-parameter-tuning"><i class="fa fa-check"></i><b>5.8.2</b> Application 2: Parameter Tuning</a></li>
<li class="chapter" data-level="5.8.3" data-path="cross-validation.html"><a href="cross-validation.html#application-3-stochastic-approximation-for-leave-d-out"><i class="fa fa-check"></i><b>5.8.3</b> Application 3: Stochastic approximation for leave d out</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>6</b> Bootstrap</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrap.html"><a href="bootstrap.html#motivation"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="bootstrap.html"><a href="bootstrap.html#the-bootstrap-distribution"><i class="fa fa-check"></i><b>6.2</b> The Bootstrap Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="bootstrap.html"><a href="bootstrap.html#bootstrap-consistency"><i class="fa fa-check"></i><b>6.3</b> Bootstrap Consistency</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.5" data-path="bootstrap.html"><a href="bootstrap.html#boostrap-estimator-of-the-generalization-error"><i class="fa fa-check"></i><b>6.5</b> Boostrap Estimator of the Generalization Error</a></li>
<li class="chapter" data-level="6.6" data-path="bootstrap.html"><a href="bootstrap.html#out-of-boostrap-sample-for-estimating-the-ge"><i class="fa fa-check"></i><b>6.6</b> Out-of-Boostrap sample for estimating the GE</a></li>
<li class="chapter" data-level="6.7" data-path="bootstrap.html"><a href="bootstrap.html#double-boostrap-confidence-intervals"><i class="fa fa-check"></i><b>6.7</b> Double Boostrap Confidence Intervals</a></li>
<li class="chapter" data-level="6.8" data-path="bootstrap.html"><a href="bootstrap.html#three-versions-of-boostrap"><i class="fa fa-check"></i><b>6.8</b> Three Versions of Boostrap</a><ul>
<li class="chapter" data-level="6.8.1" data-path="bootstrap.html"><a href="bootstrap.html#non-parametric-regression-1"><i class="fa fa-check"></i><b>6.8.1</b> Non-parametric Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="bootstrap.html"><a href="bootstrap.html#parametric-boostrap"><i class="fa fa-check"></i><b>6.8.2</b> Parametric Boostrap</a></li>
<li class="chapter" data-level="6.8.3" data-path="bootstrap.html"><a href="bootstrap.html#model-based-bootstrap"><i class="fa fa-check"></i><b>6.8.3</b> Model-Based Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="bootstrap.html"><a href="bootstrap.html#conclusion"><i class="fa fa-check"></i><b>6.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#indirect-classification---the-bayes-classifier"><i class="fa fa-check"></i><b>7.1</b> Indirect Classification - The Bayes Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#direct-classification---the-discriminant-view"><i class="fa fa-check"></i><b>7.2</b> Direct Classification - The Discriminant View</a><ul>
<li class="chapter" data-level="7.2.1" data-path="classification.html"><a href="classification.html#lda"><i class="fa fa-check"></i><b>7.2.1</b> LDA</a></li>
<li class="chapter" data-level="7.2.2" data-path="classification.html"><a href="classification.html#qda"><i class="fa fa-check"></i><b>7.2.2</b> QDA</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#indirect-classification---the-view-of-logistic-regression"><i class="fa fa-check"></i><b>7.3</b> Indirect Classification - The View of Logistic Regression</a></li>
<li class="chapter" data-level="7.4" data-path="classification.html"><a href="classification.html#discriminant-analysis-or-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Discriminant Analysis or Logistic Regression?</a></li>
<li class="chapter" data-level="7.5" data-path="classification.html"><a href="classification.html#multiclass-case-j-2"><i class="fa fa-check"></i><b>7.5</b> Multiclass case (J &gt; 2)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html"><i class="fa fa-check"></i><b>8</b> Flexible regression and classification methods</a><ul>
<li class="chapter" data-level="8.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models"><i class="fa fa-check"></i><b>8.1</b> Additive Models</a><ul>
<li class="chapter" data-level="8.1.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#structure"><i class="fa fa-check"></i><b>8.1.1</b> Structure</a></li>
<li class="chapter" data-level="8.1.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-procedure"><i class="fa fa-check"></i><b>8.1.2</b> Fitting Procedure</a></li>
<li class="chapter" data-level="8.1.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#additive-models-in-r"><i class="fa fa-check"></i><b>8.1.3</b> Additive Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#mars"><i class="fa fa-check"></i><b>8.2</b> MARS</a><ul>
<li class="chapter" data-level="8.2.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#details-for-dummies"><i class="fa fa-check"></i><b>8.2.1</b> Details for Dummies</a></li>
<li class="chapter" data-level="8.2.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#example"><i class="fa fa-check"></i><b>8.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#neural-networks"><i class="fa fa-check"></i><b>8.3</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#fitting-neural-networks-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Fitting Neural Networks (in R)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>8.4</b> Projection Pursuit Regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#proejction-pursuit-example"><i class="fa fa-check"></i><b>8.4.1</b> Proejction Pursuit Example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>8.5</b> Classification and Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#prediction-given-partitioning"><i class="fa fa-check"></i><b>8.5.1</b> Prediction given Partitioning</a></li>
<li class="chapter" data-level="8.5.2" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#assumptions-on-the-patritions"><i class="fa fa-check"></i><b>8.5.2</b> Assumptions on the Patritions</a></li>
<li class="chapter" data-level="8.5.3" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#algorithm"><i class="fa fa-check"></i><b>8.5.3</b> Algorithm</a></li>
<li class="chapter" data-level="8.5.4" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#backward-deletion-pruning"><i class="fa fa-check"></i><b>8.5.4</b> Backward Deletion / Pruning</a></li>
<li class="chapter" data-level="8.5.5" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#pros-and-cons-of-trees"><i class="fa fa-check"></i><b>8.5.5</b> Pros and Cons of Trees</a></li>
<li class="chapter" data-level="8.5.6" data-path="flexible-regression-and-classification-methods.html"><a href="flexible-regression-and-classification-methods.html#random-forests"><i class="fa fa-check"></i><b>8.5.6</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html"><i class="fa fa-check"></i><b>9</b> Variable Selection - Ridge Regression and Lasso</a><ul>
<li class="chapter" data-level="9.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#ridge-regression"><i class="fa fa-check"></i><b>9.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="9.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#lasso"><i class="fa fa-check"></i><b>9.2</b> Lasso</a></li>
<li class="chapter" data-level="9.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#extensions"><i class="fa fa-check"></i><b>9.3</b> Extensions</a><ul>
<li class="chapter" data-level="9.3.1" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#elastic-net"><i class="fa fa-check"></i><b>9.3.1</b> Elastic Net</a></li>
<li class="chapter" data-level="9.3.2" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#relaxed-lasso"><i class="fa fa-check"></i><b>9.3.3</b> Relaxed Lasso</a></li>
<li class="chapter" data-level="9.3.4" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#sparse-group-lasso"><i class="fa fa-check"></i><b>9.3.4</b> (Sparse) Group Lasso</a></li>
<li class="chapter" data-level="9.3.5" data-path="variable-selection-ridge-regression-and-lasso.html"><a href="variable-selection-ridge-regression-and-lasso.html#oracle-properties"><i class="fa fa-check"></i><b>9.3.5</b> Oracle Properties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html"><i class="fa fa-check"></i><b>10</b> Bagging and Boosting</a><ul>
<li class="chapter" data-level="10.1" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#subagging"><i class="fa fa-check"></i><b>10.2</b> Subagging</a></li>
<li class="chapter" data-level="10.3" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#l_2-boosting"><i class="fa fa-check"></i><b>10.3</b> <span class="math inline">\(L_2\)</span>-Boosting</a></li>
<li class="chapter" data-level="10.4" data-path="bagging-and-boosting.html"><a href="bagging-and-boosting.html#some-unfinished-stuff"><i class="fa fa-check"></i><b>10.4</b> Some unfinished stuff</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="round-up.html"><a href="round-up.html"><i class="fa fa-check"></i><b>11</b> Round up</a><ul>
<li class="chapter" data-level="11.1" data-path="round-up.html"><a href="round-up.html#comparing-models"><i class="fa fa-check"></i><b>11.1</b> Comparing models</a></li>
<li class="chapter" data-level="11.2" data-path="round-up.html"><a href="round-up.html#exercises-take-aways"><i class="fa fa-check"></i><b>11.2</b> Exercises Take-aways</a></li>
<li class="chapter" data-level="11.3" data-path="round-up.html"><a href="round-up.html#cheatsheet"><i class="fa fa-check"></i><b>11.3</b> Cheatsheet</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>12</b> Introduction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics - Summary</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-parametric-regression" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Non-parametric Regression</h1>
<p>We assume a model of the form</p>
<p><span class="math display">\[ E[Y|X = x] = m(x)\]</span> which is more general than assuming <span class="math inline">\(Y_i = m(x_i) + \epsilon_i\)</span> since in the former, the noise does not have to be additive. We can use Bayes’ Theorem to deduce such an estimator <span class="math inline">\(\hat{m}\)</span> using density estimates of our predictor and response variable. In that sense, non-parametric regression really builds on top of non-parametric density estimation.</p>
<p><span class="math display">\[
\begin{split}
P[Y|X] &amp;=  \frac{\mathbb{P}[X|Y]}{\mathbb{P}[Y]}\mathbb{P}[Y] \\ 
f_{Y|X} &amp; = \frac{f_{X|Y}}{f_X}f_Y \\
f_{Y|X} &amp; = \frac{f_{X, Y}}{f_X}   \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \text{|} \times y\\
f_{Y|X}y &amp; = \frac{f_{X, Y}}{f_X}y \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \text{| taking the integral}\\
E[Y|X] &amp; = \int \frac{f_{X, Y}}{f_X}ydy
\end{split}
\]</span> Whereas for <span class="math inline">\(\hat{f}_{X, Y}\)</span>, we can use a product kernel. This formula simplifies quite a bit and yields the Nadaraja-Watson Kernel, which is essentially just a weighted mean of response values. <span class="math display">\[\hat{m}(x) = \frac{\sum\limits_{i = 1}^n K\Big(\frac{x-X_i}{h}\Big)Y_i}{\sum\limits_{i = 1}^nK\Big(\frac{x-X_i}{h}\Big)} = \frac{\sum\mathcal{w}(x)_i Y_i}{\sum\mathcal{w}(x)_i} = \sum\limits_{i = 1}^n \tilde{\mathcal{w}}(x)_i Y_i = \tilde{\mathbf{w}}(x)&#39;\mathbf{Y}\]</span> The weights <span class="math inline">\(\tilde{\mathcal{w}}_i\)</span> are normalized weights, i.e. <span class="math inline">\(\tilde{\mathcal{w}}_i = \mathcal{w}_i / \sum\limits_{k = 1}^n \mathcal{w}_k\)</span></p>
<div id="alternative-interpretation" class="section level2">
<h2><span class="header-section-number">4.1</span> Alternative Interpretation</h2>
<p>It can be easily shown that the solution corresponds to the following minimization problem:</p>
<p><span class="math display">\[ m(x) = \arg\min\limits_{m_x} \sum\limits_{i = 1}^nK\Big(\frac{x-X_i}{h}\Big)\big(Y_i-m_x\big)^2 \]</span></p>
<p>We can interpret this as a weighted (local) regression. For a given <span class="math inline">\(x\)</span>, search for the best local constant <span class="math inline">\(m_x\)</span> that minimizes the weighted residual sum of squares the most. Residuals of data points close to <span class="math inline">\(x\)</span> get a high weight in this sum (via the kernel).</p>
</div>
<div id="the-bandwidth-1" class="section level2">
<h2><span class="header-section-number">4.2</span> The Bandwidth</h2>
<p>The bandwidth parameter <span class="math inline">\(h\)</span> has a similar role as in non-parametric density estimation. Large <span class="math inline">\(h\)</span> implies very smooth functions, low variance, high bias. Small <span class="math inline">\(h\)</span> on the other hand imply (more) erratic functions, high variance, low bias.</p>
<p>An interesting case is <span class="math inline">\(h \rightarrow \infty\)</span>, for which all weights become equal. This corresponds to an estimator <span class="math inline">\(m(x) = \bar{y}\)</span>, with one degree of freedom (see below).</p>
</div>
<div id="hat-matrix-1" class="section level2">
<h2><span class="header-section-number">4.3</span> Hat Matrix</h2>
<p>As in chapter 1, we can also obtain a smoother matrix <span class="math inline">\(S\)</span> that maps the observed response values to the fitted values. From above, we have: <span class="math display">\[\hat{y}_i = \tilde{\mathbf{w}}(x)&#39;\mathbf{Y}\]</span></p>
<p><span class="math display">\[
\mathbf{\hat{Y}} = \mathcal{S} \mathbf{Y} = 
\begin{pmatrix}\hat{y}_1 \\\vdots\\\hat{y}_n\end{pmatrix} =
\begin{pmatrix}\tilde{\mathbf{w}}(x_1)&#39;\\\vdots\\\tilde{\mathbf{w}}(x_n)&#39;\end{pmatrix} 
\times \begin{pmatrix}y_1 \\\vdots\\y_n\end{pmatrix}
\]</span> Where <span class="math inline">\(\tilde{\mathbf{w}}(x_1)&#39;\)</span> is a row vector of length <span class="math inline">\(n\)</span> with the normalized kernel weights.</p>
<p>Let us adopt the notation from the script (kind of) and denote the kernel weight for observation <span class="math inline">\(w_s\)</span> on observation <span class="math inline">\(x_r\)</span> with <span class="math inline">\(\tilde{w}_s(x_r)\)</span>, i.e. <span class="math display">\[ \tilde{w}_s(x_r) = \sum\limits_{s = 1} K\Big(\frac{x_r - x_s}{h}\Big) / \sum K(\cdot)\]</span> The smoother matrix from above can be rewritten as a matrix with scalar entries instead of vectors</p>
<p><span class="math display">\[ 
\begin{pmatrix}
\tilde{w}_1(x_1) \;\; \tilde{w}_2(x_1) \;\; \dots \;\;\tilde{w}_n(x_1) \\
\tilde{w}_1(x_2) \;\; \tilde{w}_2(x_2) \;\; \dots \;\;\tilde{w}_n(x_2)\\
\tilde{w}_1(x_3) \;\;\tilde{w}_2(x_3) \;\; \dots \;\;\tilde{w}_n(x_n) \\
\end{pmatrix} 
\]</span> So <span class="math inline">\([S]_{r, s} = \tilde{w}_s(x_r)\)</span>.</p>
<p>This matrix can be obtained easily by smoothing n unit vectors <span class="math inline">\(u_{n \times 1}\)</span>. To obtain the j-th column, smooth the unit vector that has in the j-th position a <span class="math inline">\(1\)</span> and all other positions <span class="math inline">\(0\)</span>. Let us assume equidistant x values. The following smoothed y values evaluated at all <span class="math inline">\(x \in \{1, ..., n\}\)</span> give the values for the first column of <span class="math inline">\(S\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">j &lt;-<span class="st"> </span><span class="dv">1</span>
n &lt;-<span class="st"> </span><span class="dv">10</span>
u &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)
u[j] &lt;-<span class="st"> </span><span class="dv">1</span>
data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(
  <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span>n,
  <span class="dt">y =</span> u
)
<span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="comp_stats_summary_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>But why? The trick is that, since all but the j-th y value are zero and therefore the sum is only taken over one element. For the non-zero element, the <span class="math inline">\(y_i\)</span> is one, so the fitted value is just the normalized kernel weight.</p>
<p><span class="math display">\[\hat{m}(x_r)^{(j)} =  \frac{K\Big(\frac{x_r - x_j}{h}\Big)}{\sum K(\cdot)} = \tilde{w}_j(x_r)\]</span></p>
<p>Here is one way to implement that in R for the Nadaraya-Watson Kernel.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(reg)
Id &lt;-<span class="st"> </span><span class="kw">diag</span>(n)
S &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, n)
h &lt;-<span class="st"> &quot;your bandwidth&quot;</span>
<span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {
  S[, j] &lt;-<span class="st"> </span><span class="kw">ksmooth</span>(data<span class="op">$</span>x, Id[, j], <span class="dt">x.point =</span> data<span class="op">$</span>x, <span class="dt">kernel =</span> <span class="st">&quot;normal&quot;</span>, <span class="dt">bandwidth =</span> h)<span class="op">$</span>y
}</code></pre></div>
</div>
<div id="degrees-of-freedom" class="section level2">
<h2><span class="header-section-number">4.4</span> Degrees of Freedom</h2>
<p>Note that we need a different measurement of degrees of freedom than the one we used in the <em>parametric</em> case in chapter 1, where the degrees of freedom simply corresponded to the number of parameters used. As this is non-parametric regression, we don’t have parameters and hence cannot sum up the number of parameters to calculate the degree of freedom. Recall from chapter 1 that the trace of the smoothing matrix was equal to the number of parameters used: <span class="math display">\[tr(P) = \textbf{tr}(X(X&#39;X)^{-1}X&#39;) = \textbf{tr}((X&#39;X)^{-1}X&#39;X) = \textbf{tr}(I_{p\times p}) = p\]</span> Hence, we can generalize the concept of degrees of freedom from number of parameters to the trace of the smoother matrix. For regression, the two coincide, for non-parametric methods, we get an estimate of the degrees of freedom only via the trace.</p>
<p>There is a one-to-one relationship between the degrees of freedom and the bandwidth parameter <span class="math inline">\(h\)</span>, which we can show in a diagram:</p>
<p><img src="figures/relationship-h-df.png" width="650px" /></p>
<p>This can be derived from the two extreme cases:</p>
<ul>
<li><span class="math inline">\(h \rightarrow \infty\)</span> means all weights are equal, which means for each data point <span class="math inline">\(x_i\)</span>, the fitted value <span class="math inline">\(\hat{m}(x_i)\)</span> is just the grand mean of <span class="math inline">\(y\)</span>. This corresponds to one degree of freedom.</li>
</ul>
<p><span class="math display">\[\hat{m}(x) = \bar{y} \]</span></p>
<ul>
<li><span class="math inline">\(h \rightarrow 0\)</span>. If we assume <span class="math inline">\(n\)</span> distinct, equi-spaced <span class="math inline">\(x_i\)</span> values, then the fitted values in the neighborhood of <span class="math inline">\(\hat{m}(x_i)\)</span> are just the observed response value <span class="math inline">\(y_i\)</span>, since all weights for points other than <span class="math inline">\(y_i\)</span> are zero. Or to be more precise: If the distance between neighboring points is <span class="math inline">\(\eta\)</span>, then, for <span class="math inline">\(h \leqslant \eta\)</span> :</li>
</ul>
<p><span class="math display">\[ \hat{m}(x) = y_{i^*} \;\;\ \text{with} \;\; i^* =  
  \arg\min\limits_{0 \leqslant i \leqslant n} |x - x_i| \]</span> This corresponds to <span class="math inline">\(n\)</span> degrees of freedom. * We can interpolate between these two extreme cases in a monotone way.</p>
<p>For non-parametric density estimation, this is very similar except that the esimates around a data point would not be constant for very small <span class="math inline">\(h\)</span>, but the weigth the kernel takes for that value of <span class="math inline">\(\frac{x-x_i}{h}\)</span> because all but one of the summands are zero in <span class="math inline">\(\sum\limits_{i = 1}^n K\Big(\frac{x -x_i}{h}\Big)\)</span>.</p>
<div id="applications" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Applications</h3>
<p>There are two main applications for the degree of freedom:</p>
<ul>
<li><strong>As a tuning parameter</strong>. For every <span class="math inline">\(h\)</span>, we can find the corresponding degrees of freedom or more interestingly - for every(desired) degree of freedom, we can find an <span class="math inline">\(h\)</span>. Instead of varying <span class="math inline">\(h\)</span> directly, we can vary the degrees of freedom, which are directly <strong>comparable</strong> accross different techniques. We can compare OLS with different kernel smoothers, splines etc. (see below), which is not possible with <span class="math inline">\(h\)</span> alone.</li>
<li><strong>For inference</strong>. For unbiased estimators (such as the variance of the residuals), we need the degrees of freedom (see below).</li>
</ul>
</div>
</div>
<div id="inference" class="section level2">
<h2><span class="header-section-number">4.5</span> Inference</h2>
<p>As with parametric regression, we want to do inference. Not on any parameters (since there are none), but on the fitted values. First and foremost, we want to obtain confidence intervals for the regression line, that is, obtaining point-wise lower and upper bounds of the confidence internal <span class="math display">\[ I_i = \hat{m}(x_i) ± 1.96 *\hat{sd}(\hat{m}(x)) \]</span> Which holds because the fitted value is asymtotically normally distributed. We know already how to obtain <span class="math inline">\(\hat{m}(x)\)</span>, now we need to find the standard deviation of the fitted values.</p>
<p>From the fundamental equation <span class="math inline">\(Cov(\mathbf{A}x) = \mathbf{A} Cov(x) \mathbf{A}&#39;\)</span>, we get <span class="math display">\[ \text{Cov}(\hat{m}(x)) = \text{Cov}(\mathcal{S}\mathbf{y}) = 
\mathcal{S} \text{Cov}(y) \mathcal{S}&#39; = \sigma_\epsilon^2\mathcal{SS}&#39;
\;\;\;\;\;|\;\; \text{since Cov}(y) = \text{Cov}(\epsilon)\]</span> Which is a <span class="math inline">\(p\times p\)</span> matrix. For a specific data point <span class="math inline">\(x_i\)</span>, we have <span class="math display">\[ \text{Var}(\hat{m}(x_i))= \text{Cov}(\hat{m}(x_i),  \hat{m}(x_i)) = \hat{\sigma}_\epsilon^2 (\mathcal{S} \mathcal{S}&#39;)_{ii} \]</span></p>
<p>Now, we only need to estimate <span class="math inline">\(\sigma^2_\epsilon\)</span>. Using the generalized method to compute degrees of freedom, we can use the following estimator:</p>
<p><span class="math display">\[ \hat{\sigma}_\epsilon^2 = \frac{1}{n-\textbf{tr}(\mathcal{S})}
\sum\limits_{i = 1}^n (Y_i - \hat{m}(x_i))^2\]</span></p>
</div>
<div id="local-polynomial-estimator" class="section level2">
<h2><span class="header-section-number">4.6</span> Local Polynomial Estimator</h2>
<p>Expanding on the formulation of the Nadaraya-Watson Kernel as a weighted least square problem, we can use centered polynomials to approximate <span class="math inline">\(y_i\)</span> instead of a local constant <span class="math inline">\(\hat{m}_x\)</span>. That is, computing our estimator as</p>
<p><span class="math display">\[\hat{m}(x) = \arg\min\limits_{m_x} \sum\limits_{i = 1}^nK\Big(\frac{x-X_i}{h}\Big)
\big(Y_i-\beta_0 + \beta_1 (x_i -x) + ... \beta_{p-1} (x_i-x)^{p-1}\big)^2 \]</span></p>
<p>This has the advantage of yielding a <em>lolcal linear</em> regression curve at the borders, whereas the Nadaraya-Watson Kernel yields <em>local constant</em> regression curves in these regions (since at the border, no new points are are taken into account).</p>
<p>Another advantage is that we can find derivaties of the regression estimate at each point easily.</p>
</div>
<div id="smoothing-splines" class="section level2">
<h2><span class="header-section-number">4.7</span> Smoothing splines</h2>
<p><span class="math display">\[\sum\limits_{i = 1}^n (Y_i - \hat{m}(x_i))^2 + \lambda\int{m&#39;&#39;(z)^2dz} \]</span> Where <span class="math inline">\(\lambda\)</span> is a smoothing parameter. There are two extreme cases:</p>
<ul>
<li><span class="math inline">\(\lambda = 0\)</span>, Any perfectly interpolating function <span class="math inline">\(m\)</span> is possible.</li>
<li><span class="math inline">\(\lambda \rightarrow \infty\)</span>. The ols solution with <span class="math inline">\(m&#39;&#39;(x) = 0\)</span>.</li>
</ul>
<p>At every distinct <span class="math inline">\(x_i\)</span>, there is a knot. The function <span class="math inline">\(\hat{m}(x)\)</span> is a piecwise polynomial function where at each knot, the left and the right polynomial agree in their valued <span class="math inline">\(\hat{m}\)</span> and the first and second derivative. For the <span class="math inline">\(n-1\)</span> polynomials, you have <span class="math inline">\(4\)</span> parameters each, so <span class="math inline">\((n-1)*4\)</span> parameters to estimate. The constraints are <span class="math inline">\((n-2)*3 + 2\)</span> conditions, leaving exactly <span class="math inline">\(n\)</span> free parameters.</p>
<p>There is a kernel estimator which approximately corresponds to the smoothing spline solution. Interestingly, this solution has a locally varying bandwidth.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="non-parametric-density-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cross-validation.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
