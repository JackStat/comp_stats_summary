# Nonparametric Density Estimation

## The Histogramm
The simplest density estimator is the histogram. The drawback is that we need
to specify two parameters. The origin $x_0$ and the bandwidth $h$. At least
the former is highly arbitrary and both affect the resulting histogram 
remarkably.


## Kernels
### The naive Estimator

We can compute the relative frequency of observations falling into some region
and use these frequencies as estimators for the probabilities.

Let's consider some example data distributed uniformly at random between 
$-0.5$ and $+0.5$. 
Let us try to estimate the densities via the relative frequencies. We consider
three points in x: $-0.5, 0$ and $+0.5$. We set $h$ to 1/2 for now.
For x equal zero, we know that all
points are comprised in the range, so the relative frequency is one, for the
two other x values, it is a half each. We can interpolate and then the a
triangle. We don't need to normalize anything since the area has size one
already, which we need for it to be a density. However, if we use a larger
$h$, for example $h = 1$, we need to divide by $2$ to get an area of size one
since at all three points, the relative frequency is one and it becomes zero
only when the absolute value of x is larger than 3/2.
Hence, we found the formula to obtain densities.
$$ f(x) = \frac{1}{2h}\mathbb{P}[x-h< X < x+h]$$
Which translates into frequencies for our estimator
$$ \hat{f}(x) = \frac{1}{2hn}\#(X_i \in [x-h, x+h))$$
We can see how changing $h$ improves our estimates density estimates:
```{r}
x <- runif(10000, min = -1/2, max = 1/2)
dens <- density(x, kernel = "rectangular", bw = 1/2)


eval_one <- function(e_eval_one, x, h) {
  1/(2*h)* mean(e_eval_one + h > x & e_eval_one - h < x)
}

naive_density <- function(x, h, x_eval) {
  data_frame(
    x_eval = x_eval, 
    y_est = map_dbl(x_eval, eval_one, x = x, h = h),
    h = h
  )
}

one_est <- naive_density(x, 1/2, seq(-1, 1, length.out = 3))

ggplot(one_est, aes(x = x_eval, y = y_est)) + geom_path()
```

This is quite wrong, as we know the true distribution is uniform between 
minus one and one.
However, as you can see by playing around with the shiny app further down the
page, it seems that with $h \rightarrow \infty$, we are approaching the true 
distribution.
The nice thing about this estimator is that there is no need to set the 
tuning parameter $x_0$ anymore.

You can represent the naive kernel differently, namely as a kernel.
We have 
\begin{equation}
\begin{split}
f(x) = & \frac{1}{2nh}\#\{X_i \in [x- d, x+ d)\} \\
     = & \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[x_i \in [x- d, x+ d)]} \\
     = & \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[(x_i - x)/h \in [-1, 1)]} \\
     = & \frac{1}{2nh} \sum\limits_{i = 1}^n 1_{[(x - x_i)/h \in [-1, 1)]} | \text{You can flip signs} \\
     = & \frac{1}{2nh} \sum\limits_{i = 1}^n w'\big(\frac{x -x_i}{h}\big) \\
\end{split}
\end{equation}

 \begin{equation}
w'\big(\frac{x - x_i}{h}\big) = \begin{cases}
      1 \;\;\ \text{if} \;\;\ | \frac{x - x_i}{h} | > 1
      \\
      0 \;\;\ \text{else}.
      \end{cases}
\end{equation}
Where we can collapse $2$ and our weight function $w'(x)$, so we end up with the 
following.

\begin{equation}
\begin{split}
f(x) = & \frac{1}{nh} \sum\limits_{i = 1}^n w\big(\frac{x - x_i}{h}\big) \\
\end{split}
\end{equation}


 \begin{equation}
w\big(\frac{x - x_i}{h}\big) = \begin{cases}
      \frac{1}{2} \;\;\ \text{if} \;\;\ | \frac{x - x_i}{h} | > 1
      \\
      0 \;\;\ \text{else}.
      \end{cases}
\end{equation}


The function $w$ is an example of a kernel function since it satifsfies the 
condition of a kernel, which are

* integrates to one, $\int\limits_{-\infty}^\infty K(X)dx = 1$
* is symmetric, $K(x) = K(-X)$
* is strictly positive, $K(X)\geqslant 0$

$w(\cdot)$ is the rectangular kernel, which becomes obvious when plotting 
it.
```{r}
rectangular <- function(x) {
  if_else(abs(x) < 1/2, 1, 0)
}

normal <- function(x) {
  (2*pi)^(-1/2)*exp(-x^2/2)
}

cosine <- function(x) {
  if_else(abs(x) < 1, pi / 4 * cos(pi / 2 * x), 0)
}

ggplot(data_frame(x = 0), aes(x = x)) +
  stat_function(fun = rectangular, aes(color = "rectangular")) +
  stat_function(fun = normal, aes(color = "normal")) +
  stat_function(fun = cosine, aes(color = "cosine")) +
  scale_x_continuous(limits = c(-2, 2))
```

It is *piece-wise* constant, since evyer point is either in or out of the 
interval. The density estimate is not smoth since the kernel is not smoth. 
$\hat{f}(x)$ inherits also other properties from the kernel, e.g. it is strictly
positive if the kernel is strictly positive and it integrates to one if the 
kernel does so. Therefore, to obtain a smooth density estimate, we need to 
choose a smooth kernel. The gaussian kernel seems to be a natural choice. 
Instead of giving an observation a weight of either one or zero, values in 
between are also possible, depending on the distance of the point relative
to the point for which we want to obtain a density esimate and depending on 
$h$. Some kernels such as the normal kernel have the drawback that they their
weights are never getting to zero, which implies computational cost at limited
gain. We can instead choose a kernel like the cosine kernel, which also smooth
but reaches zero relatively quickly, which speeds up calculations.


### The bandwidth

More important than the kernel is the bandwidth. A small bandwidth yields
a very wiggly function (since only the points in the very close neighbourhood
are given a signicifant weight), a large bandwidth will produce a function 
that is varying slowly as a function of x.
The bandwidth is closely related to the 



### Bring it all together
Here is a shiny app that lets you try different parameters.
```{r, echo = FALSE}
knitr::include_app("https://lorenzwalthert.shinyapps.io/naive_density/", 
  height = "1000px")
```

  

