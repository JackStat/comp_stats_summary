# Flexible regression and classification methods 
```{r, echo=FALSE, message=FALSE, warning=FALSE} 
library("gridExtra")
```

The curse of dimensionality makes it very hard to estimate fully nonparametric 
regression function $\hat{m} = \mathbb{E}[Y|X = x]$ or classification function 
$\hat{\pi}_j = \mathbb{P}[Y = j | X = x]$. Hence, by making some (reasonable) 
structural assumptions, we can improve our models significantly. Generally, we 
consider the mapping $\mathbb{R}^p \rightarrow \mathbb{P}$ via the function 
$g(\cdot)$ for both the regression and the classification problem.

## Additive Models 
### Structure
One assumption we can make is to assume a particular functional form of 
$g(\cdot)$, namely an *additive*. That is 
$$g_{add} = \mu + \sum\limits_{j = 1}^pg(x_j) $$ $E[g(x_j)] = 0$ is required to
make the model identifiable only. Note that we have not placed any assumption
on $g(x_j)$ yet, that is, $g(x_j)$ can be fully non-parametric, but each
dimension is mapped separately. In other words every $g(x_j) \;\; j = 1, ..., p$
models one input dimension and mapping of input to output is obtained by summing
the transformed inputs up. This eliminates the possibility of interaction
effects.

### Fitting Procedure 
Additive models can be estimated with a technique called back-fitting.
However, the model can be estimated with any nonparametric method for
one-dimensional smoothing. Here is the receipt:

* since we assume an additive model, we need to initialize all $p$ components of
  it with zero, that is setting $g_j(\cdot) = 0 \;\; j = 1,..., p$ plus setting 
  $\mu = n^{-1}\sum Y_i$. * Then we fit one-dimensional smoother repeatedly, 
  that is solving the one-dimensional smoothing problem 
  $Y - \mu - \sum\limits_{j \neq k}\hat{g}_j = \hat{g}_j(x_j)$, or put 
  differently $\hat{g}_j = S_j(Y - \mu1 - \sum\limits_{j \neq k}g_j)$. 
  This has to be done repeatedly for $j = 1, ..., p, 1, ..., p \text{etc.}$. 
* Stop iterating when functions don't change much anymore, that is, when the 
  following quantity is less than a certain tolerance. 
  $$\frac{|\hat{g}_{i, new} - \hat{g}_{i, old}|}{|\hat{g}_{i, old}|}$$ 
* Normalize the functions by subtracting the mean from them: 
  $$\tilde{g}_j = \hat{g}_j - n^{-1} \sum\limits_{i = 1}^n \hat{g}_j(x_{ij})$$

Back-fitting is a **coordinate-wise** optimization method that optimizes one 
coordinate at the time (one $g(\cdot)$, but can be more than one parameter), which
may be slower in convergence than a general gradient descent that optimizes all
directions simultaneously but also more robust.

```{r, echo=FALSE} 
knitr::include_graphics("figures/coordinate_wise.png") 
```

```{r, echo=FALSE, eval=FALSE} 
# To Do: Think about additive models for classification. 
``` 

### Additive Models in R 
You can use the package **mgcv** 
```{r} 
data("ozone", package = "gss") 
fit <- mgcv::gam(upo3 ~ s(vdht) + s(wdsp) + s(hmdt) + s(sbtp), 
           data = ozone) 

plot(fit, pages = 1) 
``` 
You can see
that vdht enters the model almost linearly. That is, with an increase of one
unit of vdht, the predicted 03 value increases linearly. sbtp is different.
Depending on the value of sbtp, the increase in the predicted value is
different. Low sbtp values hardly have an impact on the response, higher values
do.

## MARS 
MARS stands for multivariate adaptive regression splines and allows
pice-wise linear curve fitting. In contrast to GAMs, they allow for interactions
between the variables. MARS is very similar to regression trees, but it has a
continuous response. It uses reflected pairs of basis functions

\begin{equation} 
  (x_j -d)_{+} = \begin{cases} x_j - d \;\;\ \text{if} \;\;\ x_j - d > 0 \\ 
  0 \;\;\ \text{else}. 
  \end{cases} 
\end{equation} and it counterpart
$(d - x_j)_{+}$. The index $j$ refers to the j-th predictor variable, d is a
knot at one of the $x_js$. The pool of basis functions to consider is called
$\mathbf{B}$ and contains all variables with all potential knots, that is $$
\mathcal{B} = \{(x_j - d)_+ \;\;\; (d - x_j)_+ \;\;\;j = \{1, ..., p\} \;\; d =
\{x_{1j}, ..., x_{nj}\}\}$$ The model then is $$ g(x)  = \mu + \sum\limits_{m =
1}^M\beta_m h_m(x) = \sum\limits_{m = 0}^M\beta_m h_m(x)$$ The model uses
forward selection and backward deletion and optimizes some (generalized)
cross-validation criterion. Here is the receipt:

1. initialize $\mathcal{M} = \{h_0(\cdot) = 1\}$ and estimate $\beta_0$ as the 
data average of the response $n^{-1}\sum\limits_{i = 1}^n Y_i$. 2. for $r = 1,
2, ...$ search for a new pair of function $(h_{2 r-1} \;\; h_{2 r})$ which are of
the form $$h_{2 r-1} = (x_j -d)_+ \times h_l$$ $$h_{2 r} = (d - x_j)_+ \times
h_l$$ that reduce the residual sum of squares the most with some $h_l$ from 
$\mathcal{M}$ that some basis functions from $\mathcal{B}$ does *not* contain  
$x_j$. The model becomes $$\hat{g}(x) = \sum\limits_{m = 0}^{2r}\hat{\beta}_m
h_m(x)$$ which can be estimated by least squares. Update the set $\mathcal{M}$
to be $\mathcal{M} = \mathcal{M}_{old} \cup \{h_{2r-1}, h_{2r}\}$ 3. Iterate the
above step until the set $\mathcal{M}$ becomes *large enough*. 4. Do backward
deletion (*pruning*) by removing the *one* function from a reflected pair that
increases the residual sum of squares the least. 5. Stop the pruning when some
GCV score reaches its minimum.

### Details for Dummies 
Note that by using reflective pairs $\big\{(x_j - d)_+
\;\;\; (d - x_j)_{+}\; \}$, we construct a piece-wise linear function with one
knot at $d$, since we sum the functions (which both a have a zero part that does
not overlap) up. This hinge function (or better the two parts of individually)
$\beta$ will be multiplied with a respective $\beta$, so the slope is adaptive.
Also, since each of the functions have its own $d$, the kink of the function
must not occur at $y = 0$.

From the receipt above, we can see that $d$-way interactions are only possible 
if a $d-1$-way interaction with a subset of the $d$-way interaction is already 
in the model. For interpretability and other reasons, restricting the number of 
interactions to three or two is beneficial. Restricting the degree of 
interaction to one (which is actually no interaction) will yield an additive 
model.

## Example 
Let us consider the simple case of a one dimensional predictor space.
We add noise to data that is piece-wise linear up to $x= 100$ and then follows a
sine-wave. We then fit three mars models with different number of maximal knots.

```{r, message=FALSE,warning=FALSE} 
library("earth") 
sim <- data_frame( x = -25:75, 
                   y = pmax(0, x - 40) + rnorm(100, 0, 3)) %>% 
  bind_rows(data_frame( x = 75:100, y = -1*sqrt(x)* sin(x/3) + 30))

fitted <- data_frame( 
  nk3  = earth(y~x, data = sim, nk = 3), 
  nk5  = earth(y~x, data = sim, nk = 5), 
  nk100 = earth(y~x, data = sim, nk = 100)
)


sim2 <- fitted %>% 
  map_df(~predict(.x)) %>% 
  bind_cols(sim) %>% gather(key, value, -x, -y)


ggplot(sim2) + 
  geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y = value, color = key)) 

summary(fitted$nk3) 
``` 
The example shows what we expected. The
green line with a maximum of three knots uses just one knot around 30, and only
the right part of the reflected pair is used. It cannot distinguish between the
linear segment between 40 and 75 and the sine-wave afterwards. By allowing more
knots, we can see that the red line fits the data quite well. Note that the
default of `degree` is just $1$, so we don't have interaction terms in the
model. This does not matter for a one-dimensional example anyways.


## Neural Networks 
Neural networks are high-dimensional non-linear regression
models. The way it works is best illustrated with a picture.

```{r, echo=FALSE} 
knitr::include_graphics("figures/nn.png") 
```

This is a neural network with one hidden layer, $p$ input layers and $q$ output
layers. Mathematically speaking, the model is: $$ g_k(x) = f_0\Big(\alpha_k +
\sum\limits_{h = 1}^q w_{ij} \phi(\tilde{\alpha}_h + \sum\limits_{j = 1}^p
w_{jh}x_j)\Big)$$ Where $\phi(x)$ is the sigmoid function $\frac{exp(x)}{1 +
exp(x)}$, $f_0$ is the sigmoid function for classification and the identity for
regression. In the case of regression $k = 1$ is used, for classification we use
$g_0, ..., g_{J-1}$ and then use the Bayes classifier $\mathcal{\hat{C}(x)} =
\arg\max\limits_{0<j<J-1} g_j(x)$ (is that correct?), which is called the
softmax in the neural network literature.

### Fitting Neural Networks (in R) 
The `nnet` function from the package with the
same name basically uses gradient descent to maximize the likelihood. It is
important to first scale the data so the gradient descent does not get stuck in
flat regions of the sigmoid function. 
```{r, results=FALSE, message=FALSE} 
set.seed(22) 
data("ozone", package = "gss") 
unloadNamespace("MASS")
scaled <- ozone %>% 
  select(-upo3) %>% 
  scale() %>% 
  as_data_frame() %>% 
  mutate(log_upo3 = log(ozone$upo3))

fit <- nnet::nnet( log_upo3 ~., data = scaled, 
                   size = 3, # how many nodes in the *one* hidden layer. 
                   decay = 4e-4, # regularization. Multiply weights by 1 - decay after 
                   # gradient step. 
                   linout = TRUE, # linear output units (refers to f0?). 
                   skip = TRUE, # add skip-layer connections between output and input. 
                   maxit = 500 ) 
```

The weights between the nodes are:

```{r} 
summary(fit) 
```

The in-sample MSE for the regression case is

```{r} 
mean(residuals(fit)^2) 
```

## Projection Pursuit Regression 
Projection pursuit regression is similar to both neural nets and additive
models. It can be seen as an additive model whereas the predictors were first 
projected into the optimal direction. The model takes the form 
$$g_{PPR} = \mu + \sum\limits_{k = 1}^q f_k(\sum \limits_{j = 1}^p
\alpha_{jk}x_j) $$ 
With $\sum \limits_{j = 1}^p \alpha_j = 1$ and
$E[f_k(\sum \limits_{j = 1}^p \alpha_{jk}x_j)] = 0 \;\; \text{for all k}$. 


$\mathbf{\alpha}_k x_j$ is the projection of the j-th column in the design 
matrix onto $\alpha_k$. The functions $f_k$ only
vary along one direction and are hence called ridge functions. The model 
requires much smaller $q$ than a neural net requires hidden units, at the
expense of estimating the ridge functions (which is not necessary for 
neural nets, as it is already fully specified by the sigmoid function).

### Proejction Pursuit Example
In the following, we illustrate how optimal projections of the initial predictor
space can allow us to use an additive functional form to deal with 
interaction terms. Let us consider the following data-generating model 
$$ Y = X_1 \times X_2 + \epsilon \; \text{with} \;\epsilon \sim N(0, 1) \; \text{and}\; X_1, X_2 \sim \text{Unif}(-1,1)$$

Where $X \in \mathbb{R}^2$, i.e. a two-dimensional predictor space with the 
predictors $X_1$ and $X_2$. Using elementary calculus, this can be rewritten as
$$ Y = \frac{1}{4} (X_1 + X_2)^2  - \frac{1}{4}(X_1 - X_2)^2$$
Hence, we rewrote a multiplicative model as an additive model. As we are using 
arbitrary *smooth* functions $f_k$, we can easily fit the quadratic terms in the 
equation above, so the problem to solve becomes
$$Y = \mu + f_1(X_1 + X_2) - f_2(X_1 - X_2)$$
Therefore, the remaining question is how can we choose the two vectors
$\mathbf{\alpha}_1$ and $\mathbf{\alpha}_1$ such that the result of the projection
is $X_1 + X_2 \;\text{and}\;X_1 - X_2$.
With the restriction $|\alpha| = 1$, it turns out we can preceded as follows:
We project the first predictor onto $(\alpha_{11}, \alpha_{12}) = (0.7, 0.7)$
and the second predictor onto $(\alpha_{11}, \alpha_{12}) = (0.7, -0.7)$. This
yields $0.7(X_1 + X_2)$ and $0.7(X_1 - X_2)$.

Let's implement that with R
```{r}
data <- data_frame(
  x1 = runif(500, -1, 1),
  x2 = runif(500, -1, 1),
  y = x1*x2 + rnorm(500, 0, 0.005)
)
all <- ggplot(data, aes(x = x1, y = x2)) + 
  geom_point(aes(color = y), size = 3) + 
  scale_color_gradient2()
```

We can see the obvious pattern, but we can also see that an additive model would 
not do well on that.
```{r}
x1y <- ggplot(data, aes(x = x1, y = y)) + 
  geom_point(aes(color = y), size = 3) + 
  geom_smooth() + 
  scale_color_gradient2()


grid.arrange(all, x1y)
```

How about using the aforementioned projection?

```{r}
data <- data %>%
  mutate(
    projected_x1 = 0.7*(x1 + x2),
    projected_x2 = -0.7*(x1 - x2)
  )

projected_all <- ggplot(data, aes(x = projected_x1, y = projected_x2)) + 
  geom_point(aes(color = y), size = 3) + 
  scale_color_gradient2()

projected_x1 <- ggplot(data, aes(x = projected_x1, y = y)) + 
  geom_point(aes(color = y), size = 3) + 
  geom_smooth() + 
  scale_color_gradient2()

projected_x2 <- ggplot(data, aes(x = projected_x2, y = y)) + 
  geom_point(aes(color = y), size = 3) + 
  geom_smooth() + 
  scale_color_gradient2()

fitted_x1 <- mgcv::gam(y~s(projected_x1), data = data)
fitted_x2 <- mgcv::gam(y~s(projected_x2), data = data)

data <- data %>%
  mutate(fitted = predict(fitted_x1) + predict(fitted_x2))

fitted <- ggplot(data, aes(x = x1, y = x2)) + 
  geom_point(aes(color = fitted), size = 3) + 
  scale_color_gradient2()
grid.arrange(projected_all, projected_x1, projected_x2, fitted, nrow = 2)
```

The bottom right picture shows the predictions with the projection pursuit approach, 
which resembles the original data pretty well. Again, the idea is to use an
additive model to account for the interactions properly by first projecting the 
predictors optimally.
