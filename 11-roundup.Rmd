# Round up

This chapter consists of some thoughts I considered relevant for this course 
but which did not seem to fit well in any of the other chapters.

## Comparing models

The package plotmo provides nice functionality to plot diffent classes of models. 
We use it to compare a few models introduced in the previous chapters:
```{r, echo = FALSE, out.width = "650px"}
knitr::include_graphics("figures/plotmo.png")
```

```{r, echo = FALSE, eval = FALSE}

##  ............................................................................
library("earth") # for ozone1 data
data(ozone1)
oz <- ozone1[, c("O3", "humidity", "temp", "ibt")]
lm.mod <- lm(O3 ~ humidity + temp*ibt, data=oz)
plotmo(lm.mod)
library(rpart)
rpart.mod <- rpart(O3 ~ ., data=oz)
plotmo(rpart.mod)
library("randomForest")
rf.mod <- randomForest(O3 ~ ., data=oz)
plotmo(rf.mod)

##  ............................................................................

library("gbm")                                              
gbm.mod <- gbm(O3 ~ ., data=oz, dist="gaussian", inter=2, n.trees=1000)
plotmo(gbm.mod)


##  ............................................................................

library("gam")
gam.mod <- gam(O3 ~ s(humidity) + s(temp) + s(ibt), data=oz)
plotmo(gam.mod, all2=TRUE)
library("nnet")
set.seed(4)
nnet.mod <- nnet(O3 ~ ., data=scale(oz), size=2, decay=0.01, trace=FALSE)
plotmo(nnet.mod, type="raw", all2=T)  # type="raw" gets passed to predict
```

I think this graph is a nice summary of what we leared in this course. We can 
characterize the models by:

- **How smooth the fitted surface is**. Linear models are smooth by nature (if there
  are no dummy variables involved), random forest (rpart) are just piecewise
  constant. Random forest are smoother than trees because they are aggregated
  trees. Ignore gbm for now and note that neural nets are also rather smooth.
- **How flexible they are**: Linear models are not very flexible, gams are only 
  additive, so no interaction effects, trees can have interaction effects, 
  so do random forests.

This is a rough classification, but it grasps the general pattern.

## Exercises Take-aways

**Creating Formulas**
Task (Series 11): Generate a R-formula for a cubic penalized regression model 
that accounts for all 3-way interactions.

From `?formula` 

> The ^ operator indicates crossing to the specified degree. For example 
> (a+b+c)^2 is identical to (a+b+c)*(a+b+c) which in turn expands to a formula 
> containing the main effects for a, b and c together with their second-order 
> interactions.

```{r, eval = FALSE}
library(sfsmisc)
formula <- wrapFormula(
  logO3 ~., 
  data = data, 
  wrapString = "poly(*, degree = 3" # polynomial
) 
update(formula, logO3 ~ .^3) # three way interaction

```

**Matrix Multiplication**

* You can't multiply data frames. No way. Use `as.matrix` to convert.
* Don't forget the intercept in the model frame.

**Other**

* R is vectorized. Before writing complicated `map()` stuff, think about whether
  it is possible to do all calculations vectorised. Compare exercise series 8.

* Scaling and Transformations: If the response or in fact any variable is highly
  skewed, use the log as a first aid transformation to for efficiency. Also,
  some methods such as neural networks work better with scaled predictors.
  Another category are methods such as lasso or ridge regression, where it is 
  beneficial for interpretability to scale the variables.

**Accessing model parameters**

When calculating the degrees of freedom, e.g. for the mallows cp, never use
someghing like `length(coef(fit))`` as $p$, since this will only work for 
linear models. For additive models for example, this will give you the 
number of soothing terms used, which is **not** equivalent to the degrees of 
freedom. Instead, use

```{r, eval = FALSE}
fit$df.residual
```

Which is available for many classes of fitted models, i.e. also GAMs.  Also,
often it is not necessary to compute something like the residual standard 
error, since you can conveniently access it via
```{r, eval = FALSE}
summary(fit)$sigma
```

Also, do sanity checks for your results. A qick `plot(unlist(cps))` for example
can show you the various values cp takes for different model vs. their index.
As with cross validation, you know that the curve must be somehow convex.

**Inconsistent S3 generics**

Unfortunately, not all S3 generics do feature the same argument names. This 
is particularly dangerous when using `...` to pass arguments. For example,
different predict methods do not use the same argumet for new data. Some use
`newdata`, others `data` etc.

**Programming: Fail Fast and debug**

Unit testing on the ways costs next to nothing. Don't wait testing your 
functions until you are done with the whole exam. Examples are 

* boostrapping: Test whether regression coefficients are exactly the 
  *true* coefficients when no noise has been added. Otherwise, check whether 
  they are reasonable.
* Test dimensions of matrices you use.
* Test whether values you obatain are plausible
* use `browser()` extensively, even when writing a function, not just to debug 
  it.

## Cheatsheet
* The boostrapped confidence intrevals are 

$$[2 \hat{\theta} - q_{1-\alpha/2}, 2\hat{\theta}- q_{\alpha/2}]$$
